{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PolitoVandal/project-sim2real-rialti-giunti-gjinaj/blob/main/colab_template/test_random_policy.ipynb)"
      ],
      "metadata": {
        "id": "h9EafyX8NLlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"project-sim2real-rialti-giunti-gjinaj\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sim2real\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sample_data\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!git clone https://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@github.com/PolitoVandal/project-sim2real-rialti-giunti-gjinaj\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "if not os.path.exists('sim2real/models'):\n",
        "    os.makedirs('sim2real/models')\n",
        "\n",
        "if not os.path.exists('sim2real/plots'):\n",
        "    os.makedirs('sim2real/plots')"
      ],
      "metadata": {
        "id": "UG4ZM2KG_SLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ed2759-a143-4b4e-c526-1aa0c1dc46b7"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project-sim2real-rialti-giunti-gjinaj'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Counting objects: 100% (99/99), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 243 (delta 50), reused 56 (delta 21), pack-reused 144 (from 2)\u001b[K\n",
            "Receiving objects: 100% (243/243), 6.38 MiB | 20.54 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "AW6XT0jSJI8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7154dac0-a6ec-4632-fcf6-40609413b849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libglew-dev is already the newest version (2.2.0-4).\n",
            "libgl1-mesa-dev is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "libosmesa6-dev is already the newest version (23.2.1-1ubuntu3.1~22.04.3).\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "patchelf is already the newest version (0.14.3-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: free-mujoco-py in /usr/local/lib/python3.10/dist-packages (2.1.6)\n",
            "Requirement already satisfied: Cython<0.30.0,>=0.29.24 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (0.29.37)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.1)\n",
            "Requirement already satisfied: fasteners==0.15 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (0.15)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.12.0)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.36.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.17.0)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (11.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.21.0)\n",
            "Requirement already satisfied: shimmy in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.4.1)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.10.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.69.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!pip install importlib-metadata\n",
        "!pip install shimmy\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the custom Hopper environment and provided util functions\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload `custom_hopper.zip` to the current session's file storage\n",
        "2.   Un-zip it by running cell below\n"
      ],
      "metadata": {
        "id": "gwIRXGd5K3xJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1**\n"
      ],
      "metadata": {
        "id": "nUs5gQXCaiRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('project-sim2real-rialti-giunti-gjinaj/colab_template')\n",
        "!unzip custom_hopper.zip"
      ],
      "metadata": {
        "id": "T9WsofDVLaCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d37a0d6-c49d-4f1a-b8b8-de3af35228ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  custom_hopper.zip\n",
            "   creating: env/\n",
            "  inflating: env/__init__.py         \n",
            "  inflating: env/custom_hopper.py    \n",
            "  inflating: env/mujoco_env.py       \n",
            "   creating: env/assets/\n",
            "  inflating: env/assets/hopper.xml   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7pJC_JevLf1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Test a random policy on the Gym Hopper environment**\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "Play around with this code to get familiar with the\n",
        "Hopper environment.\n",
        "\n",
        "For example, what happens if you don't reset the environment\n",
        "even after the episode is over?\n",
        "When exactly is the episode over?\n",
        "What is an action here?"
      ],
      "metadata": {
        "id": "W4NsuF6pJPVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from env.custom_hopper import *"
      ],
      "metadata": {
        "id": "uTYmUufrJTNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e861c60a-d232-4e8a-9551-1c8265f63dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:running build_ext\n",
            "INFO:root:building 'mujoco_py.cymj' extension\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n",
            "INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CustomHopper-source-v0')\n",
        "# env = gym.make('CustomHopper-target-v0')\n",
        "print('State space:', env.observation_space)  # state-space\n",
        "print('Action space:', env.action_space)  # action-space\n",
        "print(\"Mass values of each link:\", env.model.body_mass)\n",
        "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper\n",
        "print(\"Bodies defined in the environment:\", env.model.body_names)\n",
        "print(\"Number of degrees of freedom (DoFs) of the robot:\", env.model.nv)\n",
        "print(\"Number of DoFs for each body:\", env.model.body_dofnum)\n",
        "print(\"Number of actuators:\", env.model.nu)"
      ],
      "metadata": {
        "id": "QcCfCGg-Jyc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3b4fde-dc9c-4019-98e6-fe8091230afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Mass values of each link: [0.         2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Bodies defined in the environment: ('world', 'torso', 'thigh', 'leg', 'foot')\n",
            "Number of degrees of freedom (DoFs) of the robot: 6\n",
            "Number of DoFs for each body: [0 3 1 1 1]\n",
            "Number of actuators: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Hopper Environment in MuJoCo\n",
        "\n",
        "This document provides a detailed analysis of the Hopper environment based on the provided terminal output and information from the MuJoCo and Gym documentation.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.1: What is the state space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **State Space Description**:\n",
        "  The state space is represented by a **Box** object with shape `(11,)`, meaning it is a vector of 11 continuous values. These values typically include the positions, velocities, and potentially other sensory data (like contact forces) of the Hopper's components.\n",
        "  \n",
        "- **Nature of State Space**:\n",
        "  The state space is **continuous**, as indicated by the range `(-inf, inf)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.2: What is the action space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Action Space Description**:\n",
        "  The action space is represented by a **Box** object with shape `(3,)`, meaning it consists of 3 continuous values. These correspond to the torques applied to the actuators controlling the Hopper's joints.\n",
        "\n",
        "- **Nature of Action Space**:\n",
        "  The action space is **continuous**, as indicated by the range `(-1.0, 1.0)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.3: What is the mass value of each link of the Hopper environment, in the source and target variants respectively?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Mass Values for the Source Variant**:\n",
        "  From the terminal output:\n",
        "Mass values of each link: [0. 2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
        "These correspond to the masses of the bodies:\n",
        "- `world`: 0.0 (fixed reference point)\n",
        "- `torso`: 2.5343\n",
        "- `thigh`: 3.9270\n",
        "- `leg`: 2.7143\n",
        "- `foot`: 5.0894\n",
        "\n",
        "- **Mass Values for the Target Variant**:\n",
        "The mass values for the target variant can be obtained by switching the environment initialization to:\n",
        "```python\n",
        "env = gym.make('CustomHopper-target-v0')\n",
        "(Ensure to re-run the relevant command to print the mass values.)\n",
        "\n",
        "Comparison of Source and Target Variants: Any differences in mass values between the source and target variants must be explicitly checked in the respective initialization. These differences typically simulate dynamics variability to test robustness.\n",
        "\n",
        "Additional Information Derived from the Environment\n",
        "Bodies Defined in the Environment:\n",
        "\n",
        "('world', 'torso', 'thigh', 'leg', 'foot')\n",
        "These represent the main components of the Hopper system.\n",
        "\n",
        "Number of Degrees of Freedom (DoFs) of the Robot: 6\n",
        "This includes translational and rotational movements of the robot.\n",
        "\n",
        "Number of DoFs for Each Body:\n",
        "[0 3 1 1 1]\n",
        "world: 0 (fixed body)\n",
        "torso: 3\n",
        "thigh: 1\n",
        "leg: 1\n",
        "foot: 1\n",
        "\n",
        "Number of Actuators:\n",
        "The Hopper has 3 actuators controlling its joints.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Hopper environment features a continuous state and action space, making it well-suited for reinforcement learning tasks. Understanding the dynamics, including body masses and degrees of freedom, is crucial for designing robust controllers and algorithms."
      ],
      "metadata": {
        "id": "AYhXnui3h70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 5\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "  done = False\n",
        "  observation = env.reset()\t # Reset environment to initial state\n",
        "\n",
        "  while not done:  # Until the episode is over\n",
        "\n",
        "    action = env.action_space.sample()\t# Sample random action\n",
        "\n",
        "    observation, reward, done, info = env.step(action)\t# Step the simulator to the next timestep"
      ],
      "metadata": {
        "id": "DT1oXr8HJ05h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "Dfas8tGOax8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task2.py --model_name sac_hopper --total_timesteps 5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKi913Qgj2xI",
        "outputId": "8ee7a512-d75c-4b85-893a-186a5af063e6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.7    |\n",
            "|    critic_loss     | 6.09     |\n",
            "|    ent_coef        | 0.387    |\n",
            "|    ent_coef_loss   | -3.23    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3364     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3470, episode_reward=213.96 +/- 6.40\n",
            "Episode length: 91.80 +/- 2.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.8     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3470     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.9    |\n",
            "|    critic_loss     | 3.86     |\n",
            "|    ent_coef        | 0.387    |\n",
            "|    ent_coef_loss   | -3.23    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3369     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3475, episode_reward=235.10 +/- 0.90\n",
            "Episode length: 99.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.6     |\n",
            "|    mean_reward     | 235      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3475     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.1    |\n",
            "|    critic_loss     | 12.2     |\n",
            "|    ent_coef        | 0.386    |\n",
            "|    ent_coef_loss   | -3.49    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3374     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3480, episode_reward=236.82 +/- 1.29\n",
            "Episode length: 101.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 101      |\n",
            "|    mean_reward     | 237      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3480     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.2    |\n",
            "|    critic_loss     | 6.01     |\n",
            "|    ent_coef        | 0.386    |\n",
            "|    ent_coef_loss   | -3.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3379     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3485, episode_reward=228.09 +/- 1.63\n",
            "Episode length: 97.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.6     |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3485     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.6    |\n",
            "|    critic_loss     | 4.87     |\n",
            "|    ent_coef        | 0.385    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3384     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3490, episode_reward=234.19 +/- 0.97\n",
            "Episode length: 100.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 100      |\n",
            "|    mean_reward     | 234      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40      |\n",
            "|    critic_loss     | 14       |\n",
            "|    ent_coef        | 0.385    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3389     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3495, episode_reward=240.45 +/- 0.52\n",
            "Episode length: 102.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 102      |\n",
            "|    mean_reward     | 240      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3495     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.1    |\n",
            "|    critic_loss     | 7.28     |\n",
            "|    ent_coef        | 0.384    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3394     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3500, episode_reward=231.15 +/- 2.45\n",
            "Episode length: 97.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.2     |\n",
            "|    mean_reward     | 231      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3500     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.2    |\n",
            "|    critic_loss     | 7.49     |\n",
            "|    ent_coef        | 0.384    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3399     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3505, episode_reward=225.00 +/- 1.71\n",
            "Episode length: 95.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.2     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3505     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.8    |\n",
            "|    critic_loss     | 5.6      |\n",
            "|    ent_coef        | 0.383    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3404     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3510, episode_reward=200.26 +/- 2.20\n",
            "Episode length: 87.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.4     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3510     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.1    |\n",
            "|    critic_loss     | 7.19     |\n",
            "|    ent_coef        | 0.383    |\n",
            "|    ent_coef_loss   | -3.2     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3409     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3515, episode_reward=181.45 +/- 1.33\n",
            "Episode length: 81.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.6     |\n",
            "|    mean_reward     | 181      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3515     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.6    |\n",
            "|    critic_loss     | 5.92     |\n",
            "|    ent_coef        | 0.382    |\n",
            "|    ent_coef_loss   | -3.12    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3414     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3520, episode_reward=202.52 +/- 1.08\n",
            "Episode length: 88.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.6     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3520     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40      |\n",
            "|    critic_loss     | 8.81     |\n",
            "|    ent_coef        | 0.382    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3419     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3525, episode_reward=227.67 +/- 1.15\n",
            "Episode length: 98.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.6     |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3525     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.4    |\n",
            "|    critic_loss     | 5.12     |\n",
            "|    ent_coef        | 0.381    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3424     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3530, episode_reward=223.93 +/- 0.98\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 224      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3530     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40      |\n",
            "|    critic_loss     | 5.72     |\n",
            "|    ent_coef        | 0.381    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3429     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3535, episode_reward=203.18 +/- 2.37\n",
            "Episode length: 88.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.4     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3535     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.6    |\n",
            "|    critic_loss     | 5.38     |\n",
            "|    ent_coef        | 0.38     |\n",
            "|    ent_coef_loss   | -3.29    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3434     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3540, episode_reward=225.28 +/- 0.97\n",
            "Episode length: 96.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.4     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.3    |\n",
            "|    critic_loss     | 11.6     |\n",
            "|    ent_coef        | 0.38     |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3439     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3545, episode_reward=241.67 +/- 1.41\n",
            "Episode length: 105.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 105      |\n",
            "|    mean_reward     | 242      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3545     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.2    |\n",
            "|    critic_loss     | 7.35     |\n",
            "|    ent_coef        | 0.379    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3444     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3550, episode_reward=244.88 +/- 1.09\n",
            "Episode length: 106.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 106      |\n",
            "|    mean_reward     | 245      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3550     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.2    |\n",
            "|    critic_loss     | 4.47     |\n",
            "|    ent_coef        | 0.379    |\n",
            "|    ent_coef_loss   | -3.3     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3449     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3555, episode_reward=244.02 +/- 0.78\n",
            "Episode length: 106.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 106      |\n",
            "|    mean_reward     | 244      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3555     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.9    |\n",
            "|    critic_loss     | 2.12     |\n",
            "|    ent_coef        | 0.378    |\n",
            "|    ent_coef_loss   | -3.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3454     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3560, episode_reward=248.71 +/- 0.58\n",
            "Episode length: 108.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 109      |\n",
            "|    mean_reward     | 249      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3560     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.1    |\n",
            "|    critic_loss     | 2.69     |\n",
            "|    ent_coef        | 0.378    |\n",
            "|    ent_coef_loss   | -3.36    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3459     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3565, episode_reward=224.79 +/- 2.74\n",
            "Episode length: 95.40 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3565     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.8    |\n",
            "|    critic_loss     | 7        |\n",
            "|    ent_coef        | 0.377    |\n",
            "|    ent_coef_loss   | -3.39    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3464     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3570, episode_reward=194.08 +/- 0.75\n",
            "Episode length: 87.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87       |\n",
            "|    mean_reward     | 194      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3570     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.1    |\n",
            "|    critic_loss     | 5.88     |\n",
            "|    ent_coef        | 0.377    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3469     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3575, episode_reward=195.26 +/- 0.87\n",
            "Episode length: 88.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.2     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3575     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.3    |\n",
            "|    critic_loss     | 7.12     |\n",
            "|    ent_coef        | 0.376    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3474     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3580, episode_reward=206.10 +/- 0.84\n",
            "Episode length: 91.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91       |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3580     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 2.06     |\n",
            "|    ent_coef        | 0.376    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3479     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3585, episode_reward=205.43 +/- 0.74\n",
            "Episode length: 90.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3585     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.4    |\n",
            "|    critic_loss     | 1.53     |\n",
            "|    ent_coef        | 0.375    |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3484     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3590, episode_reward=195.14 +/- 1.44\n",
            "Episode length: 86.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.6     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3590     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41      |\n",
            "|    critic_loss     | 5.73     |\n",
            "|    ent_coef        | 0.375    |\n",
            "|    ent_coef_loss   | -3.53    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3489     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3595, episode_reward=191.11 +/- 0.87\n",
            "Episode length: 86.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.2     |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3595     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.7    |\n",
            "|    critic_loss     | 6.14     |\n",
            "|    ent_coef        | 0.374    |\n",
            "|    ent_coef_loss   | -3.3     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3494     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3600, episode_reward=189.71 +/- 1.07\n",
            "Episode length: 87.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.6     |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.8    |\n",
            "|    critic_loss     | 4.62     |\n",
            "|    ent_coef        | 0.374    |\n",
            "|    ent_coef_loss   | -3.22    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3499     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3605, episode_reward=190.90 +/- 0.96\n",
            "Episode length: 85.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.6     |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3605     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.9    |\n",
            "|    critic_loss     | 6.59     |\n",
            "|    ent_coef        | 0.373    |\n",
            "|    ent_coef_loss   | -3.3     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3504     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3610, episode_reward=201.20 +/- 2.14\n",
            "Episode length: 88.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88       |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3610     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.7    |\n",
            "|    critic_loss     | 1.97     |\n",
            "|    ent_coef        | 0.373    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3509     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3615, episode_reward=181.66 +/- 2.09\n",
            "Episode length: 82.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 82       |\n",
            "|    mean_reward     | 182      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3615     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.3    |\n",
            "|    critic_loss     | 6.37     |\n",
            "|    ent_coef        | 0.372    |\n",
            "|    ent_coef_loss   | -3.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3514     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3620, episode_reward=180.50 +/- 1.11\n",
            "Episode length: 81.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.6     |\n",
            "|    mean_reward     | 180      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3620     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41      |\n",
            "|    critic_loss     | 10.2     |\n",
            "|    ent_coef        | 0.372    |\n",
            "|    ent_coef_loss   | -3.35    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3519     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3625, episode_reward=197.52 +/- 2.20\n",
            "Episode length: 86.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.8     |\n",
            "|    mean_reward     | 198      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3625     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.1    |\n",
            "|    critic_loss     | 6.24     |\n",
            "|    ent_coef        | 0.371    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3524     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3630, episode_reward=207.42 +/- 3.25\n",
            "Episode length: 90.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90       |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3630     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41      |\n",
            "|    critic_loss     | 8.11     |\n",
            "|    ent_coef        | 0.371    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3529     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3635, episode_reward=190.61 +/- 0.95\n",
            "Episode length: 85.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.2     |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3635     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.3    |\n",
            "|    critic_loss     | 1.84     |\n",
            "|    ent_coef        | 0.37     |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3534     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3640, episode_reward=185.89 +/- 1.00\n",
            "Episode length: 83.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 83.8     |\n",
            "|    mean_reward     | 186      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3640     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.5    |\n",
            "|    critic_loss     | 12.3     |\n",
            "|    ent_coef        | 0.37     |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3539     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3645, episode_reward=191.07 +/- 3.03\n",
            "Episode length: 84.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 84.4     |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3645     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.6    |\n",
            "|    critic_loss     | 4.98     |\n",
            "|    ent_coef        | 0.369    |\n",
            "|    ent_coef_loss   | -3.19    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3544     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3650, episode_reward=231.24 +/- 1.96\n",
            "Episode length: 99.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.2     |\n",
            "|    mean_reward     | 231      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3650     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40.8    |\n",
            "|    critic_loss     | 2.64     |\n",
            "|    ent_coef        | 0.369    |\n",
            "|    ent_coef_loss   | -3.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3549     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3655, episode_reward=238.72 +/- 0.90\n",
            "Episode length: 106.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 106      |\n",
            "|    mean_reward     | 239      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3655     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 5.83     |\n",
            "|    ent_coef        | 0.368    |\n",
            "|    ent_coef_loss   | -3.49    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3554     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3660, episode_reward=225.17 +/- 2.89\n",
            "Episode length: 95.40 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3660     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.6    |\n",
            "|    critic_loss     | 5.81     |\n",
            "|    ent_coef        | 0.368    |\n",
            "|    ent_coef_loss   | -3.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3559     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3665, episode_reward=189.03 +/- 0.92\n",
            "Episode length: 85.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.4     |\n",
            "|    mean_reward     | 189      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3665     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.2    |\n",
            "|    critic_loss     | 7.06     |\n",
            "|    ent_coef        | 0.367    |\n",
            "|    ent_coef_loss   | -3.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3564     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3670, episode_reward=193.84 +/- 0.35\n",
            "Episode length: 88.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.4     |\n",
            "|    mean_reward     | 194      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.5    |\n",
            "|    critic_loss     | 4.27     |\n",
            "|    ent_coef        | 0.367    |\n",
            "|    ent_coef_loss   | -3.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3569     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3675, episode_reward=201.64 +/- 1.51\n",
            "Episode length: 91.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3675     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.1    |\n",
            "|    critic_loss     | 5.94     |\n",
            "|    ent_coef        | 0.366    |\n",
            "|    ent_coef_loss   | -3.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3574     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3680, episode_reward=205.01 +/- 0.96\n",
            "Episode length: 90.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3680     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.7    |\n",
            "|    critic_loss     | 1.93     |\n",
            "|    ent_coef        | 0.366    |\n",
            "|    ent_coef_loss   | -3.49    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3579     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3685, episode_reward=195.88 +/- 0.70\n",
            "Episode length: 87.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.4     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3685     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.4    |\n",
            "|    critic_loss     | 2.88     |\n",
            "|    ent_coef        | 0.366    |\n",
            "|    ent_coef_loss   | -3.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3584     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3690, episode_reward=189.50 +/- 1.54\n",
            "Episode length: 84.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 84.6     |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3690     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.1    |\n",
            "|    critic_loss     | 9.52     |\n",
            "|    ent_coef        | 0.365    |\n",
            "|    ent_coef_loss   | -3.44    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3589     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3695, episode_reward=196.66 +/- 2.20\n",
            "Episode length: 86.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.8     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3695     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.6    |\n",
            "|    critic_loss     | 10.8     |\n",
            "|    ent_coef        | 0.365    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3594     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3700, episode_reward=217.45 +/- 2.77\n",
            "Episode length: 92.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.6     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3700     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43      |\n",
            "|    critic_loss     | 1.91     |\n",
            "|    ent_coef        | 0.364    |\n",
            "|    ent_coef_loss   | -3.35    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3599     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3705, episode_reward=234.52 +/- 1.59\n",
            "Episode length: 98.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.8     |\n",
            "|    mean_reward     | 235      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3705     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.5    |\n",
            "|    critic_loss     | 3.94     |\n",
            "|    ent_coef        | 0.364    |\n",
            "|    ent_coef_loss   | -3.54    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3604     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3710, episode_reward=222.53 +/- 1.63\n",
            "Episode length: 94.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3710     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.4    |\n",
            "|    critic_loss     | 5.59     |\n",
            "|    ent_coef        | 0.363    |\n",
            "|    ent_coef_loss   | -3.6     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3609     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3715, episode_reward=213.91 +/- 1.96\n",
            "Episode length: 91.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.6     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3715     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.7    |\n",
            "|    critic_loss     | 6.14     |\n",
            "|    ent_coef        | 0.363    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3614     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3720, episode_reward=197.14 +/- 1.24\n",
            "Episode length: 86.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.4     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.5    |\n",
            "|    critic_loss     | 6.72     |\n",
            "|    ent_coef        | 0.362    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3619     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3725, episode_reward=186.32 +/- 1.12\n",
            "Episode length: 83.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 83.4     |\n",
            "|    mean_reward     | 186      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3725     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.2    |\n",
            "|    critic_loss     | 5.51     |\n",
            "|    ent_coef        | 0.362    |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3624     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3730, episode_reward=194.88 +/- 1.23\n",
            "Episode length: 86.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.4     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3730     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.8    |\n",
            "|    critic_loss     | 9.73     |\n",
            "|    ent_coef        | 0.361    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3629     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3735, episode_reward=205.57 +/- 2.67\n",
            "Episode length: 89.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.6     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3735     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 8.33     |\n",
            "|    ent_coef        | 0.361    |\n",
            "|    ent_coef_loss   | -3.27    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3634     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3740, episode_reward=216.61 +/- 3.16\n",
            "Episode length: 92.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.6     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3740     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 6.26     |\n",
            "|    ent_coef        | 0.36     |\n",
            "|    ent_coef_loss   | -3.51    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3639     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3745, episode_reward=215.90 +/- 2.61\n",
            "Episode length: 92.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92       |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3745     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.9    |\n",
            "|    critic_loss     | 11.5     |\n",
            "|    ent_coef        | 0.36     |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3644     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3750, episode_reward=204.29 +/- 4.31\n",
            "Episode length: 88.60 +/- 1.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.6     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3750     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.6    |\n",
            "|    critic_loss     | 5.49     |\n",
            "|    ent_coef        | 0.359    |\n",
            "|    ent_coef_loss   | -3.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3649     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3755, episode_reward=212.44 +/- 1.94\n",
            "Episode length: 91.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.4     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3755     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.7    |\n",
            "|    critic_loss     | 3.33     |\n",
            "|    ent_coef        | 0.359    |\n",
            "|    ent_coef_loss   | -3.62    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3654     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 67.1     |\n",
            "|    ep_rew_mean     | 115      |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 8        |\n",
            "|    time_elapsed    | 456      |\n",
            "|    total_timesteps | 3756     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.2    |\n",
            "|    critic_loss     | 3.5      |\n",
            "|    ent_coef        | 0.359    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3655     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3760, episode_reward=232.35 +/- 1.37\n",
            "Episode length: 99.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99       |\n",
            "|    mean_reward     | 232      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3760     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.1    |\n",
            "|    critic_loss     | 11.1     |\n",
            "|    ent_coef        | 0.358    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3659     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3765, episode_reward=217.52 +/- 2.06\n",
            "Episode length: 92.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3765     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.4    |\n",
            "|    critic_loss     | 4.66     |\n",
            "|    ent_coef        | 0.358    |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3664     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3770, episode_reward=192.56 +/- 1.45\n",
            "Episode length: 85.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.4     |\n",
            "|    mean_reward     | 193      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.1    |\n",
            "|    critic_loss     | 5.82     |\n",
            "|    ent_coef        | 0.357    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3669     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3775, episode_reward=192.73 +/- 1.20\n",
            "Episode length: 87.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.8     |\n",
            "|    mean_reward     | 193      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3775     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.5    |\n",
            "|    critic_loss     | 2.26     |\n",
            "|    ent_coef        | 0.357    |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3674     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3780, episode_reward=196.23 +/- 0.56\n",
            "Episode length: 88.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88       |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3780     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.5    |\n",
            "|    critic_loss     | 4.3      |\n",
            "|    ent_coef        | 0.356    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3679     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3785, episode_reward=218.39 +/- 3.05\n",
            "Episode length: 92.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3785     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.9    |\n",
            "|    critic_loss     | 5.17     |\n",
            "|    ent_coef        | 0.356    |\n",
            "|    ent_coef_loss   | -3.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3684     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3790, episode_reward=231.00 +/- 2.72\n",
            "Episode length: 97.40 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.4     |\n",
            "|    mean_reward     | 231      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3790     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.9    |\n",
            "|    critic_loss     | 6.56     |\n",
            "|    ent_coef        | 0.356    |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3689     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3795, episode_reward=199.65 +/- 2.07\n",
            "Episode length: 87.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87       |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3795     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.9    |\n",
            "|    critic_loss     | 3.64     |\n",
            "|    ent_coef        | 0.355    |\n",
            "|    ent_coef_loss   | -3.6     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3694     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3800, episode_reward=192.63 +/- 2.32\n",
            "Episode length: 84.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 84.6     |\n",
            "|    mean_reward     | 193      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.8    |\n",
            "|    critic_loss     | 5.12     |\n",
            "|    ent_coef        | 0.355    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3699     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3805, episode_reward=212.81 +/- 1.95\n",
            "Episode length: 91.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3805     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.5    |\n",
            "|    critic_loss     | 6.45     |\n",
            "|    ent_coef        | 0.354    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3704     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3810, episode_reward=202.59 +/- 1.23\n",
            "Episode length: 89.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.4     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3810     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.2    |\n",
            "|    critic_loss     | 6.2      |\n",
            "|    ent_coef        | 0.354    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3709     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3815, episode_reward=199.49 +/- 0.98\n",
            "Episode length: 91.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91       |\n",
            "|    mean_reward     | 199      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3815     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 2.29     |\n",
            "|    ent_coef        | 0.353    |\n",
            "|    ent_coef_loss   | -3.44    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3714     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3820, episode_reward=189.66 +/- 1.15\n",
            "Episode length: 88.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.4     |\n",
            "|    mean_reward     | 190      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3820     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.1    |\n",
            "|    critic_loss     | 3.69     |\n",
            "|    ent_coef        | 0.353    |\n",
            "|    ent_coef_loss   | -3.39    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3719     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3825, episode_reward=187.65 +/- 0.26\n",
            "Episode length: 85.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85       |\n",
            "|    mean_reward     | 188      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3825     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 4.6      |\n",
            "|    ent_coef        | 0.352    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3724     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3830, episode_reward=191.06 +/- 0.20\n",
            "Episode length: 85.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85       |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3830     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.3    |\n",
            "|    critic_loss     | 2.33     |\n",
            "|    ent_coef        | 0.352    |\n",
            "|    ent_coef_loss   | -3.39    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3729     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3835, episode_reward=206.16 +/- 5.34\n",
            "Episode length: 89.20 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.2     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3835     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.5    |\n",
            "|    critic_loss     | 5.43     |\n",
            "|    ent_coef        | 0.351    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3734     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3840, episode_reward=218.26 +/- 2.44\n",
            "Episode length: 92.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.6     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3840     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.8    |\n",
            "|    critic_loss     | 7.94     |\n",
            "|    ent_coef        | 0.351    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3739     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3845, episode_reward=215.89 +/- 3.00\n",
            "Episode length: 92.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3845     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.8    |\n",
            "|    critic_loss     | 5.24     |\n",
            "|    ent_coef        | 0.35     |\n",
            "|    ent_coef_loss   | -3.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3744     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3850, episode_reward=205.54 +/- 2.53\n",
            "Episode length: 89.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89       |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.4    |\n",
            "|    critic_loss     | 5.55     |\n",
            "|    ent_coef        | 0.35     |\n",
            "|    ent_coef_loss   | -3.41    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3749     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3855, episode_reward=223.03 +/- 2.27\n",
            "Episode length: 94.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3855     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.8    |\n",
            "|    critic_loss     | 9.2      |\n",
            "|    ent_coef        | 0.349    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3754     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3860, episode_reward=212.61 +/- 1.39\n",
            "Episode length: 91.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3860     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.2    |\n",
            "|    critic_loss     | 5.66     |\n",
            "|    ent_coef        | 0.349    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3759     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3865, episode_reward=200.31 +/- 4.78\n",
            "Episode length: 87.20 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.2     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3865     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43      |\n",
            "|    critic_loss     | 4.78     |\n",
            "|    ent_coef        | 0.349    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3764     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3870, episode_reward=196.78 +/- 2.73\n",
            "Episode length: 86.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.4     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3870     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.8    |\n",
            "|    critic_loss     | 6.12     |\n",
            "|    ent_coef        | 0.348    |\n",
            "|    ent_coef_loss   | -3.41    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3769     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3875, episode_reward=199.99 +/- 2.79\n",
            "Episode length: 87.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.4     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3875     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.6    |\n",
            "|    critic_loss     | 2.69     |\n",
            "|    ent_coef        | 0.348    |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3774     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3880, episode_reward=204.12 +/- 4.79\n",
            "Episode length: 88.80 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.8     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3880     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.4    |\n",
            "|    critic_loss     | 9.51     |\n",
            "|    ent_coef        | 0.347    |\n",
            "|    ent_coef_loss   | -3.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3779     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3885, episode_reward=193.76 +/- 0.78\n",
            "Episode length: 86.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.8     |\n",
            "|    mean_reward     | 194      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3885     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.2    |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    ent_coef        | 0.347    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3784     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3890, episode_reward=194.96 +/- 0.88\n",
            "Episode length: 88.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.2     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3890     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.4    |\n",
            "|    critic_loss     | 5.67     |\n",
            "|    ent_coef        | 0.346    |\n",
            "|    ent_coef_loss   | -3.38    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3789     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3895, episode_reward=196.57 +/- 0.74\n",
            "Episode length: 89.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.2     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3895     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.3    |\n",
            "|    critic_loss     | 4.71     |\n",
            "|    ent_coef        | 0.346    |\n",
            "|    ent_coef_loss   | -3.35    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3794     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3900, episode_reward=198.11 +/- 0.89\n",
            "Episode length: 91.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.6     |\n",
            "|    mean_reward     | 198      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.1    |\n",
            "|    critic_loss     | 9.52     |\n",
            "|    ent_coef        | 0.345    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3799     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3905, episode_reward=200.05 +/- 1.32\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3905     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.1    |\n",
            "|    critic_loss     | 4.31     |\n",
            "|    ent_coef        | 0.345    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3804     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3910, episode_reward=206.46 +/- 1.99\n",
            "Episode length: 91.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3910     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.6    |\n",
            "|    critic_loss     | 7.22     |\n",
            "|    ent_coef        | 0.344    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3809     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3915, episode_reward=220.36 +/- 2.21\n",
            "Episode length: 93.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3915     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.1    |\n",
            "|    critic_loss     | 4.43     |\n",
            "|    ent_coef        | 0.344    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3814     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3920, episode_reward=218.45 +/- 0.78\n",
            "Episode length: 92.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3920     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.3    |\n",
            "|    critic_loss     | 8.12     |\n",
            "|    ent_coef        | 0.344    |\n",
            "|    ent_coef_loss   | -3.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3819     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3925, episode_reward=201.50 +/- 3.01\n",
            "Episode length: 87.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.6     |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3925     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.4    |\n",
            "|    critic_loss     | 6.46     |\n",
            "|    ent_coef        | 0.343    |\n",
            "|    ent_coef_loss   | -3.51    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3824     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3930, episode_reward=196.47 +/- 0.50\n",
            "Episode length: 87.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.2     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3930     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.5    |\n",
            "|    critic_loss     | 5.47     |\n",
            "|    ent_coef        | 0.343    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3829     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3935, episode_reward=211.40 +/- 1.06\n",
            "Episode length: 92.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3935     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.5    |\n",
            "|    critic_loss     | 9.9      |\n",
            "|    ent_coef        | 0.342    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3834     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3940, episode_reward=221.23 +/- 0.82\n",
            "Episode length: 95.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.2     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3940     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 1.74     |\n",
            "|    ent_coef        | 0.342    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3839     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3945, episode_reward=218.02 +/- 0.66\n",
            "Episode length: 95.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.8     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3945     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.5    |\n",
            "|    critic_loss     | 5.95     |\n",
            "|    ent_coef        | 0.341    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3844     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3950, episode_reward=211.55 +/- 0.84\n",
            "Episode length: 94.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 12.1     |\n",
            "|    ent_coef        | 0.341    |\n",
            "|    ent_coef_loss   | -3.53    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3849     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3955, episode_reward=204.65 +/- 1.27\n",
            "Episode length: 90.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3955     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45      |\n",
            "|    critic_loss     | 4.48     |\n",
            "|    ent_coef        | 0.34     |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3854     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3960, episode_reward=201.99 +/- 1.06\n",
            "Episode length: 90.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.4     |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3960     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.9    |\n",
            "|    critic_loss     | 4.2      |\n",
            "|    ent_coef        | 0.34     |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3859     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3965, episode_reward=204.73 +/- 0.53\n",
            "Episode length: 90.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3965     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.6    |\n",
            "|    critic_loss     | 8.49     |\n",
            "|    ent_coef        | 0.339    |\n",
            "|    ent_coef_loss   | -3.4     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3864     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3970, episode_reward=202.65 +/- 1.32\n",
            "Episode length: 88.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.6     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3970     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.7    |\n",
            "|    critic_loss     | 5.37     |\n",
            "|    ent_coef        | 0.339    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3869     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3975, episode_reward=201.09 +/- 0.68\n",
            "Episode length: 89.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89       |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3975     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.9    |\n",
            "|    critic_loss     | 2.67     |\n",
            "|    ent_coef        | 0.339    |\n",
            "|    ent_coef_loss   | -3.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3874     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3980, episode_reward=203.35 +/- 1.03\n",
            "Episode length: 90.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.6     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.5    |\n",
            "|    critic_loss     | 5.76     |\n",
            "|    ent_coef        | 0.338    |\n",
            "|    ent_coef_loss   | -3.82    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3879     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3985, episode_reward=200.20 +/- 1.02\n",
            "Episode length: 90.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3985     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.1    |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    ent_coef        | 0.338    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3884     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3990, episode_reward=208.72 +/- 0.66\n",
            "Episode length: 92.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.4     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3990     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.4    |\n",
            "|    critic_loss     | 1.9      |\n",
            "|    ent_coef        | 0.337    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3889     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3995, episode_reward=216.46 +/- 0.86\n",
            "Episode length: 93.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3995     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.9    |\n",
            "|    critic_loss     | 3.9      |\n",
            "|    ent_coef        | 0.337    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3894     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4000, episode_reward=201.81 +/- 1.48\n",
            "Episode length: 87.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.6     |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.2    |\n",
            "|    critic_loss     | 6.4      |\n",
            "|    ent_coef        | 0.336    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4005, episode_reward=193.51 +/- 0.95\n",
            "Episode length: 85.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.4     |\n",
            "|    mean_reward     | 194      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4005     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.6    |\n",
            "|    critic_loss     | 1.75     |\n",
            "|    ent_coef        | 0.336    |\n",
            "|    ent_coef_loss   | -3.79    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3904     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4010, episode_reward=197.27 +/- 1.02\n",
            "Episode length: 86.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.8     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4010     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.5    |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.336    |\n",
            "|    ent_coef_loss   | -3.53    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3909     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4015, episode_reward=199.54 +/- 0.73\n",
            "Episode length: 89.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.2     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4015     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.7    |\n",
            "|    critic_loss     | 2.29     |\n",
            "|    ent_coef        | 0.335    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3914     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4020, episode_reward=200.98 +/- 1.60\n",
            "Episode length: 89.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.8     |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4020     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.9    |\n",
            "|    critic_loss     | 4.26     |\n",
            "|    ent_coef        | 0.335    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3919     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4025, episode_reward=206.41 +/- 1.28\n",
            "Episode length: 89.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.6     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4025     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.7    |\n",
            "|    critic_loss     | 9.27     |\n",
            "|    ent_coef        | 0.334    |\n",
            "|    ent_coef_loss   | -3.3     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3924     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4030, episode_reward=201.57 +/- 0.32\n",
            "Episode length: 87.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87       |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4030     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.4    |\n",
            "|    critic_loss     | 4.25     |\n",
            "|    ent_coef        | 0.334    |\n",
            "|    ent_coef_loss   | -3.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3929     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4035, episode_reward=201.34 +/- 0.51\n",
            "Episode length: 87.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87       |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4035     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.1    |\n",
            "|    critic_loss     | 7.02     |\n",
            "|    ent_coef        | 0.333    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3934     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4040, episode_reward=202.45 +/- 0.47\n",
            "Episode length: 88.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88       |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4040     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.2    |\n",
            "|    critic_loss     | 4.76     |\n",
            "|    ent_coef        | 0.333    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3939     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4045, episode_reward=206.21 +/- 1.36\n",
            "Episode length: 89.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.4     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4045     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45      |\n",
            "|    critic_loss     | 4.01     |\n",
            "|    ent_coef        | 0.332    |\n",
            "|    ent_coef_loss   | -3.6     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3944     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4050, episode_reward=212.63 +/- 1.04\n",
            "Episode length: 94.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4050     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.9    |\n",
            "|    critic_loss     | 6.26     |\n",
            "|    ent_coef        | 0.332    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3949     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4055, episode_reward=202.72 +/- 1.63\n",
            "Episode length: 94.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4055     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.5    |\n",
            "|    critic_loss     | 9.2      |\n",
            "|    ent_coef        | 0.332    |\n",
            "|    ent_coef_loss   | -3.14    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3954     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4060, episode_reward=221.75 +/- 0.52\n",
            "Episode length: 98.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 222      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4060     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.4    |\n",
            "|    critic_loss     | 6.78     |\n",
            "|    ent_coef        | 0.331    |\n",
            "|    ent_coef_loss   | -3.26    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3959     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4065, episode_reward=237.56 +/- 0.83\n",
            "Episode length: 98.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.8     |\n",
            "|    mean_reward     | 238      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4065     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.7    |\n",
            "|    critic_loss     | 2.55     |\n",
            "|    ent_coef        | 0.331    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3964     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4070, episode_reward=235.90 +/- 7.43\n",
            "Episode length: 96.80 +/- 2.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.8     |\n",
            "|    mean_reward     | 236      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4070     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.3    |\n",
            "|    critic_loss     | 3.66     |\n",
            "|    ent_coef        | 0.33     |\n",
            "|    ent_coef_loss   | -4       |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3969     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4075, episode_reward=203.77 +/- 1.76\n",
            "Episode length: 87.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.4     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4075     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.7    |\n",
            "|    critic_loss     | 5.59     |\n",
            "|    ent_coef        | 0.33     |\n",
            "|    ent_coef_loss   | -3.61    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3974     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4080, episode_reward=188.86 +/- 1.17\n",
            "Episode length: 85.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.4     |\n",
            "|    mean_reward     | 189      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.4    |\n",
            "|    critic_loss     | 2.39     |\n",
            "|    ent_coef        | 0.329    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3979     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4085, episode_reward=198.06 +/- 0.96\n",
            "Episode length: 92.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.4     |\n",
            "|    mean_reward     | 198      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4085     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.6    |\n",
            "|    critic_loss     | 4.68     |\n",
            "|    ent_coef        | 0.329    |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3984     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4090, episode_reward=210.06 +/- 2.33\n",
            "Episode length: 96.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.8     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4090     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.7    |\n",
            "|    critic_loss     | 6.24     |\n",
            "|    ent_coef        | 0.329    |\n",
            "|    ent_coef_loss   | -3.39    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3989     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4095, episode_reward=218.74 +/- 1.36\n",
            "Episode length: 98.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.8     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4095     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.5    |\n",
            "|    critic_loss     | 3        |\n",
            "|    ent_coef        | 0.328    |\n",
            "|    ent_coef_loss   | -3.34    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3994     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4100, episode_reward=217.05 +/- 1.51\n",
            "Episode length: 95.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95       |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4100     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.6    |\n",
            "|    critic_loss     | 4.85     |\n",
            "|    ent_coef        | 0.328    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3999     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4105, episode_reward=208.91 +/- 0.64\n",
            "Episode length: 92.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4105     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.3    |\n",
            "|    critic_loss     | 6.67     |\n",
            "|    ent_coef        | 0.327    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4004     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4110, episode_reward=213.61 +/- 1.06\n",
            "Episode length: 92.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.4     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4110     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.9    |\n",
            "|    critic_loss     | 4.73     |\n",
            "|    ent_coef        | 0.327    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4009     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4115, episode_reward=218.79 +/- 0.73\n",
            "Episode length: 94.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4115     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.5    |\n",
            "|    critic_loss     | 9.59     |\n",
            "|    ent_coef        | 0.326    |\n",
            "|    ent_coef_loss   | -3.29    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4014     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 68.6     |\n",
            "|    ep_rew_mean     | 121      |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 8        |\n",
            "|    time_elapsed    | 500      |\n",
            "|    total_timesteps | 4118     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.9    |\n",
            "|    critic_loss     | 10.5     |\n",
            "|    ent_coef        | 0.326    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4017     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4120, episode_reward=211.48 +/- 0.34\n",
            "Episode length: 94.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4120     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.5    |\n",
            "|    critic_loss     | 4.51     |\n",
            "|    ent_coef        | 0.326    |\n",
            "|    ent_coef_loss   | -3.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4019     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4125, episode_reward=205.30 +/- 0.36\n",
            "Episode length: 93.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.6     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4125     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.2    |\n",
            "|    critic_loss     | 4.05     |\n",
            "|    ent_coef        | 0.326    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4024     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4130, episode_reward=203.04 +/- 2.09\n",
            "Episode length: 94.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4130     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.2    |\n",
            "|    critic_loss     | 8.47     |\n",
            "|    ent_coef        | 0.325    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4029     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4135, episode_reward=209.99 +/- 2.77\n",
            "Episode length: 96.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96       |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4135     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.8    |\n",
            "|    critic_loss     | 5.89     |\n",
            "|    ent_coef        | 0.325    |\n",
            "|    ent_coef_loss   | -3.14    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4034     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4140, episode_reward=224.93 +/- 0.48\n",
            "Episode length: 96.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.8     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4140     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.1    |\n",
            "|    critic_loss     | 5.2      |\n",
            "|    ent_coef        | 0.324    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4039     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4145, episode_reward=208.58 +/- 0.67\n",
            "Episode length: 90.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4145     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46      |\n",
            "|    critic_loss     | 7.86     |\n",
            "|    ent_coef        | 0.324    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4044     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4150, episode_reward=205.98 +/- 2.41\n",
            "Episode length: 91.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.8     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4150     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.8    |\n",
            "|    critic_loss     | 7.89     |\n",
            "|    ent_coef        | 0.323    |\n",
            "|    ent_coef_loss   | -3.82    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4049     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4155, episode_reward=195.39 +/- 2.48\n",
            "Episode length: 88.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.6     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4155     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.8    |\n",
            "|    critic_loss     | 8.34     |\n",
            "|    ent_coef        | 0.323    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4054     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4160, episode_reward=196.63 +/- 1.95\n",
            "Episode length: 86.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.4     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4160     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.6    |\n",
            "|    critic_loss     | 5.65     |\n",
            "|    ent_coef        | 0.323    |\n",
            "|    ent_coef_loss   | -3.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4059     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4165, episode_reward=204.58 +/- 1.19\n",
            "Episode length: 87.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.8     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4165     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.6    |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    ent_coef        | 0.322    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4064     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4170, episode_reward=243.92 +/- 4.52\n",
            "Episode length: 98.80 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.8     |\n",
            "|    mean_reward     | 244      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4170     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.5    |\n",
            "|    critic_loss     | 3.17     |\n",
            "|    ent_coef        | 0.322    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4069     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4175, episode_reward=235.84 +/- 1.77\n",
            "Episode length: 97.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.4     |\n",
            "|    mean_reward     | 236      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4175     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.1    |\n",
            "|    critic_loss     | 3.87     |\n",
            "|    ent_coef        | 0.321    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4074     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4180, episode_reward=200.53 +/- 0.38\n",
            "Episode length: 88.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.2     |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4180     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47      |\n",
            "|    critic_loss     | 4.99     |\n",
            "|    ent_coef        | 0.321    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4079     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4185, episode_reward=200.44 +/- 0.91\n",
            "Episode length: 91.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.4     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4185     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.9    |\n",
            "|    critic_loss     | 7.3      |\n",
            "|    ent_coef        | 0.321    |\n",
            "|    ent_coef_loss   | -3.54    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4084     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4190, episode_reward=208.91 +/- 0.80\n",
            "Episode length: 93.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4190     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.4    |\n",
            "|    critic_loss     | 8.93     |\n",
            "|    ent_coef        | 0.32     |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4089     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4195, episode_reward=215.76 +/- 0.74\n",
            "Episode length: 94.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4195     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47      |\n",
            "|    critic_loss     | 6.63     |\n",
            "|    ent_coef        | 0.32     |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4094     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4200, episode_reward=213.23 +/- 0.61\n",
            "Episode length: 94.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.2    |\n",
            "|    critic_loss     | 6.43     |\n",
            "|    ent_coef        | 0.319    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4099     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4205, episode_reward=200.63 +/- 0.80\n",
            "Episode length: 89.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.8     |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4205     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.1    |\n",
            "|    critic_loss     | 9.85     |\n",
            "|    ent_coef        | 0.319    |\n",
            "|    ent_coef_loss   | -3.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4104     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4210, episode_reward=196.08 +/- 1.33\n",
            "Episode length: 89.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.6     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4210     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.9    |\n",
            "|    critic_loss     | 5.95     |\n",
            "|    ent_coef        | 0.318    |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4109     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4215, episode_reward=208.18 +/- 0.79\n",
            "Episode length: 93.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4215     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.6    |\n",
            "|    critic_loss     | 4.93     |\n",
            "|    ent_coef        | 0.318    |\n",
            "|    ent_coef_loss   | -3.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4114     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4220, episode_reward=226.35 +/- 0.40\n",
            "Episode length: 97.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97       |\n",
            "|    mean_reward     | 226      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4220     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.3    |\n",
            "|    critic_loss     | 3.29     |\n",
            "|    ent_coef        | 0.318    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4119     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4225, episode_reward=219.48 +/- 1.48\n",
            "Episode length: 96.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96       |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4225     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.4    |\n",
            "|    critic_loss     | 4.45     |\n",
            "|    ent_coef        | 0.317    |\n",
            "|    ent_coef_loss   | -3.77    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4124     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4230, episode_reward=204.50 +/- 1.78\n",
            "Episode length: 90.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4230     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.4    |\n",
            "|    critic_loss     | 2.65     |\n",
            "|    ent_coef        | 0.317    |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4129     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4235, episode_reward=203.99 +/- 1.11\n",
            "Episode length: 90.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4235     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.4    |\n",
            "|    critic_loss     | 3.99     |\n",
            "|    ent_coef        | 0.316    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4134     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4240, episode_reward=207.74 +/- 0.66\n",
            "Episode length: 92.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92       |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.8    |\n",
            "|    critic_loss     | 6.78     |\n",
            "|    ent_coef        | 0.316    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4139     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4245, episode_reward=206.34 +/- 0.46\n",
            "Episode length: 92.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4245     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.6    |\n",
            "|    critic_loss     | 3.47     |\n",
            "|    ent_coef        | 0.316    |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4144     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4250, episode_reward=203.30 +/- 0.70\n",
            "Episode length: 91.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.8     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4250     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.2    |\n",
            "|    critic_loss     | 6.85     |\n",
            "|    ent_coef        | 0.315    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4149     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4255, episode_reward=203.28 +/- 0.94\n",
            "Episode length: 91.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.4     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4255     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.4    |\n",
            "|    critic_loss     | 4.92     |\n",
            "|    ent_coef        | 0.315    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4154     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4260, episode_reward=210.28 +/- 0.99\n",
            "Episode length: 91.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.8     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.7    |\n",
            "|    critic_loss     | 7.8      |\n",
            "|    ent_coef        | 0.314    |\n",
            "|    ent_coef_loss   | -3.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4159     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4265, episode_reward=208.59 +/- 1.26\n",
            "Episode length: 89.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.8     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4265     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.8    |\n",
            "|    critic_loss     | 6.59     |\n",
            "|    ent_coef        | 0.314    |\n",
            "|    ent_coef_loss   | -3.93    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4164     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4270, episode_reward=199.31 +/- 0.25\n",
            "Episode length: 87.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87       |\n",
            "|    mean_reward     | 199      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4270     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.7    |\n",
            "|    critic_loss     | 6.92     |\n",
            "|    ent_coef        | 0.314    |\n",
            "|    ent_coef_loss   | -3.6     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4169     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4275, episode_reward=196.15 +/- 0.98\n",
            "Episode length: 87.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.6     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4275     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.8    |\n",
            "|    critic_loss     | 4.56     |\n",
            "|    ent_coef        | 0.313    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4174     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4280, episode_reward=202.43 +/- 0.82\n",
            "Episode length: 90.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 202      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4280     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.6    |\n",
            "|    critic_loss     | 6.86     |\n",
            "|    ent_coef        | 0.313    |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4179     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4285, episode_reward=208.58 +/- 0.72\n",
            "Episode length: 93.40 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.4     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4285     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47      |\n",
            "|    critic_loss     | 3.8      |\n",
            "|    ent_coef        | 0.312    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4184     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4290, episode_reward=209.59 +/- 0.61\n",
            "Episode length: 94.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4290     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.1    |\n",
            "|    critic_loss     | 11.4     |\n",
            "|    ent_coef        | 0.312    |\n",
            "|    ent_coef_loss   | -3.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4189     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4295, episode_reward=206.58 +/- 1.04\n",
            "Episode length: 94.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.4     |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4295     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.8    |\n",
            "|    critic_loss     | 6.33     |\n",
            "|    ent_coef        | 0.312    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4194     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4300, episode_reward=206.93 +/- 0.65\n",
            "Episode length: 94.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.4     |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4300     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.1    |\n",
            "|    critic_loss     | 10.4     |\n",
            "|    ent_coef        | 0.311    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4199     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4305, episode_reward=210.66 +/- 1.02\n",
            "Episode length: 92.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4305     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48      |\n",
            "|    critic_loss     | 5.34     |\n",
            "|    ent_coef        | 0.311    |\n",
            "|    ent_coef_loss   | -3.62    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4204     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4310, episode_reward=217.10 +/- 0.94\n",
            "Episode length: 92.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4310     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.4    |\n",
            "|    critic_loss     | 5.94     |\n",
            "|    ent_coef        | 0.31     |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4209     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4315, episode_reward=215.45 +/- 1.21\n",
            "Episode length: 92.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4315     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.8    |\n",
            "|    critic_loss     | 8.77     |\n",
            "|    ent_coef        | 0.31     |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4214     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4320, episode_reward=211.60 +/- 1.39\n",
            "Episode length: 91.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.6     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4320     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48      |\n",
            "|    critic_loss     | 2.06     |\n",
            "|    ent_coef        | 0.31     |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4219     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4325, episode_reward=212.59 +/- 0.56\n",
            "Episode length: 92.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4325     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.3    |\n",
            "|    critic_loss     | 7.54     |\n",
            "|    ent_coef        | 0.309    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4224     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4330, episode_reward=217.97 +/- 1.22\n",
            "Episode length: 95.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95       |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4330     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.5    |\n",
            "|    critic_loss     | 2.54     |\n",
            "|    ent_coef        | 0.309    |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4229     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4335, episode_reward=220.80 +/- 1.01\n",
            "Episode length: 96.60 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4335     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.3    |\n",
            "|    critic_loss     | 3.87     |\n",
            "|    ent_coef        | 0.308    |\n",
            "|    ent_coef_loss   | -3.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4234     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4340, episode_reward=221.39 +/- 0.65\n",
            "Episode length: 98.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4340     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.7    |\n",
            "|    critic_loss     | 4        |\n",
            "|    ent_coef        | 0.308    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4239     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4345, episode_reward=224.78 +/- 1.19\n",
            "Episode length: 98.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4345     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.1    |\n",
            "|    critic_loss     | 13.1     |\n",
            "|    ent_coef        | 0.308    |\n",
            "|    ent_coef_loss   | -3.43    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4244     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4350, episode_reward=222.63 +/- 1.20\n",
            "Episode length: 95.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.8     |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4350     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.2    |\n",
            "|    critic_loss     | 2.99     |\n",
            "|    ent_coef        | 0.307    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4249     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4355, episode_reward=213.97 +/- 1.66\n",
            "Episode length: 92.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4355     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.1    |\n",
            "|    critic_loss     | 3.71     |\n",
            "|    ent_coef        | 0.307    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4254     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4360, episode_reward=205.41 +/- 0.82\n",
            "Episode length: 90.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4360     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.6    |\n",
            "|    critic_loss     | 5.79     |\n",
            "|    ent_coef        | 0.306    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4259     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4365, episode_reward=200.32 +/- 0.43\n",
            "Episode length: 89.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.2     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4365     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48      |\n",
            "|    critic_loss     | 5.36     |\n",
            "|    ent_coef        | 0.306    |\n",
            "|    ent_coef_loss   | -3.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4264     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4370, episode_reward=202.68 +/- 1.11\n",
            "Episode length: 89.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.8     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4370     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.7    |\n",
            "|    critic_loss     | 7.59     |\n",
            "|    ent_coef        | 0.306    |\n",
            "|    ent_coef_loss   | -3.72    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4269     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4375, episode_reward=213.47 +/- 1.21\n",
            "Episode length: 90.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4375     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.7    |\n",
            "|    critic_loss     | 8.33     |\n",
            "|    ent_coef        | 0.305    |\n",
            "|    ent_coef_loss   | -3.76    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4274     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4380, episode_reward=218.97 +/- 3.01\n",
            "Episode length: 92.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92       |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4380     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.2    |\n",
            "|    critic_loss     | 6.02     |\n",
            "|    ent_coef        | 0.305    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4279     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4385, episode_reward=208.38 +/- 1.28\n",
            "Episode length: 93.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.4     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4385     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.6    |\n",
            "|    critic_loss     | 2.15     |\n",
            "|    ent_coef        | 0.304    |\n",
            "|    ent_coef_loss   | -3.79    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4284     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4390, episode_reward=205.05 +/- 0.71\n",
            "Episode length: 94.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4390     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.9    |\n",
            "|    critic_loss     | 4.51     |\n",
            "|    ent_coef        | 0.304    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4289     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4395, episode_reward=212.61 +/- 0.97\n",
            "Episode length: 95.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4395     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49      |\n",
            "|    critic_loss     | 5.11     |\n",
            "|    ent_coef        | 0.304    |\n",
            "|    ent_coef_loss   | -3.61    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4294     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4400, episode_reward=223.38 +/- 1.42\n",
            "Episode length: 94.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.7    |\n",
            "|    critic_loss     | 6.55     |\n",
            "|    ent_coef        | 0.303    |\n",
            "|    ent_coef_loss   | -3.79    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4299     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4405, episode_reward=248.75 +/- 0.91\n",
            "Episode length: 101.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 102      |\n",
            "|    mean_reward     | 249      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4405     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.8    |\n",
            "|    critic_loss     | 2.7      |\n",
            "|    ent_coef        | 0.303    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4304     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4410, episode_reward=235.57 +/- 0.96\n",
            "Episode length: 98.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.6     |\n",
            "|    mean_reward     | 236      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4410     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.7    |\n",
            "|    critic_loss     | 5.87     |\n",
            "|    ent_coef        | 0.302    |\n",
            "|    ent_coef_loss   | -3.81    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4309     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4415, episode_reward=221.19 +/- 0.89\n",
            "Episode length: 96.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.4     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4415     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.4    |\n",
            "|    critic_loss     | 3.57     |\n",
            "|    ent_coef        | 0.302    |\n",
            "|    ent_coef_loss   | -4.12    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4314     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4420, episode_reward=212.47 +/- 0.55\n",
            "Episode length: 95.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.6     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4420     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.5    |\n",
            "|    critic_loss     | 6.24     |\n",
            "|    ent_coef        | 0.302    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4319     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4425, episode_reward=216.96 +/- 1.09\n",
            "Episode length: 95.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4425     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.4    |\n",
            "|    critic_loss     | 12.5     |\n",
            "|    ent_coef        | 0.301    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4324     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4430, episode_reward=219.93 +/- 0.86\n",
            "Episode length: 94.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4430     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.9    |\n",
            "|    critic_loss     | 2.87     |\n",
            "|    ent_coef        | 0.301    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4329     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4435, episode_reward=220.58 +/- 1.05\n",
            "Episode length: 93.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4435     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.2    |\n",
            "|    critic_loss     | 2.24     |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4334     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4440, episode_reward=218.85 +/- 0.69\n",
            "Episode length: 93.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4440     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.8    |\n",
            "|    critic_loss     | 3.88     |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    ent_coef_loss   | -3.18    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4339     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4445, episode_reward=216.06 +/- 1.57\n",
            "Episode length: 93.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4445     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.4    |\n",
            "|    critic_loss     | 3.64     |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4344     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4450, episode_reward=230.26 +/- 0.73\n",
            "Episode length: 96.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.4     |\n",
            "|    mean_reward     | 230      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4450     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.8    |\n",
            "|    critic_loss     | 12       |\n",
            "|    ent_coef        | 0.299    |\n",
            "|    ent_coef_loss   | -3.54    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4349     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4455, episode_reward=233.05 +/- 1.28\n",
            "Episode length: 97.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.2     |\n",
            "|    mean_reward     | 233      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4455     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.2    |\n",
            "|    critic_loss     | 5.13     |\n",
            "|    ent_coef        | 0.299    |\n",
            "|    ent_coef_loss   | -3.53    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4354     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4460, episode_reward=224.08 +/- 0.69\n",
            "Episode length: 94.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.4     |\n",
            "|    mean_reward     | 224      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4460     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.6    |\n",
            "|    critic_loss     | 5.24     |\n",
            "|    ent_coef        | 0.298    |\n",
            "|    ent_coef_loss   | -3.77    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4359     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4465, episode_reward=214.73 +/- 0.94\n",
            "Episode length: 94.60 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4465     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.6    |\n",
            "|    critic_loss     | 7.11     |\n",
            "|    ent_coef        | 0.298    |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4364     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4470, episode_reward=216.96 +/- 0.71\n",
            "Episode length: 96.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4470     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50      |\n",
            "|    critic_loss     | 6.08     |\n",
            "|    ent_coef        | 0.298    |\n",
            "|    ent_coef_loss   | -3.54    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4369     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4475, episode_reward=228.42 +/- 1.07\n",
            "Episode length: 99.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.6     |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4475     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.5    |\n",
            "|    critic_loss     | 2.89     |\n",
            "|    ent_coef        | 0.297    |\n",
            "|    ent_coef_loss   | -3.72    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4374     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4480, episode_reward=226.19 +/- 1.02\n",
            "Episode length: 98.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.4     |\n",
            "|    mean_reward     | 226      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4480     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.5    |\n",
            "|    critic_loss     | 3.35     |\n",
            "|    ent_coef        | 0.297    |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4379     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4485, episode_reward=216.12 +/- 1.02\n",
            "Episode length: 97.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.4     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4485     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49      |\n",
            "|    critic_loss     | 4.94     |\n",
            "|    ent_coef        | 0.296    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4384     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 70.1     |\n",
            "|    ep_rew_mean     | 126      |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 8        |\n",
            "|    time_elapsed    | 546      |\n",
            "|    total_timesteps | 4487     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.4    |\n",
            "|    critic_loss     | 3.79     |\n",
            "|    ent_coef        | 0.296    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4386     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4490, episode_reward=211.40 +/- 0.43\n",
            "Episode length: 94.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.1    |\n",
            "|    critic_loss     | 6.49     |\n",
            "|    ent_coef        | 0.296    |\n",
            "|    ent_coef_loss   | -3.68    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4389     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4495, episode_reward=208.86 +/- 0.85\n",
            "Episode length: 90.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.4     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4495     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50      |\n",
            "|    critic_loss     | 4.9      |\n",
            "|    ent_coef        | 0.296    |\n",
            "|    ent_coef_loss   | -3.69    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4394     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=208.08 +/- 0.93\n",
            "Episode length: 89.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.8     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4500     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.9    |\n",
            "|    critic_loss     | 5.36     |\n",
            "|    ent_coef        | 0.295    |\n",
            "|    ent_coef_loss   | -3.8     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4399     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4505, episode_reward=204.39 +/- 0.80\n",
            "Episode length: 89.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.4     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4505     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.5    |\n",
            "|    critic_loss     | 3.2      |\n",
            "|    ent_coef        | 0.295    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4404     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4510, episode_reward=205.45 +/- 1.18\n",
            "Episode length: 91.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.4     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4510     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.8    |\n",
            "|    critic_loss     | 10.1     |\n",
            "|    ent_coef        | 0.295    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4409     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4515, episode_reward=209.79 +/- 0.92\n",
            "Episode length: 94.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4515     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 8.87     |\n",
            "|    ent_coef        | 0.294    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4414     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4520, episode_reward=219.56 +/- 0.65\n",
            "Episode length: 97.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.6     |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4520     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.2    |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    ent_coef        | 0.294    |\n",
            "|    ent_coef_loss   | -4.09    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4419     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4525, episode_reward=225.01 +/- 0.78\n",
            "Episode length: 98.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.8     |\n",
            "|    mean_reward     | 225      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4525     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 12.8     |\n",
            "|    ent_coef        | 0.293    |\n",
            "|    ent_coef_loss   | -3.86    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4424     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4530, episode_reward=220.18 +/- 0.73\n",
            "Episode length: 97.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97       |\n",
            "|    mean_reward     | 220      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4530     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.7    |\n",
            "|    critic_loss     | 4.54     |\n",
            "|    ent_coef        | 0.293    |\n",
            "|    ent_coef_loss   | -3.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4429     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4535, episode_reward=210.66 +/- 0.70\n",
            "Episode length: 92.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.4     |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4535     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.9    |\n",
            "|    critic_loss     | 9.7      |\n",
            "|    ent_coef        | 0.293    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4434     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4540, episode_reward=209.85 +/- 0.49\n",
            "Episode length: 92.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 8.05     |\n",
            "|    ent_coef        | 0.292    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4439     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4545, episode_reward=209.17 +/- 1.28\n",
            "Episode length: 91.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.4     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4545     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 5.92     |\n",
            "|    ent_coef        | 0.292    |\n",
            "|    ent_coef_loss   | -3.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4444     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4550, episode_reward=217.12 +/- 1.38\n",
            "Episode length: 94.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4550     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.6    |\n",
            "|    critic_loss     | 5.88     |\n",
            "|    ent_coef        | 0.292    |\n",
            "|    ent_coef_loss   | -3.91    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4449     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4555, episode_reward=221.51 +/- 0.96\n",
            "Episode length: 95.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 222      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4555     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51      |\n",
            "|    critic_loss     | 5.09     |\n",
            "|    ent_coef        | 0.291    |\n",
            "|    ent_coef_loss   | -3.8     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4454     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4560, episode_reward=218.85 +/- 0.24\n",
            "Episode length: 93.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4560     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.1    |\n",
            "|    critic_loss     | 5.75     |\n",
            "|    ent_coef        | 0.291    |\n",
            "|    ent_coef_loss   | -3.62    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4459     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4565, episode_reward=218.51 +/- 1.16\n",
            "Episode length: 93.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.6     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4565     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50      |\n",
            "|    critic_loss     | 7.02     |\n",
            "|    ent_coef        | 0.29     |\n",
            "|    ent_coef_loss   | -3.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4464     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4570, episode_reward=224.05 +/- 1.25\n",
            "Episode length: 96.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 224      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4570     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.5    |\n",
            "|    critic_loss     | 5.5      |\n",
            "|    ent_coef        | 0.29     |\n",
            "|    ent_coef_loss   | -4.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4469     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4575, episode_reward=224.22 +/- 1.30\n",
            "Episode length: 96.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 224      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4575     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 5.8      |\n",
            "|    ent_coef        | 0.29     |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4474     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4580, episode_reward=216.82 +/- 1.49\n",
            "Episode length: 94.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4580     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.9    |\n",
            "|    critic_loss     | 2.35     |\n",
            "|    ent_coef        | 0.289    |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4479     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4585, episode_reward=219.10 +/- 0.70\n",
            "Episode length: 95.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4585     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.8    |\n",
            "|    critic_loss     | 3.77     |\n",
            "|    ent_coef        | 0.289    |\n",
            "|    ent_coef_loss   | -3.77    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4484     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4590, episode_reward=221.40 +/- 1.26\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4590     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.9    |\n",
            "|    critic_loss     | 3.3      |\n",
            "|    ent_coef        | 0.288    |\n",
            "|    ent_coef_loss   | -3.72    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4489     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4595, episode_reward=219.31 +/- 1.60\n",
            "Episode length: 96.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4595     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.2    |\n",
            "|    critic_loss     | 2.82     |\n",
            "|    ent_coef        | 0.288    |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4494     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4600, episode_reward=216.31 +/- 1.08\n",
            "Episode length: 94.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.3    |\n",
            "|    critic_loss     | 6.54     |\n",
            "|    ent_coef        | 0.288    |\n",
            "|    ent_coef_loss   | -3.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4499     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4605, episode_reward=221.26 +/- 1.33\n",
            "Episode length: 95.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.6     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4605     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.4    |\n",
            "|    critic_loss     | 3.95     |\n",
            "|    ent_coef        | 0.287    |\n",
            "|    ent_coef_loss   | -3.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4504     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4610, episode_reward=230.26 +/- 0.56\n",
            "Episode length: 99.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99       |\n",
            "|    mean_reward     | 230      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4610     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.8    |\n",
            "|    critic_loss     | 10.3     |\n",
            "|    ent_coef        | 0.287    |\n",
            "|    ent_coef_loss   | -3.79    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4509     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4615, episode_reward=227.81 +/- 0.51\n",
            "Episode length: 98.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4615     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.8    |\n",
            "|    critic_loss     | 4.48     |\n",
            "|    ent_coef        | 0.287    |\n",
            "|    ent_coef_loss   | -3.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4514     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4620, episode_reward=221.40 +/- 1.40\n",
            "Episode length: 98.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.4     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4620     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.2    |\n",
            "|    critic_loss     | 8.73     |\n",
            "|    ent_coef        | 0.286    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4519     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4625, episode_reward=228.62 +/- 0.75\n",
            "Episode length: 102.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 103      |\n",
            "|    mean_reward     | 229      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4625     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.8    |\n",
            "|    critic_loss     | 7.44     |\n",
            "|    ent_coef        | 0.286    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4524     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4630, episode_reward=229.94 +/- 0.77\n",
            "Episode length: 101.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 102      |\n",
            "|    mean_reward     | 230      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4630     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.3    |\n",
            "|    critic_loss     | 9.12     |\n",
            "|    ent_coef        | 0.285    |\n",
            "|    ent_coef_loss   | -3.61    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4529     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4635, episode_reward=221.41 +/- 1.04\n",
            "Episode length: 98.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.6     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4635     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.6    |\n",
            "|    critic_loss     | 5.76     |\n",
            "|    ent_coef        | 0.285    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4534     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4640, episode_reward=217.58 +/- 0.41\n",
            "Episode length: 96.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4640     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.6    |\n",
            "|    critic_loss     | 6.42     |\n",
            "|    ent_coef        | 0.285    |\n",
            "|    ent_coef_loss   | -3.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4539     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4645, episode_reward=214.40 +/- 0.54\n",
            "Episode length: 94.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4645     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.8    |\n",
            "|    critic_loss     | 3.33     |\n",
            "|    ent_coef        | 0.284    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4544     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4650, episode_reward=207.93 +/- 1.15\n",
            "Episode length: 91.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.8     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4650     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.6    |\n",
            "|    critic_loss     | 4.12     |\n",
            "|    ent_coef        | 0.284    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4549     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4655, episode_reward=205.30 +/- 0.55\n",
            "Episode length: 89.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.6     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4655     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.8    |\n",
            "|    critic_loss     | 3.67     |\n",
            "|    ent_coef        | 0.284    |\n",
            "|    ent_coef_loss   | -3.99    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4554     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4660, episode_reward=207.88 +/- 0.95\n",
            "Episode length: 88.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.8     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4660     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.7    |\n",
            "|    critic_loss     | 4.46     |\n",
            "|    ent_coef        | 0.283    |\n",
            "|    ent_coef_loss   | -3.91    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4559     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4665, episode_reward=209.50 +/- 2.70\n",
            "Episode length: 89.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.6     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4665     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52      |\n",
            "|    critic_loss     | 10.3     |\n",
            "|    ent_coef        | 0.283    |\n",
            "|    ent_coef_loss   | -3.99    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4564     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4670, episode_reward=203.12 +/- 3.48\n",
            "Episode length: 89.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89       |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.2    |\n",
            "|    critic_loss     | 5.66     |\n",
            "|    ent_coef        | 0.282    |\n",
            "|    ent_coef_loss   | -3.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4569     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4675, episode_reward=206.73 +/- 1.06\n",
            "Episode length: 90.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4675     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.4    |\n",
            "|    critic_loss     | 6.95     |\n",
            "|    ent_coef        | 0.282    |\n",
            "|    ent_coef_loss   | -4.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4574     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4680, episode_reward=216.28 +/- 0.58\n",
            "Episode length: 93.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.6     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4680     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.7    |\n",
            "|    critic_loss     | 5.9      |\n",
            "|    ent_coef        | 0.282    |\n",
            "|    ent_coef_loss   | -3.61    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4579     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4685, episode_reward=214.77 +/- 0.09\n",
            "Episode length: 93.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4685     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.3    |\n",
            "|    critic_loss     | 2.38     |\n",
            "|    ent_coef        | 0.281    |\n",
            "|    ent_coef_loss   | -4.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4584     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4690, episode_reward=204.68 +/- 0.80\n",
            "Episode length: 90.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.6     |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4690     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.1    |\n",
            "|    critic_loss     | 5.85     |\n",
            "|    ent_coef        | 0.281    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4589     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4695, episode_reward=203.94 +/- 1.13\n",
            "Episode length: 91.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91       |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4695     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.7    |\n",
            "|    critic_loss     | 7.28     |\n",
            "|    ent_coef        | 0.281    |\n",
            "|    ent_coef_loss   | -3.72    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4594     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4700, episode_reward=206.36 +/- 1.40\n",
            "Episode length: 91.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4700     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.6    |\n",
            "|    critic_loss     | 6.6      |\n",
            "|    ent_coef        | 0.28     |\n",
            "|    ent_coef_loss   | -3.92    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4599     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4705, episode_reward=210.42 +/- 1.16\n",
            "Episode length: 94.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4705     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.1    |\n",
            "|    critic_loss     | 3.55     |\n",
            "|    ent_coef        | 0.28     |\n",
            "|    ent_coef_loss   | -3.88    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4604     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4710, episode_reward=214.55 +/- 0.90\n",
            "Episode length: 95.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.8     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4710     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.1    |\n",
            "|    critic_loss     | 6.15     |\n",
            "|    ent_coef        | 0.279    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4609     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4715, episode_reward=215.12 +/- 1.50\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4715     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.2    |\n",
            "|    critic_loss     | 6.55     |\n",
            "|    ent_coef        | 0.279    |\n",
            "|    ent_coef_loss   | -3.72    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4614     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4720, episode_reward=213.46 +/- 1.14\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.2    |\n",
            "|    critic_loss     | 4.88     |\n",
            "|    ent_coef        | 0.279    |\n",
            "|    ent_coef_loss   | -3.86    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4619     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4725, episode_reward=214.27 +/- 0.74\n",
            "Episode length: 95.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.4     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4725     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.1    |\n",
            "|    critic_loss     | 7.81     |\n",
            "|    ent_coef        | 0.278    |\n",
            "|    ent_coef_loss   | -3.79    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4624     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4730, episode_reward=218.52 +/- 1.82\n",
            "Episode length: 94.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4730     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.1    |\n",
            "|    critic_loss     | 5.87     |\n",
            "|    ent_coef        | 0.278    |\n",
            "|    ent_coef_loss   | -3.9     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4629     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4735, episode_reward=232.65 +/- 3.24\n",
            "Episode length: 100.80 +/- 1.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 101      |\n",
            "|    mean_reward     | 233      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4735     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.6    |\n",
            "|    critic_loss     | 3.64     |\n",
            "|    ent_coef        | 0.278    |\n",
            "|    ent_coef_loss   | -4.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4634     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4740, episode_reward=232.25 +/- 0.54\n",
            "Episode length: 99.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.2     |\n",
            "|    mean_reward     | 232      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4740     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.4    |\n",
            "|    critic_loss     | 4.18     |\n",
            "|    ent_coef        | 0.277    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4639     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4745, episode_reward=220.98 +/- 0.97\n",
            "Episode length: 97.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.4     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4745     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51      |\n",
            "|    critic_loss     | 3.02     |\n",
            "|    ent_coef        | 0.277    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4644     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4750, episode_reward=213.33 +/- 0.80\n",
            "Episode length: 94.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4750     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.8    |\n",
            "|    critic_loss     | 3.17     |\n",
            "|    ent_coef        | 0.276    |\n",
            "|    ent_coef_loss   | -3.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4649     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4755, episode_reward=212.85 +/- 0.92\n",
            "Episode length: 94.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4755     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.1    |\n",
            "|    critic_loss     | 4.8      |\n",
            "|    ent_coef        | 0.276    |\n",
            "|    ent_coef_loss   | -3.53    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4654     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4760, episode_reward=215.50 +/- 1.01\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4760     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.2    |\n",
            "|    critic_loss     | 8.92     |\n",
            "|    ent_coef        | 0.276    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4659     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4765, episode_reward=218.66 +/- 0.48\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 219      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4765     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.7    |\n",
            "|    critic_loss     | 6.26     |\n",
            "|    ent_coef        | 0.275    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4664     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4770, episode_reward=213.80 +/- 0.95\n",
            "Episode length: 92.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.5    |\n",
            "|    critic_loss     | 7.37     |\n",
            "|    ent_coef        | 0.275    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4669     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4775, episode_reward=207.82 +/- 0.94\n",
            "Episode length: 92.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92       |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4775     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.4    |\n",
            "|    critic_loss     | 13.5     |\n",
            "|    ent_coef        | 0.275    |\n",
            "|    ent_coef_loss   | -4.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4674     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4780, episode_reward=214.22 +/- 0.64\n",
            "Episode length: 95.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95       |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4780     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.9    |\n",
            "|    critic_loss     | 4.38     |\n",
            "|    ent_coef        | 0.274    |\n",
            "|    ent_coef_loss   | -3.81    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4679     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4785, episode_reward=218.15 +/- 0.72\n",
            "Episode length: 97.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.2     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4785     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52      |\n",
            "|    critic_loss     | 4.28     |\n",
            "|    ent_coef        | 0.274    |\n",
            "|    ent_coef_loss   | -3.9     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4684     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4790, episode_reward=217.72 +/- 1.33\n",
            "Episode length: 96.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96       |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4790     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.4    |\n",
            "|    critic_loss     | 6.6      |\n",
            "|    ent_coef        | 0.274    |\n",
            "|    ent_coef_loss   | -3.76    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4689     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4795, episode_reward=215.16 +/- 1.39\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4795     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.2    |\n",
            "|    critic_loss     | 4.79     |\n",
            "|    ent_coef        | 0.273    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4694     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4800, episode_reward=215.81 +/- 1.17\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.9    |\n",
            "|    critic_loss     | 4.87     |\n",
            "|    ent_coef        | 0.273    |\n",
            "|    ent_coef_loss   | -3.86    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4699     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4805, episode_reward=218.37 +/- 1.08\n",
            "Episode length: 94.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.8     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4805     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.5    |\n",
            "|    critic_loss     | 3.67     |\n",
            "|    ent_coef        | 0.273    |\n",
            "|    ent_coef_loss   | -4.15    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4704     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4810, episode_reward=213.43 +/- 0.50\n",
            "Episode length: 94.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.4     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4810     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.4    |\n",
            "|    critic_loss     | 4.04     |\n",
            "|    ent_coef        | 0.272    |\n",
            "|    ent_coef_loss   | -3.62    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4709     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4815, episode_reward=217.56 +/- 0.76\n",
            "Episode length: 96.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.4     |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4815     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.8    |\n",
            "|    critic_loss     | 4.52     |\n",
            "|    ent_coef        | 0.272    |\n",
            "|    ent_coef_loss   | -4.07    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4714     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4820, episode_reward=224.35 +/- 0.80\n",
            "Episode length: 98.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 224      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4820     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.7    |\n",
            "|    critic_loss     | 4.42     |\n",
            "|    ent_coef        | 0.271    |\n",
            "|    ent_coef_loss   | -3.8     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4719     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4825, episode_reward=221.42 +/- 1.29\n",
            "Episode length: 97.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.6     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4825     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.9    |\n",
            "|    critic_loss     | 4.26     |\n",
            "|    ent_coef        | 0.271    |\n",
            "|    ent_coef_loss   | -4.03    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4724     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4830, episode_reward=210.22 +/- 0.71\n",
            "Episode length: 94.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.2     |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4830     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52      |\n",
            "|    critic_loss     | 4.9      |\n",
            "|    ent_coef        | 0.271    |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4729     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4835, episode_reward=204.43 +/- 0.88\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4835     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.4    |\n",
            "|    critic_loss     | 3.98     |\n",
            "|    ent_coef        | 0.27     |\n",
            "|    ent_coef_loss   | -3.89    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4734     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4840, episode_reward=207.66 +/- 0.90\n",
            "Episode length: 92.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 208      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4840     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.3    |\n",
            "|    critic_loss     | 7.99     |\n",
            "|    ent_coef        | 0.27     |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4739     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4845, episode_reward=213.46 +/- 2.15\n",
            "Episode length: 94.60 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4845     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.7    |\n",
            "|    critic_loss     | 3.77     |\n",
            "|    ent_coef        | 0.27     |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4744     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4850, episode_reward=221.27 +/- 1.42\n",
            "Episode length: 96.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.5    |\n",
            "|    critic_loss     | 2.96     |\n",
            "|    ent_coef        | 0.269    |\n",
            "|    ent_coef_loss   | -4.09    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4749     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4855, episode_reward=221.24 +/- 0.72\n",
            "Episode length: 96.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4855     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.8    |\n",
            "|    critic_loss     | 8.77     |\n",
            "|    ent_coef        | 0.269    |\n",
            "|    ent_coef_loss   | -3.77    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4754     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4860, episode_reward=212.07 +/- 0.87\n",
            "Episode length: 92.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.2     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4860     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.8    |\n",
            "|    critic_loss     | 8.22     |\n",
            "|    ent_coef        | 0.269    |\n",
            "|    ent_coef_loss   | -3.9     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4759     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 71.5     |\n",
            "|    ep_rew_mean     | 132      |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 8        |\n",
            "|    time_elapsed    | 593      |\n",
            "|    total_timesteps | 4864     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.9    |\n",
            "|    critic_loss     | 5.54     |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -4.03    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4763     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4865, episode_reward=208.95 +/- 0.62\n",
            "Episode length: 90.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4865     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.7    |\n",
            "|    critic_loss     | 5.99     |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -3.93    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4764     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4870, episode_reward=216.06 +/- 2.42\n",
            "Episode length: 93.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4870     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.5    |\n",
            "|    critic_loss     | 3.8      |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -4.08    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4769     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4875, episode_reward=229.05 +/- 2.79\n",
            "Episode length: 102.00 +/- 1.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 102      |\n",
            "|    mean_reward     | 229      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4875     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.6    |\n",
            "|    critic_loss     | 7.56     |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -3.89    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4774     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4880, episode_reward=220.95 +/- 0.61\n",
            "Episode length: 98.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98       |\n",
            "|    mean_reward     | 221      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4880     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.7    |\n",
            "|    critic_loss     | 5.21     |\n",
            "|    ent_coef        | 0.267    |\n",
            "|    ent_coef_loss   | -4.06    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4779     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4885, episode_reward=211.55 +/- 0.95\n",
            "Episode length: 95.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.6     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4885     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.3    |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.267    |\n",
            "|    ent_coef_loss   | -3.97    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4784     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4890, episode_reward=211.51 +/- 1.46\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4890     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.2    |\n",
            "|    critic_loss     | 5.33     |\n",
            "|    ent_coef        | 0.266    |\n",
            "|    ent_coef_loss   | -3.95    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4789     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4895, episode_reward=214.21 +/- 0.98\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4895     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.2    |\n",
            "|    critic_loss     | 3.32     |\n",
            "|    ent_coef        | 0.266    |\n",
            "|    ent_coef_loss   | -3.61    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4794     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4900, episode_reward=212.82 +/- 0.70\n",
            "Episode length: 92.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.6     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.3    |\n",
            "|    critic_loss     | 5.2      |\n",
            "|    ent_coef        | 0.266    |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4799     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4905, episode_reward=214.07 +/- 1.41\n",
            "Episode length: 93.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.4     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4905     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.1    |\n",
            "|    critic_loss     | 6.02     |\n",
            "|    ent_coef        | 0.265    |\n",
            "|    ent_coef_loss   | -4       |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4804     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4910, episode_reward=211.67 +/- 2.25\n",
            "Episode length: 93.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4910     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.7    |\n",
            "|    critic_loss     | 7.76     |\n",
            "|    ent_coef        | 0.265    |\n",
            "|    ent_coef_loss   | -4.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4809     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4915, episode_reward=212.57 +/- 0.56\n",
            "Episode length: 93.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.8     |\n",
            "|    mean_reward     | 213      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4915     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.4    |\n",
            "|    critic_loss     | 5.18     |\n",
            "|    ent_coef        | 0.265    |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4814     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4920, episode_reward=216.59 +/- 1.58\n",
            "Episode length: 94.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 217      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4920     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.7    |\n",
            "|    critic_loss     | 2.8      |\n",
            "|    ent_coef        | 0.264    |\n",
            "|    ent_coef_loss   | -3.84    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4819     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4925, episode_reward=225.72 +/- 2.53\n",
            "Episode length: 98.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.2     |\n",
            "|    mean_reward     | 226      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4925     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.7    |\n",
            "|    critic_loss     | 9.51     |\n",
            "|    ent_coef        | 0.264    |\n",
            "|    ent_coef_loss   | -4.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4824     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4930, episode_reward=216.20 +/- 0.44\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 216      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4930     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.2    |\n",
            "|    critic_loss     | 3.8      |\n",
            "|    ent_coef        | 0.264    |\n",
            "|    ent_coef_loss   | -4.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4829     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4935, episode_reward=214.69 +/- 1.20\n",
            "Episode length: 95.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.2     |\n",
            "|    mean_reward     | 215      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4935     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.9    |\n",
            "|    critic_loss     | 8.59     |\n",
            "|    ent_coef        | 0.263    |\n",
            "|    ent_coef_loss   | -3.82    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4834     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4940, episode_reward=206.98 +/- 0.84\n",
            "Episode length: 93.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4940     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.4    |\n",
            "|    critic_loss     | 4.33     |\n",
            "|    ent_coef        | 0.263    |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4839     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4945, episode_reward=200.50 +/- 0.82\n",
            "Episode length: 90.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.2     |\n",
            "|    mean_reward     | 201      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4945     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52      |\n",
            "|    critic_loss     | 3.65     |\n",
            "|    ent_coef        | 0.263    |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4844     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4950, episode_reward=199.12 +/- 0.90\n",
            "Episode length: 87.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.2     |\n",
            "|    mean_reward     | 199      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.3    |\n",
            "|    critic_loss     | 4.36     |\n",
            "|    ent_coef        | 0.262    |\n",
            "|    ent_coef_loss   | -4.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4849     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4955, episode_reward=207.43 +/- 1.72\n",
            "Episode length: 90.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.4     |\n",
            "|    mean_reward     | 207      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4955     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.1    |\n",
            "|    critic_loss     | 4.76     |\n",
            "|    ent_coef        | 0.262    |\n",
            "|    ent_coef_loss   | -4.13    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4854     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4960, episode_reward=217.66 +/- 1.73\n",
            "Episode length: 97.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97       |\n",
            "|    mean_reward     | 218      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4960     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.4    |\n",
            "|    critic_loss     | 15.8     |\n",
            "|    ent_coef        | 0.262    |\n",
            "|    ent_coef_loss   | -3.99    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4859     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4965, episode_reward=223.48 +/- 0.89\n",
            "Episode length: 103.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 103      |\n",
            "|    mean_reward     | 223      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4965     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.3    |\n",
            "|    critic_loss     | 4.84     |\n",
            "|    ent_coef        | 0.261    |\n",
            "|    ent_coef_loss   | -3.8     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4864     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4970, episode_reward=205.98 +/- 1.24\n",
            "Episode length: 94.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4970     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.5    |\n",
            "|    critic_loss     | 4.58     |\n",
            "|    ent_coef        | 0.261    |\n",
            "|    ent_coef_loss   | -4.08    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4869     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4975, episode_reward=195.49 +/- 1.48\n",
            "Episode length: 90.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.6     |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4975     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.4    |\n",
            "|    critic_loss     | 4.74     |\n",
            "|    ent_coef        | 0.261    |\n",
            "|    ent_coef_loss   | -4.07    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4874     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4980, episode_reward=196.32 +/- 1.37\n",
            "Episode length: 91.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.1    |\n",
            "|    critic_loss     | 4.8      |\n",
            "|    ent_coef        | 0.26     |\n",
            "|    ent_coef_loss   | -3.75    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4879     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4985, episode_reward=203.73 +/- 1.07\n",
            "Episode length: 93.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4985     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.2    |\n",
            "|    critic_loss     | 5.47     |\n",
            "|    ent_coef        | 0.26     |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4884     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4990, episode_reward=208.59 +/- 0.74\n",
            "Episode length: 94.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94.6     |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4990     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.9    |\n",
            "|    critic_loss     | 9.19     |\n",
            "|    ent_coef        | 0.26     |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4889     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4995, episode_reward=211.47 +/- 0.74\n",
            "Episode length: 96.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.2     |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4995     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.1    |\n",
            "|    critic_loss     | 4.09     |\n",
            "|    ent_coef        | 0.259    |\n",
            "|    ent_coef_loss   | -3.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4894     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=5000, episode_reward=211.63 +/- 1.08\n",
            "Episode length: 95.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95.6     |\n",
            "|    mean_reward     | 212      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 5000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.5    |\n",
            "|    critic_loss     | 2.78     |\n",
            "|    ent_coef        | 0.259    |\n",
            "|    ent_coef_loss   | -4.18    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4899     |\n",
            "---------------------------------\n",
            "Modello salvato come ./sim2real/models/sac_hopper.zip\n",
            "/usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "given by the platformdirs library.  To remove this warning and\n",
            "see the appropriate new directories, set the environment variable\n",
            "`JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "The use of platformdirs will be the default in `jupyter_core` v6\n",
            "  from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n",
            "Figure(1000x600)\n",
            "Reward plot saved in ./sim2real/plots/rewards_plot.png\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3**"
      ],
      "metadata": {
        "id": "F1A4wHFZIoSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name sac_hopper_source_5000ts --env source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKXEOnK2It8A",
        "outputId": "711c01b2-b49e-4180-d41a-86e7d0dbc912"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-10 17:13:50.457656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-10 17:13:50.482839: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-10 17:13:50.490240: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-10 17:13:50.507809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-10 17:13:51.898424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task3.py\", line 88, in <module>\n",
            "    main()\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task3.py\", line 71, in main\n",
            "    model= SAC.load(model_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\", line 680, in load\n",
            "    data, params, pytorch_variables = load_from_zip_file(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py\", line 403, in load_from_zip_file\n",
            "    file = open_path(load_path, \"r\", verbose=verbose, suffix=\"zip\")\n",
            "  File \"/usr/lib/python3.10/functools.py\", line 889, in wrapper\n",
            "    return dispatch(args[0].__class__)(*args, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py\", line 240, in open_path_str\n",
            "    return open_path_pathlib(pathlib.Path(path), mode, verbose, suffix)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py\", line 291, in open_path_pathlib\n",
            "    return open_path_pathlib(path, mode, verbose, suffix)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py\", line 272, in open_path_pathlib\n",
            "    raise error\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/save_util.py\", line 264, in open_path_pathlib\n",
            "    return open_path(path.open(\"rb\"), mode, verbose, suffix)\n",
            "  File \"/usr/lib/python3.10/pathlib.py\", line 1119, in open\n",
            "    return self._accessor.open(self, mode, buffering, encoding, errors,\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'sim2real/models/sac_hopper_source_5000ts.zip'\n"
          ]
        }
      ]
    }
  ]
}