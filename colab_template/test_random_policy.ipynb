{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PolitoVandal/project-sim2real-rialti-giunti-gjinaj/blob/main/colab_template/test_random_policy.ipynb)"
   ],
   "metadata": {
    "id": "h9EafyX8NLlA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf \"project-sim2real-rialti-giunti-gjinaj\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!rm -rf \"sim2real\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!rm -rf \"sample_data\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!git clone https://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@github.com/PolitoVandal/project-sim2real-rialti-giunti-gjinaj\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "if not os.path.exists('sim2realtmp/models'):\n",
    "    os.makedirs('sim2realtmp/models')\n",
    "\n",
    "if not os.path.exists('sim2realtmp/plots'):\n",
    "    os.makedirs('sim2realtmp/plots')"
   ],
   "metadata": {
    "id": "UG4ZM2KG_SLS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
<<<<<<< HEAD
    "outputId": "bdf587d6-f886-4777-c961-c4de212b9f53",
    "collapsed": true
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AW6XT0jSJI8e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fb0ba1ff-a1c9-4e27-b6ba-5bb0637ef0a1",
    "collapsed": true
   },
   "source": [
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "!apt-get install -y xvfb ffmpeg\n",
    "\n",
    "!pip install gym\n",
    "!pip install free-mujoco-py\n",
    "!pip install importlib-metadata\n",
    "!pip install shimmy\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install pyvirtualdisplay"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the custom Hopper environment and provided util functions\n",
    "\n",
    "\n",
    "\n",
    "1.   Upload `custom_hopper.zip` to the current session's file storage\n",
    "2.   Un-zip it by running cell below\n"
   ],
   "metadata": {
    "id": "gwIRXGd5K3xJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**\n"
   ],
   "metadata": {
    "id": "nUs5gQXCaiRS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('project-sim2real-rialti-giunti-gjinaj/colab_template')\n",
    "!unzip custom_hopper.zip"
   ],
   "metadata": {
    "id": "T9WsofDVLaCC",
    "colab": {
     "base_uri": "https://localhost:8080/"
=======
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"project-sim2real-rialti-giunti-gjinaj\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sim2real\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sample_data\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!git clone https://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@github.com/PolitoVandal/project-sim2real-rialti-giunti-gjinaj\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "if not os.path.exists('sim2realtmp/models'):\n",
        "    os.makedirs('sim2realtmp/models')\n",
        "\n",
        "if not os.path.exists('sim2realtmp/plots'):\n",
        "    os.makedirs('sim2realtmp/plots')"
      ],
      "metadata": {
        "id": "UG4ZM2KG_SLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fab3b9-4456-4b8f-8981-b808eb13e290",
        "collapsed": true
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project-sim2real-rialti-giunti-gjinaj'...\n",
            "remote: Enumerating objects: 392, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 392 (delta 54), reused 53 (delta 36), pack-reused 310 (from 2)\u001b[K\n",
            "Receiving objects: 100% (392/392), 16.75 MiB | 21.77 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AW6XT0jSJI8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9117ac-fecc-45c8-fa3e-85b43dc8837a",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "The following additional packages will be installed:\n",
            "  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "The following NEW packages will be installed:\n",
            "  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "  libosmesa6-dev\n",
            "0 upgraded, 15 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,013 kB of archives.\n",
            "After this operation, 19.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.3 [3,115 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [8,984 B]\n",
            "Fetched 4,013 kB in 0s (12.4 MB/s)\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../13-libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../14-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  patchelf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 72.1 kB of archives.\n",
            "After this operation, 186 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Fetched 72.1 kB in 0s (576 kB/s)\n",
            "Selecting previously unselected package patchelf.\n",
            "(Reading database ... 123772 files and directories currently installed.)\n",
            "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 7,815 kB of archives.\n",
            "After this operation, 11.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.12 [28.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.12 [864 kB]\n",
            "Fetched 7,815 kB in 0s (23.4 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 123778 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.12_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.12_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting free-mujoco-py\n",
            "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl.metadata (586 bytes)\n",
            "Collecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.1)\n",
            "Collecting fasteners==0.15 (from free-mujoco-py)\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.36.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.17.0)\n",
            "Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (11.1.0)\n",
            "Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
            "Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.11\n",
            "    Uninstalling Cython-3.0.11:\n",
            "      Successfully uninstalled Cython-3.0.11\n",
            "Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.21.0)\n",
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0a1 (from shimmy)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0a1->shimmy)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, shimmy\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 shimmy-2.0.0\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.69.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.4.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py, stable-baselines3\n",
            "Successfully installed ale-py-0.10.1 stable-baselines3-2.4.1\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "!apt-get install -y xvfb ffmpeg\n",
        "\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!pip install importlib-metadata\n",
        "!pip install shimmy\n",
        "!pip install stable-baselines3[extra]\n",
        "!pip install pyvirtualdisplay"
      ]
>>>>>>> f42d68bfcd91b5c6e025235d44a46817b6b43650
    },
    "outputId": "3d37a0d6-c49d-4f1a-b8b8-de3af35228ac",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "7pJC_JevLf1f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Test a random policy on the Gym Hopper environment**\n",
    "\n",
    "\\\n",
    "\n",
    "\n",
    "\n",
    "Play around with this code to get familiar with the\n",
    "Hopper environment.\n",
    "\n",
    "For example, what happens if you don't reset the environment\n",
    "even after the episode is over?\n",
    "When exactly is the episode over?\n",
    "What is an action here?"
   ],
   "metadata": {
    "id": "W4NsuF6pJPVJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from env.custom_hopper import *"
   ],
   "metadata": {
    "id": "uTYmUufrJTNl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e861c60a-d232-4e8a-9551-1c8265f63dd0",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make('CustomHopper-source-v0')\n",
    "# env = gym.make('CustomHopper-target-v0')\n",
    "print('State space:', env.observation_space)  # state-space\n",
    "print('Action space:', env.action_space)  # action-space\n",
    "print(\"Mass values of each link:\", env.model.body_mass)\n",
    "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper\n",
    "print(\"Bodies defined in the environment:\", env.model.body_names)\n",
    "print(\"Number of degrees of freedom (DoFs) of the robot:\", env.model.nv)\n",
    "print(\"Number of DoFs for each body:\", env.model.body_dofnum)\n",
    "print(\"Number of actuators:\", env.model.nu)"
   ],
   "metadata": {
    "id": "QcCfCGg-Jyc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a3b4fde-dc9c-4019-98e6-fe8091230afd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of the Hopper Environment in MuJoCo\n",
    "\n",
    "This document provides a detailed analysis of the Hopper environment based on the provided terminal output and information from the MuJoCo and Gym documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.1: What is the state space in the Hopper environment? Is it discrete or continuous?**\n",
    "\n",
    "### **Answer:**\n",
    "- **State Space Description**:\n",
    "  The state space is represented by a **Box** object with shape `(11,)`, meaning it is a vector of 11 continuous values. These values typically include the positions, velocities, and potentially other sensory data (like contact forces) of the Hopper's components.\n",
    "  \n",
    "- **Nature of State Space**:\n",
    "  The state space is **continuous**, as indicated by the range `(-inf, inf)` and the use of the `Box` object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.2: What is the action space in the Hopper environment? Is it discrete or continuous?**\n",
    "\n",
    "### **Answer:**\n",
    "- **Action Space Description**:\n",
    "  The action space is represented by a **Box** object with shape `(3,)`, meaning it consists of 3 continuous values. These correspond to the torques applied to the actuators controlling the Hopper's joints.\n",
    "\n",
    "- **Nature of Action Space**:\n",
    "  The action space is **continuous**, as indicated by the range `(-1.0, 1.0)` and the use of the `Box` object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.3: What is the mass value of each link of the Hopper environment, in the source and target variants respectively?**\n",
    "\n",
    "### **Answer:**\n",
    "- **Mass Values for the Source Variant**:\n",
    "  From the terminal output:\n",
    "Mass values of each link: [0. 2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
    "These correspond to the masses of the bodies:\n",
    "- `world`: 0.0 (fixed reference point)\n",
    "- `torso`: 2.5343\n",
    "- `thigh`: 3.9270\n",
    "- `leg`: 2.7143\n",
    "- `foot`: 5.0894\n",
    "\n",
    "- **Mass Values for the Target Variant**:\n",
    "The mass values for the target variant can be obtained by switching the environment initialization to:\n",
    "```python\n",
    "env = gym.make('CustomHopper-target-v0')\n",
    "(Ensure to re-run the relevant command to print the mass values.)\n",
    "\n",
    "Comparison of Source and Target Variants: Any differences in mass values between the source and target variants must be explicitly checked in the respective initialization. These differences typically simulate dynamics variability to test robustness.\n",
    "\n",
    "Additional Information Derived from the Environment\n",
    "Bodies Defined in the Environment:\n",
    "\n",
    "('world', 'torso', 'thigh', 'leg', 'foot')\n",
    "These represent the main components of the Hopper system.\n",
    "\n",
    "Number of Degrees of Freedom (DoFs) of the Robot: 6\n",
    "This includes translational and rotational movements of the robot.\n",
    "\n",
    "Number of DoFs for Each Body:\n",
    "[0 3 1 1 1]\n",
    "world: 0 (fixed body)\n",
    "torso: 3\n",
    "thigh: 1\n",
    "leg: 1\n",
    "foot: 1\n",
    "\n",
    "Number of Actuators:\n",
    "The Hopper has 3 actuators controlling its joints.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The Hopper environment features a continuous state and action space, making it well-suited for reinforcement learning tasks. Understanding the dynamics, including body masses and degrees of freedom, is crucial for designing robust controllers and algorithms."
   ],
   "metadata": {
    "id": "AYhXnui3h70A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "  done = False\n",
    "  observation = env.reset()\t # Reset environment to initial state\n",
    "\n",
    "  while not done:  # Until the episode is over\n",
    "\n",
    "    action = env.action_space.sample()\t# Sample random action\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\t# Step the simulator to the next timestep"
   ],
   "metadata": {
    "id": "DT1oXr8HJ05h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 2**"
   ],
   "metadata": {
    "id": "Dfas8tGOax8E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task2.py --model_name df_JR --total_timesteps 100000 --env CustomHopper-target-v0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKi913Qgj2xI",
    "outputId": "ad27e56e-3628-4070-c480-110cbdea7612",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 3**\n",
    "\n",
    "Task 3 Train two agents with your algorithm of choice, on the source and target domains respectively. Then,\n",
    "test each model and report its average return over 50 test episodes. In particular, report results for the\n",
    "following “training→test” configurations:\n",
    "● source→source,\n",
    "● source→target (lower bound),\n",
    "● target→target (upper bound).\n",
    "Test with different hyperparameters and report the best results found together with the parameters used.\n",
    "Question 3.1 Why do we expect lower performances from the “source→target” configuration w.r.t. the\n",
    "“target→target”?\n",
    "Question 3.2 If higher performances can be reached by training on the target environment directly, what\n",
    "prevents us from doing so (in a sim-to-real setting)?\n"
   ],
   "metadata": {
    "id": "F1A4wHFZIoSH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Source->Source"
   ],
   "metadata": {
    "id": "CHD2OM2zVvq8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_100k --env source"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
<<<<<<< HEAD
    "id": "GKXEOnK2It8A",
    "outputId": "9f7d1888-26e0-42c5-c822-162fbd4b12f1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Source -> Target"
   ],
   "metadata": {
    "id": "jBRkzZQ2VzTP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_100k --env target"
   ],
   "metadata": {
    "id": "i3eCsyBAO0_T",
    "outputId": "32004be7-ea20-41e0-fe86-7987febe65c4",
    "colab": {
     "base_uri": "https://localhost:8080/"
=======
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Test a random policy on the Gym Hopper environment**\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "Play around with this code to get familiar with the\n",
        "Hopper environment.\n",
        "\n",
        "For example, what happens if you don't reset the environment\n",
        "even after the episode is over?\n",
        "When exactly is the episode over?\n",
        "What is an action here?"
      ],
      "metadata": {
        "id": "W4NsuF6pJPVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from env.custom_hopper import *"
      ],
      "metadata": {
        "id": "uTYmUufrJTNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e861c60a-d232-4e8a-9551-1c8265f63dd0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:running build_ext\n",
            "INFO:root:building 'mujoco_py.cymj' extension\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n",
            "INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CustomHopper-source-v0')\n",
        "# env = gym.make('CustomHopper-target-v0')\n",
        "print('State space:', env.observation_space)  # state-space\n",
        "print('Action space:', env.action_space)  # action-space\n",
        "print(\"Mass values of each link:\", env.model.body_mass)\n",
        "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper\n",
        "print(\"Bodies defined in the environment:\", env.model.body_names)\n",
        "print(\"Number of degrees of freedom (DoFs) of the robot:\", env.model.nv)\n",
        "print(\"Number of DoFs for each body:\", env.model.body_dofnum)\n",
        "print(\"Number of actuators:\", env.model.nu)"
      ],
      "metadata": {
        "id": "QcCfCGg-Jyc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3b4fde-dc9c-4019-98e6-fe8091230afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Mass values of each link: [0.         2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Bodies defined in the environment: ('world', 'torso', 'thigh', 'leg', 'foot')\n",
            "Number of degrees of freedom (DoFs) of the robot: 6\n",
            "Number of DoFs for each body: [0 3 1 1 1]\n",
            "Number of actuators: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Hopper Environment in MuJoCo\n",
        "\n",
        "This document provides a detailed analysis of the Hopper environment based on the provided terminal output and information from the MuJoCo and Gym documentation.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.1: What is the state space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **State Space Description**:\n",
        "  The state space is represented by a **Box** object with shape `(11,)`, meaning it is a vector of 11 continuous values. These values typically include the positions, velocities, and potentially other sensory data (like contact forces) of the Hopper's components.\n",
        "  \n",
        "- **Nature of State Space**:\n",
        "  The state space is **continuous**, as indicated by the range `(-inf, inf)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.2: What is the action space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Action Space Description**:\n",
        "  The action space is represented by a **Box** object with shape `(3,)`, meaning it consists of 3 continuous values. These correspond to the torques applied to the actuators controlling the Hopper's joints.\n",
        "\n",
        "- **Nature of Action Space**:\n",
        "  The action space is **continuous**, as indicated by the range `(-1.0, 1.0)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.3: What is the mass value of each link of the Hopper environment, in the source and target variants respectively?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Mass Values for the Source Variant**:\n",
        "  From the terminal output:\n",
        "Mass values of each link: [0. 2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
        "These correspond to the masses of the bodies:\n",
        "- `world`: 0.0 (fixed reference point)\n",
        "- `torso`: 2.5343\n",
        "- `thigh`: 3.9270\n",
        "- `leg`: 2.7143\n",
        "- `foot`: 5.0894\n",
        "\n",
        "- **Mass Values for the Target Variant**:\n",
        "The mass values for the target variant can be obtained by switching the environment initialization to:\n",
        "```python\n",
        "env = gym.make('CustomHopper-target-v0')\n",
        "(Ensure to re-run the relevant command to print the mass values.)\n",
        "\n",
        "Comparison of Source and Target Variants: Any differences in mass values between the source and target variants must be explicitly checked in the respective initialization. These differences typically simulate dynamics variability to test robustness.\n",
        "\n",
        "Additional Information Derived from the Environment\n",
        "Bodies Defined in the Environment:\n",
        "\n",
        "('world', 'torso', 'thigh', 'leg', 'foot')\n",
        "These represent the main components of the Hopper system.\n",
        "\n",
        "Number of Degrees of Freedom (DoFs) of the Robot: 6\n",
        "This includes translational and rotational movements of the robot.\n",
        "\n",
        "Number of DoFs for Each Body:\n",
        "[0 3 1 1 1]\n",
        "world: 0 (fixed body)\n",
        "torso: 3\n",
        "thigh: 1\n",
        "leg: 1\n",
        "foot: 1\n",
        "\n",
        "Number of Actuators:\n",
        "The Hopper has 3 actuators controlling its joints.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Hopper environment features a continuous state and action space, making it well-suited for reinforcement learning tasks. Understanding the dynamics, including body masses and degrees of freedom, is crucial for designing robust controllers and algorithms."
      ],
      "metadata": {
        "id": "AYhXnui3h70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 5\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "  done = False\n",
        "  observation = env.reset()\t # Reset environment to initial state\n",
        "\n",
        "  while not done:  # Until the episode is over\n",
        "\n",
        "    action = env.action_space.sample()\t# Sample random action\n",
        "\n",
        "    observation, reward, done, info = env.step(action)\t# Step the simulator to the next timestep"
      ],
      "metadata": {
        "id": "DT1oXr8HJ05h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "Dfas8tGOax8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task2.py --model_name df_JR --total_timesteps 100000 --env CustomHopper-target-v0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKi913Qgj2xI",
        "outputId": "ad27e56e-3628-4070-c480-110cbdea7612",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mOutput streaming troncato alle ultime 5000 righe.\u001b[0m\n",
            "Eval num_timesteps=90960, episode_reward=708.72 +/- 29.81\n",
            "Episode length: 230.60 +/- 10.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 231      |\n",
            "|    mean_reward     | 709      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.88     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.188   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 90859    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90990, episode_reward=691.11 +/- 181.24\n",
            "Episode length: 234.40 +/- 45.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 234      |\n",
            "|    mean_reward     | 691      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90990    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.65     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | 0.44     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 90889    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91020, episode_reward=746.90 +/- 54.14\n",
            "Episode length: 260.00 +/- 18.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 260      |\n",
            "|    mean_reward     | 747      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.46     |\n",
            "|    ent_coef        | 0.0594   |\n",
            "|    ent_coef_loss   | 0.165    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 90919    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91050, episode_reward=670.74 +/- 3.95\n",
            "Episode length: 216.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 671      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91050    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 4.68     |\n",
            "|    ent_coef        | 0.0594   |\n",
            "|    ent_coef_loss   | -0.586   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 90949    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91080, episode_reward=671.70 +/- 2.05\n",
            "Episode length: 220.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 672      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91080    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -171     |\n",
            "|    critic_loss     | 3.97     |\n",
            "|    ent_coef        | 0.0592   |\n",
            "|    ent_coef_loss   | -0.901   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 90979    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91110, episode_reward=747.20 +/- 19.77\n",
            "Episode length: 258.40 +/- 5.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 747      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91110    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -171     |\n",
            "|    critic_loss     | 4.44     |\n",
            "|    ent_coef        | 0.0591   |\n",
            "|    ent_coef_loss   | 0.0599   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91009    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91140, episode_reward=899.66 +/- 224.95\n",
            "Episode length: 315.60 +/- 73.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 316      |\n",
            "|    mean_reward     | 900      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.69     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | 0.171    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91039    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91170, episode_reward=753.52 +/- 8.19\n",
            "Episode length: 263.80 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 754      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91170    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.52     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.171   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91069    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91200, episode_reward=545.60 +/- 31.77\n",
            "Episode length: 212.20 +/- 6.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 546      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 8.17     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | 0.842    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91099    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91230, episode_reward=843.20 +/- 41.68\n",
            "Episode length: 287.20 +/- 12.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | 843      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91230    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.19     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | -0.646   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91129    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91260, episode_reward=704.63 +/- 8.11\n",
            "Episode length: 228.40 +/- 2.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 228      |\n",
            "|    mean_reward     | 705      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91260    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.43     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | 0.609    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91159    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 238      |\n",
            "|    ep_rew_mean     | 701      |\n",
            "| time/              |          |\n",
            "|    episodes        | 592      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 5581     |\n",
            "|    total_timesteps | 91262    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 6.73     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | 0.292    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91161    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91290, episode_reward=528.62 +/- 5.41\n",
            "Episode length: 207.40 +/- 3.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 207      |\n",
            "|    mean_reward     | 529      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.89     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.584   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91189    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91320, episode_reward=822.20 +/- 39.31\n",
            "Episode length: 268.80 +/- 10.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 822      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.59     |\n",
            "|    ent_coef        | 0.0596   |\n",
            "|    ent_coef_loss   | -0.272   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91219    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91350, episode_reward=661.24 +/- 180.81\n",
            "Episode length: 247.80 +/- 50.78\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 248      |\n",
            "|    mean_reward     | 661      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91350    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.44     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.245   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91249    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91380, episode_reward=710.97 +/- 8.33\n",
            "Episode length: 262.40 +/- 1.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 711      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | 1.42     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91279    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91410, episode_reward=686.42 +/- 11.77\n",
            "Episode length: 220.40 +/- 3.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 686      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91410    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    ent_coef        | 0.0597   |\n",
            "|    ent_coef_loss   | -0.359   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91309    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91440, episode_reward=687.15 +/- 77.74\n",
            "Episode length: 240.00 +/- 32.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 240      |\n",
            "|    mean_reward     | 687      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91440    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    ent_coef        | 0.0597   |\n",
            "|    ent_coef_loss   | 0.0462   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91339    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91470, episode_reward=754.76 +/- 46.12\n",
            "Episode length: 252.00 +/- 12.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 755      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91470    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3        |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.0818  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91369    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91500, episode_reward=783.90 +/- 17.90\n",
            "Episode length: 272.80 +/- 2.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 784      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91500    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.39     |\n",
            "|    ent_coef        | 0.0595   |\n",
            "|    ent_coef_loss   | -0.121   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91399    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91530, episode_reward=966.28 +/- 116.26\n",
            "Episode length: 338.00 +/- 49.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 338      |\n",
            "|    mean_reward     | 966      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91530    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.0594   |\n",
            "|    ent_coef_loss   | -0.514   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91429    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91560, episode_reward=776.57 +/- 125.08\n",
            "Episode length: 249.20 +/- 39.07\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 249      |\n",
            "|    mean_reward     | 777      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91560    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 4.3      |\n",
            "|    ent_coef        | 0.0594   |\n",
            "|    ent_coef_loss   | 0.321    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91459    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91590, episode_reward=783.47 +/- 72.51\n",
            "Episode length: 287.60 +/- 31.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 288      |\n",
            "|    mean_reward     | 783      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91590    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.0594   |\n",
            "|    ent_coef_loss   | -0.368   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91489    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91620, episode_reward=869.22 +/- 174.26\n",
            "Episode length: 310.40 +/- 52.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 310      |\n",
            "|    mean_reward     | 869      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.26     |\n",
            "|    ent_coef        | 0.0593   |\n",
            "|    ent_coef_loss   | 0.0752   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91519    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91650, episode_reward=925.79 +/- 7.01\n",
            "Episode length: 300.60 +/- 1.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 301      |\n",
            "|    mean_reward     | 926      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 19.9     |\n",
            "|    ent_coef        | 0.0596   |\n",
            "|    ent_coef_loss   | 0.247    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91549    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91680, episode_reward=730.00 +/- 10.02\n",
            "Episode length: 261.00 +/- 2.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 261      |\n",
            "|    mean_reward     | 730      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -166     |\n",
            "|    critic_loss     | 3.27     |\n",
            "|    ent_coef        | 0.06     |\n",
            "|    ent_coef_loss   | -0.569   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91579    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91710, episode_reward=699.56 +/- 18.02\n",
            "Episode length: 239.40 +/- 4.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 239      |\n",
            "|    mean_reward     | 700      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91710    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 4.65     |\n",
            "|    ent_coef        | 0.0605   |\n",
            "|    ent_coef_loss   | 0.476    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91609    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91740, episode_reward=721.73 +/- 47.12\n",
            "Episode length: 245.40 +/- 11.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 245      |\n",
            "|    mean_reward     | 722      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91740    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.77     |\n",
            "|    ent_coef        | 0.0605   |\n",
            "|    ent_coef_loss   | 0.54     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91639    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91770, episode_reward=645.62 +/- 1.94\n",
            "Episode length: 216.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 646      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91770    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.0606   |\n",
            "|    ent_coef_loss   | -0.292   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91669    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91800, episode_reward=618.48 +/- 102.50\n",
            "Episode length: 231.60 +/- 26.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 232      |\n",
            "|    mean_reward     | 618      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.89     |\n",
            "|    ent_coef        | 0.0609   |\n",
            "|    ent_coef_loss   | 0.631    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91699    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91830, episode_reward=816.69 +/- 236.19\n",
            "Episode length: 268.00 +/- 84.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 817      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.33     |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | -0.0363  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91729    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91860, episode_reward=544.46 +/- 11.29\n",
            "Episode length: 206.60 +/- 2.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 207      |\n",
            "|    mean_reward     | 544      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 5.23     |\n",
            "|    ent_coef        | 0.061    |\n",
            "|    ent_coef_loss   | -0.245   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91759    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91890, episode_reward=769.07 +/- 18.32\n",
            "Episode length: 258.40 +/- 5.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 769      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91890    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 6.81     |\n",
            "|    ent_coef        | 0.0611   |\n",
            "|    ent_coef_loss   | 0.978    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91789    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91920, episode_reward=853.11 +/- 9.57\n",
            "Episode length: 294.60 +/- 2.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 295      |\n",
            "|    mean_reward     | 853      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.24     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.0944   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91819    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91950, episode_reward=725.66 +/- 52.89\n",
            "Episode length: 247.40 +/- 14.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 247      |\n",
            "|    mean_reward     | 726      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91950    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.85     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.572    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91849    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=91980, episode_reward=842.09 +/- 9.76\n",
            "Episode length: 294.40 +/- 3.88\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 294      |\n",
            "|    mean_reward     | 842      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 91980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 2.99     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.714   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91879    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92010, episode_reward=801.80 +/- 41.25\n",
            "Episode length: 278.40 +/- 11.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 802      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92010    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.66     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.364   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91909    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92040, episode_reward=795.28 +/- 54.39\n",
            "Episode length: 273.80 +/- 19.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 795      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.1      |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 1.31     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91939    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92070, episode_reward=765.72 +/- 40.80\n",
            "Episode length: 252.00 +/- 9.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 766      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92070    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | 1.18     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91969    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92100, episode_reward=977.28 +/- 36.72\n",
            "Episode length: 340.40 +/- 7.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 340      |\n",
            "|    mean_reward     | 977      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92100    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.87     |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | -0.282   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 91999    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92130, episode_reward=712.23 +/- 37.91\n",
            "Episode length: 240.20 +/- 15.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 240      |\n",
            "|    mean_reward     | 712      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92130    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -167     |\n",
            "|    critic_loss     | 4.62     |\n",
            "|    ent_coef        | 0.0625   |\n",
            "|    ent_coef_loss   | -0.536   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92029    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92160, episode_reward=788.13 +/- 57.90\n",
            "Episode length: 288.80 +/- 9.56\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 289      |\n",
            "|    mean_reward     | 788      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.64     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.234    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92059    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92190, episode_reward=957.05 +/- 69.00\n",
            "Episode length: 332.00 +/- 27.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 332      |\n",
            "|    mean_reward     | 957      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92190    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.47     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.194    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92089    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92220, episode_reward=758.59 +/- 128.09\n",
            "Episode length: 260.80 +/- 45.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 261      |\n",
            "|    mean_reward     | 759      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.785   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92119    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92250, episode_reward=753.32 +/- 79.15\n",
            "Episode length: 259.40 +/- 31.55\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | 753      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92250    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.62     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.275   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92149    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92280, episode_reward=723.34 +/- 9.96\n",
            "Episode length: 255.60 +/- 2.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 256      |\n",
            "|    mean_reward     | 723      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.19     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.756    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92179    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92310, episode_reward=667.00 +/- 7.20\n",
            "Episode length: 220.20 +/- 2.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 667      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92310    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.7      |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.227    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92209    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 240      |\n",
            "|    ep_rew_mean     | 705      |\n",
            "| time/              |          |\n",
            "|    episodes        | 596      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 5663     |\n",
            "|    total_timesteps | 92333    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.311    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92232    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92340, episode_reward=1227.08 +/- 169.30\n",
            "Episode length: 419.60 +/- 57.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 420      |\n",
            "|    mean_reward     | 1.23e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 4.37     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.41     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92239    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92370, episode_reward=767.14 +/- 38.24\n",
            "Episode length: 241.00 +/- 9.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 241      |\n",
            "|    mean_reward     | 767      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92370    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 7.45     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.252   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92269    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92400, episode_reward=798.96 +/- 43.13\n",
            "Episode length: 258.00 +/- 12.57\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 799      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 2.94     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.236   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92299    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92430, episode_reward=837.51 +/- 127.51\n",
            "Episode length: 273.80 +/- 36.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 838      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92430    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 2.81     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.204   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92329    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92460, episode_reward=908.23 +/- 283.64\n",
            "Episode length: 306.20 +/- 97.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 306      |\n",
            "|    mean_reward     | 908      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 2.82     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.455   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92359    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92490, episode_reward=795.18 +/- 56.75\n",
            "Episode length: 248.60 +/- 11.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 249      |\n",
            "|    mean_reward     | 795      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92490    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 6.13     |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | -0.629   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92389    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92520, episode_reward=732.77 +/- 31.47\n",
            "Episode length: 247.20 +/- 7.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 247      |\n",
            "|    mean_reward     | 733      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 5.62     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | 1.02     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92419    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92550, episode_reward=816.21 +/- 111.06\n",
            "Episode length: 259.00 +/- 35.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | 816      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92550    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 4.42     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.39     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92449    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92580, episode_reward=624.16 +/- 8.68\n",
            "Episode length: 218.00 +/- 4.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 218      |\n",
            "|    mean_reward     | 624      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92580    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -170     |\n",
            "|    critic_loss     | 2.94     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.665    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92479    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92610, episode_reward=708.39 +/- 27.86\n",
            "Episode length: 231.40 +/- 8.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 231      |\n",
            "|    mean_reward     | 708      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92610    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.291    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92509    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92640, episode_reward=782.72 +/- 56.25\n",
            "Episode length: 266.00 +/- 17.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 266      |\n",
            "|    mean_reward     | 783      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92640    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -170     |\n",
            "|    critic_loss     | 15.3     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | -0.465   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92539    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92670, episode_reward=784.83 +/- 2.48\n",
            "Episode length: 266.60 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 267      |\n",
            "|    mean_reward     | 785      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92670    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.08     |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | -0.244   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92569    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92700, episode_reward=685.41 +/- 15.44\n",
            "Episode length: 217.80 +/- 2.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 218      |\n",
            "|    mean_reward     | 685      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.57     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.0765  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92599    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92730, episode_reward=654.18 +/- 2.08\n",
            "Episode length: 207.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 207      |\n",
            "|    mean_reward     | 654      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92730    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -168     |\n",
            "|    critic_loss     | 5.94     |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | 1.17     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92629    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92760, episode_reward=823.21 +/- 9.11\n",
            "Episode length: 274.20 +/- 5.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 823      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92760    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.59     |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | 0.107    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92659    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92790, episode_reward=672.57 +/- 12.28\n",
            "Episode length: 216.60 +/- 8.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 673      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92790    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.46     |\n",
            "|    ent_coef        | 0.0627   |\n",
            "|    ent_coef_loss   | -0.683   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92689    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92820, episode_reward=1035.87 +/- 97.27\n",
            "Episode length: 337.40 +/- 29.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 337      |\n",
            "|    mean_reward     | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92820    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.14     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | 0.201    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92719    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92850, episode_reward=1370.51 +/- 127.99\n",
            "Episode length: 453.20 +/- 43.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 453      |\n",
            "|    mean_reward     | 1.37e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92850    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.84     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | 0.0845   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92749    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=92880, episode_reward=858.36 +/- 222.30\n",
            "Episode length: 279.00 +/- 91.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 858      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -170     |\n",
            "|    critic_loss     | 4.74     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -1.2     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92779    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92910, episode_reward=818.15 +/- 8.44\n",
            "Episode length: 263.20 +/- 4.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 263      |\n",
            "|    mean_reward     | 818      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92910    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.7      |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.434   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92809    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92940, episode_reward=1101.49 +/- 287.23\n",
            "Episode length: 350.20 +/- 94.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 350      |\n",
            "|    mean_reward     | 1.1e+03  |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.62     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.786   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92839    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=92970, episode_reward=916.84 +/- 212.22\n",
            "Episode length: 318.60 +/- 90.77\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 319      |\n",
            "|    mean_reward     | 917      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 92970    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.03     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.131   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92869    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93000, episode_reward=811.26 +/- 100.14\n",
            "Episode length: 277.80 +/- 30.91\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 811      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 4.59     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.478    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93030, episode_reward=981.69 +/- 89.67\n",
            "Episode length: 327.40 +/- 25.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 327      |\n",
            "|    mean_reward     | 982      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93030    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 6.12     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.303   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92929    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93060, episode_reward=1416.99 +/- 75.70\n",
            "Episode length: 477.20 +/- 28.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 477      |\n",
            "|    mean_reward     | 1.42e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.54     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.0748   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92959    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=93090, episode_reward=947.28 +/- 168.66\n",
            "Episode length: 298.00 +/- 47.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 298      |\n",
            "|    mean_reward     | 947      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93090    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 2.83     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.252   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 92989    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93120, episode_reward=746.67 +/- 79.95\n",
            "Episode length: 240.20 +/- 15.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 240      |\n",
            "|    mean_reward     | 747      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.37     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.31    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93019    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93150, episode_reward=808.27 +/- 19.36\n",
            "Episode length: 254.60 +/- 6.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 255      |\n",
            "|    mean_reward     | 808      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93150    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 9.31     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.275   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93049    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93180, episode_reward=864.89 +/- 8.35\n",
            "Episode length: 276.20 +/- 3.66\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 276      |\n",
            "|    mean_reward     | 865      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93180    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 4.54     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 0.397    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93079    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93210, episode_reward=679.15 +/- 19.66\n",
            "Episode length: 213.60 +/- 6.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 679      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.94     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.432   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93109    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 241      |\n",
            "|    ep_rew_mean     | 711      |\n",
            "| time/              |          |\n",
            "|    episodes        | 600      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 5734     |\n",
            "|    total_timesteps | 93226    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 5.8      |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.0441  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93125    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93240, episode_reward=653.46 +/- 1.58\n",
            "Episode length: 209.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 653      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.79     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | 0.141    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93139    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93270, episode_reward=711.97 +/- 66.65\n",
            "Episode length: 234.60 +/- 20.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 235      |\n",
            "|    mean_reward     | 712      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93270    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 5.9      |\n",
            "|    ent_coef        | 0.0611   |\n",
            "|    ent_coef_loss   | -0.207   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93169    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93300, episode_reward=1082.37 +/- 24.82\n",
            "Episode length: 342.40 +/- 12.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 342      |\n",
            "|    mean_reward     | 1.08e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 13.7     |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | -0.225   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93199    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93330, episode_reward=911.11 +/- 71.87\n",
            "Episode length: 291.20 +/- 17.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 291      |\n",
            "|    mean_reward     | 911      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93330    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 4.05     |\n",
            "|    ent_coef        | 0.0609   |\n",
            "|    ent_coef_loss   | -0.0532  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93229    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93360, episode_reward=1244.41 +/- 79.00\n",
            "Episode length: 395.20 +/- 22.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 395      |\n",
            "|    mean_reward     | 1.24e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.84     |\n",
            "|    ent_coef        | 0.0608   |\n",
            "|    ent_coef_loss   | -0.798   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93259    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93390, episode_reward=856.80 +/- 101.17\n",
            "Episode length: 275.60 +/- 33.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 276      |\n",
            "|    mean_reward     | 857      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93390    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.89     |\n",
            "|    ent_coef        | 0.0608   |\n",
            "|    ent_coef_loss   | -0.455   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93289    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93420, episode_reward=1161.55 +/- 147.28\n",
            "Episode length: 365.00 +/- 53.08\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 365      |\n",
            "|    mean_reward     | 1.16e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.48     |\n",
            "|    ent_coef        | 0.0607   |\n",
            "|    ent_coef_loss   | 0.287    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93319    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93450, episode_reward=667.42 +/- 5.87\n",
            "Episode length: 217.80 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 218      |\n",
            "|    mean_reward     | 667      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93450    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.8      |\n",
            "|    ent_coef        | 0.0608   |\n",
            "|    ent_coef_loss   | 0.502    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93349    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93480, episode_reward=921.72 +/- 68.77\n",
            "Episode length: 303.40 +/- 22.62\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 303      |\n",
            "|    mean_reward     | 922      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 6.55     |\n",
            "|    ent_coef        | 0.0608   |\n",
            "|    ent_coef_loss   | -0.366   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93379    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93510, episode_reward=1316.32 +/- 228.02\n",
            "Episode length: 448.60 +/- 78.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 449      |\n",
            "|    mean_reward     | 1.32e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93510    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 2.61     |\n",
            "|    ent_coef        | 0.0607   |\n",
            "|    ent_coef_loss   | -0.00341 |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93409    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93540, episode_reward=1237.18 +/- 309.97\n",
            "Episode length: 398.40 +/- 114.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 398      |\n",
            "|    mean_reward     | 1.24e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93540    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -171     |\n",
            "|    critic_loss     | 3.07     |\n",
            "|    ent_coef        | 0.0605   |\n",
            "|    ent_coef_loss   | 0.684    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93439    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93570, episode_reward=913.00 +/- 142.50\n",
            "Episode length: 282.00 +/- 40.78\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 282      |\n",
            "|    mean_reward     | 913      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.25     |\n",
            "|    ent_coef        | 0.0608   |\n",
            "|    ent_coef_loss   | 0.49     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93469    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93600, episode_reward=652.85 +/- 6.29\n",
            "Episode length: 208.40 +/- 2.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 208      |\n",
            "|    mean_reward     | 653      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 2.47     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | 0.168    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93499    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93630, episode_reward=786.09 +/- 114.65\n",
            "Episode length: 247.80 +/- 36.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 248      |\n",
            "|    mean_reward     | 786      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93630    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.71     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.0246  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93529    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93660, episode_reward=1373.23 +/- 213.16\n",
            "Episode length: 436.00 +/- 79.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 436      |\n",
            "|    mean_reward     | 1.37e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93660    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.95     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.851   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93559    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93690, episode_reward=1440.95 +/- 88.70\n",
            "Episode length: 469.60 +/- 38.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 470      |\n",
            "|    mean_reward     | 1.44e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93690    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.25     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.211   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93589    |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=93720, episode_reward=934.97 +/- 121.05\n",
            "Episode length: 292.00 +/- 30.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 292      |\n",
            "|    mean_reward     | 935      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.22     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.269   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93619    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93750, episode_reward=825.16 +/- 141.33\n",
            "Episode length: 255.20 +/- 37.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 255      |\n",
            "|    mean_reward     | 825      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 4.58     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.215   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93649    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93780, episode_reward=660.35 +/- 16.50\n",
            "Episode length: 217.20 +/- 5.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 660      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93780    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.88     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 0.504    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93679    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93810, episode_reward=865.27 +/- 5.72\n",
            "Episode length: 262.20 +/- 2.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 865      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93810    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.01     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.146    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93709    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93840, episode_reward=1326.80 +/- 186.91\n",
            "Episode length: 436.00 +/- 78.48\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 436      |\n",
            "|    mean_reward     | 1.33e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 79.3     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.164    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93739    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93870, episode_reward=846.88 +/- 44.43\n",
            "Episode length: 269.00 +/- 13.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 847      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93870    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.133   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93769    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93900, episode_reward=745.14 +/- 49.83\n",
            "Episode length: 239.60 +/- 12.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 240      |\n",
            "|    mean_reward     | 745      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93900    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 2.79     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.553    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93799    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93930, episode_reward=1069.63 +/- 45.58\n",
            "Episode length: 330.00 +/- 12.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 330      |\n",
            "|    mean_reward     | 1.07e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -190     |\n",
            "|    critic_loss     | 3.6      |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.165   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93829    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93960, episode_reward=734.09 +/- 83.84\n",
            "Episode length: 233.20 +/- 23.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 233      |\n",
            "|    mean_reward     | 734      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.8      |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.0875  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93859    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=93990, episode_reward=1037.61 +/- 111.18\n",
            "Episode length: 333.00 +/- 35.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 333      |\n",
            "|    mean_reward     | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 93990    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.91     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.351   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93889    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94020, episode_reward=894.40 +/- 30.61\n",
            "Episode length: 277.80 +/- 6.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 894      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 3.88     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 0.848    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93919    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94050, episode_reward=782.33 +/- 122.21\n",
            "Episode length: 248.20 +/- 37.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 248      |\n",
            "|    mean_reward     | 782      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94050    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.73     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.611   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93949    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94080, episode_reward=758.63 +/- 72.09\n",
            "Episode length: 237.00 +/- 15.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 759      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94080    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 2.6      |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 0.835    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 93979    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94110, episode_reward=836.99 +/- 107.85\n",
            "Episode length: 261.60 +/- 30.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 837      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94110    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.6      |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.435    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94009    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94140, episode_reward=855.57 +/- 22.25\n",
            "Episode length: 265.80 +/- 8.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 266      |\n",
            "|    mean_reward     | 856      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 8.75     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 1.14     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94039    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94170, episode_reward=879.02 +/- 8.51\n",
            "Episode length: 272.00 +/- 4.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 272      |\n",
            "|    mean_reward     | 879      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94170    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 4.2      |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.168    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94069    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94200, episode_reward=870.89 +/- 78.35\n",
            "Episode length: 276.80 +/- 23.22\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 277      |\n",
            "|    mean_reward     | 871      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.97     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.223   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94099    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94230, episode_reward=892.13 +/- 11.26\n",
            "Episode length: 275.20 +/- 6.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 892      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94230    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 5.61     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.0628   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94129    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94260, episode_reward=1321.73 +/- 153.63\n",
            "Episode length: 412.00 +/- 49.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 412      |\n",
            "|    mean_reward     | 1.32e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94260    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 8.69     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.271   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94159    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94290, episode_reward=860.14 +/- 21.88\n",
            "Episode length: 270.40 +/- 6.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 860      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 4.43     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.1     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94189    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94320, episode_reward=724.85 +/- 86.38\n",
            "Episode length: 237.00 +/- 22.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 725      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.815   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94219    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94350, episode_reward=843.48 +/- 11.39\n",
            "Episode length: 258.00 +/- 4.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 843      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94350    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 4.82     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | -1.15    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94249    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 244      |\n",
            "|    ep_rew_mean     | 721      |\n",
            "| time/              |          |\n",
            "|    episodes        | 604      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 5828     |\n",
            "|    total_timesteps | 94362    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -171     |\n",
            "|    critic_loss     | 15.3     |\n",
            "|    ent_coef        | 0.061    |\n",
            "|    ent_coef_loss   | 0.0779   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94261    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94380, episode_reward=886.18 +/- 16.47\n",
            "Episode length: 274.80 +/- 3.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 886      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 5.57     |\n",
            "|    ent_coef        | 0.061    |\n",
            "|    ent_coef_loss   | 0.618    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94279    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94410, episode_reward=879.40 +/- 6.08\n",
            "Episode length: 267.40 +/- 2.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 267      |\n",
            "|    mean_reward     | 879      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94410    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 4.11     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.194   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94309    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94440, episode_reward=1038.80 +/- 247.34\n",
            "Episode length: 332.00 +/- 85.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 332      |\n",
            "|    mean_reward     | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94440    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 5.47     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.652   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94339    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94470, episode_reward=874.70 +/- 9.79\n",
            "Episode length: 273.20 +/- 8.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 875      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94470    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.878    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94369    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94500, episode_reward=825.76 +/- 12.02\n",
            "Episode length: 252.60 +/- 2.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 253      |\n",
            "|    mean_reward     | 826      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94500    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.51     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.00299 |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94399    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94530, episode_reward=899.55 +/- 3.84\n",
            "Episode length: 277.80 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 900      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94530    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.01     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.23     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94429    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94560, episode_reward=874.78 +/- 27.98\n",
            "Episode length: 271.00 +/- 10.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 271      |\n",
            "|    mean_reward     | 875      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94560    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.49     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | -0.602   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94459    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94590, episode_reward=704.70 +/- 13.48\n",
            "Episode length: 227.00 +/- 5.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 227      |\n",
            "|    mean_reward     | 705      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94590    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 8.79     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.505    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94489    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94620, episode_reward=865.51 +/- 15.89\n",
            "Episode length: 268.40 +/- 5.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 866      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.24     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.136   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94519    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94650, episode_reward=822.74 +/- 25.16\n",
            "Episode length: 253.60 +/- 10.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | 823      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.36     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.502    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94549    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94680, episode_reward=654.07 +/- 7.50\n",
            "Episode length: 217.20 +/- 2.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 654      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3        |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.304   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94579    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94710, episode_reward=1033.53 +/- 443.71\n",
            "Episode length: 331.80 +/- 137.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 332      |\n",
            "|    mean_reward     | 1.03e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94710    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.24     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.503   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94609    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94740, episode_reward=699.50 +/- 37.04\n",
            "Episode length: 222.20 +/- 8.42\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 700      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94740    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 19.9     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.168   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94639    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94770, episode_reward=891.73 +/- 4.19\n",
            "Episode length: 278.80 +/- 2.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 892      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94770    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.84     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | 0.471    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94669    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94800, episode_reward=848.44 +/- 22.70\n",
            "Episode length: 263.80 +/- 9.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 848      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.2      |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.144   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94699    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94830, episode_reward=871.88 +/- 6.52\n",
            "Episode length: 268.80 +/- 3.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 872      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.762   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94729    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94860, episode_reward=948.17 +/- 169.71\n",
            "Episode length: 284.00 +/- 46.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 284      |\n",
            "|    mean_reward     | 948      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.08     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 1.01     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94759    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94890, episode_reward=992.23 +/- 99.46\n",
            "Episode length: 308.00 +/- 25.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 308      |\n",
            "|    mean_reward     | 992      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94890    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 6.31     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.384   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94789    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94920, episode_reward=934.92 +/- 54.37\n",
            "Episode length: 297.80 +/- 12.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 298      |\n",
            "|    mean_reward     | 935      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.36     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.597   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94819    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94950, episode_reward=673.71 +/- 19.55\n",
            "Episode length: 219.60 +/- 5.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 674      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94950    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 5.61     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | -0.221   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94849    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=94980, episode_reward=747.50 +/- 66.53\n",
            "Episode length: 234.00 +/- 14.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 234      |\n",
            "|    mean_reward     | 747      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 94980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -187     |\n",
            "|    critic_loss     | 5.12     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.0125   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94879    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95010, episode_reward=898.96 +/- 207.41\n",
            "Episode length: 278.40 +/- 57.35\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 899      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95010    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.21     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.689   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94909    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95040, episode_reward=810.44 +/- 52.68\n",
            "Episode length: 250.20 +/- 13.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 250      |\n",
            "|    mean_reward     | 810      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 2.77     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.0315  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94939    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95070, episode_reward=998.88 +/- 106.57\n",
            "Episode length: 304.00 +/- 31.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 304      |\n",
            "|    mean_reward     | 999      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95070    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 4.29     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.943    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94969    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95100, episode_reward=726.25 +/- 117.10\n",
            "Episode length: 237.20 +/- 36.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 726      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95100    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.79     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.626   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 94999    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95130, episode_reward=1382.27 +/- 257.02\n",
            "Episode length: 441.60 +/- 85.82\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 442      |\n",
            "|    mean_reward     | 1.38e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95130    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.32     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.0486  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95029    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95160, episode_reward=669.52 +/- 29.40\n",
            "Episode length: 221.80 +/- 8.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 670      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.07     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.118   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95059    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95190, episode_reward=861.98 +/- 1.43\n",
            "Episode length: 267.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 862      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95190    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 4.02     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.205    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95089    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95220, episode_reward=816.94 +/- 59.77\n",
            "Episode length: 251.00 +/- 13.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 251      |\n",
            "|    mean_reward     | 817      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.37     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.522   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95119    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95250, episode_reward=808.80 +/- 80.40\n",
            "Episode length: 254.00 +/- 22.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 254      |\n",
            "|    mean_reward     | 809      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95250    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 6.1      |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.57     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95149    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95280, episode_reward=863.14 +/- 3.56\n",
            "Episode length: 273.20 +/- 2.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 863      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.96     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.0677  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95179    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95310, episode_reward=863.62 +/- 6.04\n",
            "Episode length: 268.20 +/- 1.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 864      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95310    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.26     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.167   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95209    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95340, episode_reward=864.24 +/- 13.08\n",
            "Episode length: 268.20 +/- 6.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 864      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 63.8     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.105    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95239    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95370, episode_reward=749.11 +/- 63.16\n",
            "Episode length: 237.00 +/- 14.67\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 237      |\n",
            "|    mean_reward     | 749      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95370    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.2      |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.148   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95269    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95400, episode_reward=934.78 +/- 99.37\n",
            "Episode length: 289.80 +/- 27.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 290      |\n",
            "|    mean_reward     | 935      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 3.23     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.398   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95299    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95430, episode_reward=747.72 +/- 116.26\n",
            "Episode length: 245.20 +/- 34.38\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 245      |\n",
            "|    mean_reward     | 748      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95430    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 63.7     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 1.06     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95329    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 244      |\n",
            "|    ep_rew_mean     | 724      |\n",
            "| time/              |          |\n",
            "|    episodes        | 608      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 5911     |\n",
            "|    total_timesteps | 95432    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.86     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.0811   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95331    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95460, episode_reward=1385.13 +/- 223.39\n",
            "Episode length: 439.20 +/- 74.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 439      |\n",
            "|    mean_reward     | 1.39e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 2.89     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.041    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95359    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95490, episode_reward=817.60 +/- 87.50\n",
            "Episode length: 259.60 +/- 23.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 260      |\n",
            "|    mean_reward     | 818      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95490    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.99     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.391   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95389    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95520, episode_reward=857.00 +/- 11.02\n",
            "Episode length: 265.60 +/- 4.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 266      |\n",
            "|    mean_reward     | 857      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.32     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.119   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95419    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95550, episode_reward=953.42 +/- 92.08\n",
            "Episode length: 301.60 +/- 24.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 302      |\n",
            "|    mean_reward     | 953      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95550    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.52     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.76     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95449    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95580, episode_reward=1001.14 +/- 198.04\n",
            "Episode length: 324.80 +/- 63.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 325      |\n",
            "|    mean_reward     | 1e+03    |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95580    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 3.32     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.382    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95479    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95610, episode_reward=922.62 +/- 81.27\n",
            "Episode length: 287.40 +/- 20.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | 923      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95610    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.81     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.203   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95509    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95640, episode_reward=860.92 +/- 10.58\n",
            "Episode length: 268.20 +/- 3.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 861      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95640    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.37     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.243    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95539    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95670, episode_reward=1154.76 +/- 252.21\n",
            "Episode length: 366.80 +/- 83.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 367      |\n",
            "|    mean_reward     | 1.15e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95670    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.77     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.576   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95569    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95700, episode_reward=827.94 +/- 55.14\n",
            "Episode length: 263.00 +/- 16.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 263      |\n",
            "|    mean_reward     | 828      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 2.97     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 1.39     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95599    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95730, episode_reward=870.33 +/- 2.08\n",
            "Episode length: 277.80 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 278      |\n",
            "|    mean_reward     | 870      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95730    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 2.84     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | -0.451   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95629    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95760, episode_reward=672.41 +/- 62.85\n",
            "Episode length: 221.20 +/- 17.43\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 221      |\n",
            "|    mean_reward     | 672      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95760    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.24     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.374    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95659    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95790, episode_reward=875.92 +/- 8.45\n",
            "Episode length: 272.60 +/- 5.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 876      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95790    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.74     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.337   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95689    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95820, episode_reward=835.18 +/- 87.07\n",
            "Episode length: 265.60 +/- 24.43\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 266      |\n",
            "|    mean_reward     | 835      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95820    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 4.75     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.632   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95719    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95850, episode_reward=676.45 +/- 32.52\n",
            "Episode length: 224.80 +/- 9.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 225      |\n",
            "|    mean_reward     | 676      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95850    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.69     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.143   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95749    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95880, episode_reward=771.42 +/- 67.78\n",
            "Episode length: 249.60 +/- 18.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 250      |\n",
            "|    mean_reward     | 771      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.8      |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.00559  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95779    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95910, episode_reward=919.59 +/- 236.19\n",
            "Episode length: 289.60 +/- 68.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 290      |\n",
            "|    mean_reward     | 920      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95910    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 5.82     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.215    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95809    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95940, episode_reward=880.62 +/- 350.51\n",
            "Episode length: 279.40 +/- 99.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 881      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 5.74     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.329   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95839    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=95970, episode_reward=911.58 +/- 146.40\n",
            "Episode length: 284.60 +/- 42.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 285      |\n",
            "|    mean_reward     | 912      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 95970    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.84     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.0499   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95869    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96000, episode_reward=1112.74 +/- 313.65\n",
            "Episode length: 340.40 +/- 94.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 340      |\n",
            "|    mean_reward     | 1.11e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 3.46     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.336    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96030, episode_reward=678.08 +/- 9.43\n",
            "Episode length: 220.20 +/- 2.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 678      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96030    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.99     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -1.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95929    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96060, episode_reward=666.10 +/- 14.42\n",
            "Episode length: 217.40 +/- 3.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 666      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.05     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.268   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95959    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96090, episode_reward=854.86 +/- 22.36\n",
            "Episode length: 264.00 +/- 9.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 855      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96090    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.81     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.444    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 95989    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96120, episode_reward=857.86 +/- 31.24\n",
            "Episode length: 269.20 +/- 12.51\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 858      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.26     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | -0.0238  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96019    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96150, episode_reward=748.28 +/- 172.79\n",
            "Episode length: 245.60 +/- 50.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 246      |\n",
            "|    mean_reward     | 748      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96150    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.46     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.122   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96049    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96180, episode_reward=685.36 +/- 55.04\n",
            "Episode length: 224.00 +/- 12.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 224      |\n",
            "|    mean_reward     | 685      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96180    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 11.1     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.316   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96079    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96210, episode_reward=883.82 +/- 17.04\n",
            "Episode length: 278.80 +/- 7.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 884      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.15     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.327   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96109    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96240, episode_reward=938.86 +/- 69.23\n",
            "Episode length: 288.00 +/- 17.38\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 288      |\n",
            "|    mean_reward     | 939      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.73     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 1.05     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96139    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96270, episode_reward=898.23 +/- 172.44\n",
            "Episode length: 299.00 +/- 52.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 299      |\n",
            "|    mean_reward     | 898      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96270    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 3.57     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.127    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96169    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96300, episode_reward=882.82 +/- 26.63\n",
            "Episode length: 275.80 +/- 12.48\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 276      |\n",
            "|    mean_reward     | 883      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.94     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.765   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96199    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96330, episode_reward=781.76 +/- 105.72\n",
            "Episode length: 251.80 +/- 27.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 782      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96330    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 2.8      |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.0846  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96229    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96360, episode_reward=1144.49 +/- 266.40\n",
            "Episode length: 368.20 +/- 85.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 368      |\n",
            "|    mean_reward     | 1.14e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.08     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.651    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96259    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96390, episode_reward=687.61 +/- 95.76\n",
            "Episode length: 228.40 +/- 29.43\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 228      |\n",
            "|    mean_reward     | 688      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96390    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.21     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.0895   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96289    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96420, episode_reward=683.25 +/- 63.68\n",
            "Episode length: 222.20 +/- 14.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 222      |\n",
            "|    mean_reward     | 683      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 2.38     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.61     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96319    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96450, episode_reward=712.08 +/- 94.91\n",
            "Episode length: 233.00 +/- 29.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 233      |\n",
            "|    mean_reward     | 712      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96450    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 2.68     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.317   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96349    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96480, episode_reward=1167.22 +/- 341.80\n",
            "Episode length: 388.80 +/- 114.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 389      |\n",
            "|    mean_reward     | 1.17e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.4      |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.626   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96379    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96510, episode_reward=854.37 +/- 7.75\n",
            "Episode length: 269.60 +/- 3.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 854      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96510    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -167     |\n",
            "|    critic_loss     | 3.88     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.0113  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96409    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96540, episode_reward=713.00 +/- 86.86\n",
            "Episode length: 233.00 +/- 26.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 233      |\n",
            "|    mean_reward     | 713      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96540    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.44     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.361    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96439    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96570, episode_reward=666.20 +/- 11.45\n",
            "Episode length: 219.20 +/- 3.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 219      |\n",
            "|    mean_reward     | 666      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.75     |\n",
            "|    ent_coef        | 0.0625   |\n",
            "|    ent_coef_loss   | 0.521    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96469    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96600, episode_reward=829.72 +/- 15.13\n",
            "Episode length: 266.20 +/- 8.45\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 266      |\n",
            "|    mean_reward     | 830      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.3      |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | -0.103   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96499    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 249      |\n",
            "|    ep_rew_mean     | 742      |\n",
            "| time/              |          |\n",
            "|    episodes        | 612      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 6003     |\n",
            "|    total_timesteps | 96608    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 4.7      |\n",
            "|    ent_coef        | 0.0626   |\n",
            "|    ent_coef_loss   | -0.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96507    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96630, episode_reward=923.43 +/- 250.39\n",
            "Episode length: 297.60 +/- 75.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 298      |\n",
            "|    mean_reward     | 923      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96630    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.14     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.838   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96529    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96660, episode_reward=843.20 +/- 12.31\n",
            "Episode length: 258.60 +/- 4.13\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | 843      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96660    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 3.15     |\n",
            "|    ent_coef        | 0.0624   |\n",
            "|    ent_coef_loss   | -0.0675  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96559    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96690, episode_reward=877.67 +/- 4.57\n",
            "Episode length: 279.00 +/- 2.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 279      |\n",
            "|    mean_reward     | 878      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96690    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.13     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | -0.148   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96589    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96720, episode_reward=883.47 +/- 10.27\n",
            "Episode length: 276.00 +/- 4.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 276      |\n",
            "|    mean_reward     | 883      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 4.97     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.0667  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96619    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96750, episode_reward=644.04 +/- 6.49\n",
            "Episode length: 213.20 +/- 3.19\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 213      |\n",
            "|    mean_reward     | 644      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 10.9     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.2     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96649    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96780, episode_reward=835.77 +/- 80.48\n",
            "Episode length: 261.80 +/- 24.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 262      |\n",
            "|    mean_reward     | 836      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96780    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.48     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.532    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96679    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96810, episode_reward=646.38 +/- 3.79\n",
            "Episode length: 212.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 646      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96810    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 5.32     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.653   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96709    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96840, episode_reward=843.33 +/- 18.93\n",
            "Episode length: 260.80 +/- 6.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 261      |\n",
            "|    mean_reward     | 843      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 4.87     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.424   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96739    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96870, episode_reward=653.84 +/- 5.16\n",
            "Episode length: 215.80 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 654      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96870    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 2.83     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.436   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96769    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96900, episode_reward=700.48 +/- 65.04\n",
            "Episode length: 227.00 +/- 13.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 227      |\n",
            "|    mean_reward     | 700      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96900    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -169     |\n",
            "|    critic_loss     | 3.26     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.513   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96799    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96930, episode_reward=880.13 +/- 16.81\n",
            "Episode length: 274.40 +/- 8.21\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 880      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 2.9      |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.454    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96829    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96960, episode_reward=799.89 +/- 60.99\n",
            "Episode length: 245.20 +/- 12.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 245      |\n",
            "|    mean_reward     | 800      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 6.61     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.88     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96859    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=96990, episode_reward=645.44 +/- 3.67\n",
            "Episode length: 213.80 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 645      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 96990    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.03     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.372    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96889    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97020, episode_reward=879.42 +/- 11.95\n",
            "Episode length: 270.60 +/- 5.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 271      |\n",
            "|    mean_reward     | 879      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97020    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 2.87     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.929    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96919    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97050, episode_reward=645.05 +/- 4.30\n",
            "Episode length: 211.20 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 211      |\n",
            "|    mean_reward     | 645      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97050    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.87     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.452    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96949    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97080, episode_reward=737.60 +/- 122.83\n",
            "Episode length: 243.20 +/- 38.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 243      |\n",
            "|    mean_reward     | 738      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97080    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.26     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.0865   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 96979    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97110, episode_reward=895.78 +/- 8.63\n",
            "Episode length: 274.40 +/- 3.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 896      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97110    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.44     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.0848  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97009    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97140, episode_reward=637.29 +/- 1.71\n",
            "Episode length: 210.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 637      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97140    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.24     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.511    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97039    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97170, episode_reward=653.45 +/- 2.42\n",
            "Episode length: 219.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 653      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97170    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -189     |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.0849   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97069    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97200, episode_reward=686.34 +/- 92.10\n",
            "Episode length: 225.60 +/- 24.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 686      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97200    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 5.32     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.199    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97099    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97230, episode_reward=643.83 +/- 1.51\n",
            "Episode length: 209.80 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 644      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97230    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.68     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.186    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97129    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97260, episode_reward=858.68 +/- 5.12\n",
            "Episode length: 263.80 +/- 1.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 859      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97260    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.76     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.753    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97159    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97290, episode_reward=651.63 +/- 5.58\n",
            "Episode length: 212.80 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 213      |\n",
            "|    mean_reward     | 652      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97290    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -171     |\n",
            "|    critic_loss     | 4.41     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.487    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97189    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97320, episode_reward=643.00 +/- 7.66\n",
            "Episode length: 205.20 +/- 2.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 205      |\n",
            "|    mean_reward     | 643      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97320    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.17     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.215   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97219    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97350, episode_reward=646.50 +/- 5.05\n",
            "Episode length: 213.80 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 647      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97350    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 3.2      |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.392   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97249    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97380, episode_reward=661.27 +/- 73.19\n",
            "Episode length: 217.60 +/- 20.70\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 218      |\n",
            "|    mean_reward     | 661      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97380    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.43     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.815   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97279    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97410, episode_reward=1130.65 +/- 141.07\n",
            "Episode length: 374.00 +/- 43.18\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 374      |\n",
            "|    mean_reward     | 1.13e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97410    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.72     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.608    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97309    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97440, episode_reward=784.74 +/- 50.62\n",
            "Episode length: 245.60 +/- 11.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 246      |\n",
            "|    mean_reward     | 785      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97440    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 4.18     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | -0.778   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97339    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97470, episode_reward=834.23 +/- 43.11\n",
            "Episode length: 252.80 +/- 9.87\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 253      |\n",
            "|    mean_reward     | 834      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97470    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 6.86     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.431    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97369    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97500, episode_reward=936.59 +/- 72.96\n",
            "Episode length: 303.00 +/- 24.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 303      |\n",
            "|    mean_reward     | 937      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97500    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.06     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.245    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97399    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97530, episode_reward=1072.53 +/- 211.05\n",
            "Episode length: 348.40 +/- 69.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 348      |\n",
            "|    mean_reward     | 1.07e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97530    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.96     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.542    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97429    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97560, episode_reward=960.51 +/- 115.00\n",
            "Episode length: 309.40 +/- 33.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 309      |\n",
            "|    mean_reward     | 961      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97560    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -170     |\n",
            "|    critic_loss     | 2.82     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.533    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97459    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97590, episode_reward=882.65 +/- 6.50\n",
            "Episode length: 267.80 +/- 2.32\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 883      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97590    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.89     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.143    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97489    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 249      |\n",
            "|    ep_rew_mean     | 744      |\n",
            "| time/              |          |\n",
            "|    episodes        | 616      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 6080     |\n",
            "|    total_timesteps | 97593    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.42     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.83     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97492    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97620, episode_reward=875.09 +/- 12.39\n",
            "Episode length: 283.60 +/- 5.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 284      |\n",
            "|    mean_reward     | 875      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97620    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.06     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | 0.0494   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97519    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97650, episode_reward=647.02 +/- 6.94\n",
            "Episode length: 213.60 +/- 1.96\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 647      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97650    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 13       |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.567    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97549    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97680, episode_reward=736.26 +/- 116.40\n",
            "Episode length: 242.60 +/- 38.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 243      |\n",
            "|    mean_reward     | 736      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97680    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.18     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | -0.0355  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97579    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97710, episode_reward=799.82 +/- 108.77\n",
            "Episode length: 264.20 +/- 35.76\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 800      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97710    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 2.85     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.393    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97609    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97740, episode_reward=1033.96 +/- 68.84\n",
            "Episode length: 327.20 +/- 16.68\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 327      |\n",
            "|    mean_reward     | 1.03e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97740    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.7      |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | -0.629   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97639    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97770, episode_reward=945.33 +/- 60.04\n",
            "Episode length: 303.40 +/- 15.44\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 303      |\n",
            "|    mean_reward     | 945      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97770    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.66     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.402   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97669    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97800, episode_reward=767.09 +/- 89.10\n",
            "Episode length: 239.40 +/- 19.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 239      |\n",
            "|    mean_reward     | 767      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97800    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 4.02     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.0198  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97699    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97830, episode_reward=647.67 +/- 7.22\n",
            "Episode length: 213.20 +/- 2.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 213      |\n",
            "|    mean_reward     | 648      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97830    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 4.31     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.544   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97729    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97860, episode_reward=796.91 +/- 9.86\n",
            "Episode length: 244.80 +/- 3.25\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 245      |\n",
            "|    mean_reward     | 797      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97860    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.35     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.524   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97759    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97890, episode_reward=993.72 +/- 63.16\n",
            "Episode length: 308.00 +/- 17.46\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 308      |\n",
            "|    mean_reward     | 994      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97890    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.97     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.272   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97789    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97920, episode_reward=637.31 +/- 7.53\n",
            "Episode length: 211.40 +/- 3.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 211      |\n",
            "|    mean_reward     | 637      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97920    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 4.07     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 1.06     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97819    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97950, episode_reward=684.96 +/- 70.46\n",
            "Episode length: 225.80 +/- 16.30\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 226      |\n",
            "|    mean_reward     | 685      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97950    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 7.49     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.211    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97849    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=97980, episode_reward=791.62 +/- 64.11\n",
            "Episode length: 247.20 +/- 13.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 247      |\n",
            "|    mean_reward     | 792      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 97980    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 3.13     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.799    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97879    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98010, episode_reward=645.30 +/- 5.62\n",
            "Episode length: 216.80 +/- 2.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 645      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98010    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.19     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.298    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97909    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98040, episode_reward=847.70 +/- 9.35\n",
            "Episode length: 261.00 +/- 3.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 261      |\n",
            "|    mean_reward     | 848      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98040    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.47     |\n",
            "|    ent_coef        | 0.0625   |\n",
            "|    ent_coef_loss   | -0.827   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97939    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98070, episode_reward=649.68 +/- 7.71\n",
            "Episode length: 216.80 +/- 2.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 650      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98070    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.95     |\n",
            "|    ent_coef        | 0.0623   |\n",
            "|    ent_coef_loss   | 0.737    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97969    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98100, episode_reward=853.81 +/- 24.39\n",
            "Episode length: 265.20 +/- 8.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 265      |\n",
            "|    mean_reward     | 854      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98100    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 3.61     |\n",
            "|    ent_coef        | 0.0622   |\n",
            "|    ent_coef_loss   | 0.216    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 97999    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98130, episode_reward=1036.69 +/- 90.66\n",
            "Episode length: 329.00 +/- 31.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 329      |\n",
            "|    mean_reward     | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98130    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 4.03     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.0472   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98029    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98160, episode_reward=652.53 +/- 5.78\n",
            "Episode length: 215.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 215      |\n",
            "|    mean_reward     | 653      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98160    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 3.11     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -1.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98059    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98190, episode_reward=650.77 +/- 6.29\n",
            "Episode length: 216.60 +/- 2.65\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 217      |\n",
            "|    mean_reward     | 651      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98190    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.78     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.11     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98089    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98220, episode_reward=778.43 +/- 55.26\n",
            "Episode length: 243.00 +/- 12.62\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 243      |\n",
            "|    mean_reward     | 778      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98220    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 11.8     |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | -0.00354 |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98119    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98250, episode_reward=693.30 +/- 107.83\n",
            "Episode length: 228.00 +/- 35.15\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 228      |\n",
            "|    mean_reward     | 693      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98250    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 4.12     |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | 1.38     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98149    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98280, episode_reward=701.79 +/- 114.95\n",
            "Episode length: 229.40 +/- 34.84\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 229      |\n",
            "|    mean_reward     | 702      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98280    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 8.44     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.198   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98179    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98310, episode_reward=766.75 +/- 168.16\n",
            "Episode length: 251.80 +/- 49.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 767      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98310    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.53     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | -0.0256  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98209    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98340, episode_reward=828.78 +/- 90.37\n",
            "Episode length: 272.80 +/- 30.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 829      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98340    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 5.02     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98239    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98370, episode_reward=647.92 +/- 12.95\n",
            "Episode length: 215.80 +/- 5.04\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 648      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98370    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.99     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98269    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98400, episode_reward=840.45 +/- 174.96\n",
            "Episode length: 268.80 +/- 51.11\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 840      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98400    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 80.6     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.049    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98299    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98430, episode_reward=855.06 +/- 16.71\n",
            "Episode length: 267.00 +/- 7.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 267      |\n",
            "|    mean_reward     | 855      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98430    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -173     |\n",
            "|    critic_loss     | 3.63     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.24     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98329    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98460, episode_reward=814.45 +/- 19.48\n",
            "Episode length: 258.20 +/- 9.28\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 814      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98460    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 14.1     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | 0.208    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98359    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98490, episode_reward=861.58 +/- 6.72\n",
            "Episode length: 283.00 +/- 1.67\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 283      |\n",
            "|    mean_reward     | 862      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98490    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 7.1      |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.573   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98389    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98520, episode_reward=674.85 +/- 89.71\n",
            "Episode length: 224.00 +/- 27.52\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 224      |\n",
            "|    mean_reward     | 675      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98520    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 8.41     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.165    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98419    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98550, episode_reward=647.28 +/- 8.78\n",
            "Episode length: 213.40 +/- 2.73\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 213      |\n",
            "|    mean_reward     | 647      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98550    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.51     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | -0.916   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98449    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98580, episode_reward=690.96 +/- 12.44\n",
            "Episode length: 223.20 +/- 2.93\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 223      |\n",
            "|    mean_reward     | 691      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98580    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 4.12     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.452   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98479    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 250      |\n",
            "|    ep_rew_mean     | 747      |\n",
            "| time/              |          |\n",
            "|    episodes        | 620      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 6151     |\n",
            "|    total_timesteps | 98595    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.78     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.15    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98494    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98610, episode_reward=643.52 +/- 7.99\n",
            "Episode length: 211.80 +/- 2.71\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 644      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98610    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 3.1      |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.0945  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98509    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98640, episode_reward=865.45 +/- 2.81\n",
            "Episode length: 259.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 259      |\n",
            "|    mean_reward     | 865      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98640    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 3.34     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.576    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98539    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98670, episode_reward=849.37 +/- 6.89\n",
            "Episode length: 264.00 +/- 5.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 849      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98670    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 107      |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.0496   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98569    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98700, episode_reward=1043.62 +/- 61.87\n",
            "Episode length: 321.20 +/- 14.39\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 321      |\n",
            "|    mean_reward     | 1.04e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98700    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -186     |\n",
            "|    critic_loss     | 3.25     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.0958   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98599    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98730, episode_reward=690.30 +/- 59.70\n",
            "Episode length: 223.20 +/- 14.27\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 223      |\n",
            "|    mean_reward     | 690      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98730    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -186     |\n",
            "|    critic_loss     | 3.07     |\n",
            "|    ent_coef        | 0.0621   |\n",
            "|    ent_coef_loss   | -0.526   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98629    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98760, episode_reward=631.54 +/- 2.89\n",
            "Episode length: 209.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 209      |\n",
            "|    mean_reward     | 632      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98760    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.37     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.429   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98659    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98790, episode_reward=633.03 +/- 4.09\n",
            "Episode length: 210.00 +/- 1.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 633      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98790    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.04     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.709   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98689    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98820, episode_reward=634.62 +/- 1.24\n",
            "Episode length: 206.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 207      |\n",
            "|    mean_reward     | 635      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98820    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -177     |\n",
            "|    critic_loss     | 3.52     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98719    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98850, episode_reward=830.63 +/- 18.79\n",
            "Episode length: 256.00 +/- 6.16\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 256      |\n",
            "|    mean_reward     | 831      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98850    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 3.29     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.882    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98749    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98880, episode_reward=617.95 +/- 8.18\n",
            "Episode length: 207.20 +/- 2.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 207      |\n",
            "|    mean_reward     | 618      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98880    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.31     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.339   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98779    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98910, episode_reward=676.69 +/- 92.20\n",
            "Episode length: 224.60 +/- 28.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 225      |\n",
            "|    mean_reward     | 677      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98910    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 3.95     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.368   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98809    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98940, episode_reward=869.10 +/- 148.62\n",
            "Episode length: 286.80 +/- 46.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 287      |\n",
            "|    mean_reward     | 869      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98940    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 4.93     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.433   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98839    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=98970, episode_reward=890.17 +/- 4.41\n",
            "Episode length: 269.80 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 890      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 98970    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.88     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.00833 |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98869    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99000, episode_reward=819.98 +/- 88.66\n",
            "Episode length: 270.00 +/- 27.93\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 270      |\n",
            "|    mean_reward     | 820      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99000    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 3.47     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.523    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98899    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99030, episode_reward=640.98 +/- 1.51\n",
            "Episode length: 211.60 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 641      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99030    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.15     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.178    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98929    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99060, episode_reward=820.41 +/- 165.61\n",
            "Episode length: 272.80 +/- 54.54\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 273      |\n",
            "|    mean_reward     | 820      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99060    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 3.18     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.0779   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98959    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99090, episode_reward=821.14 +/- 15.29\n",
            "Episode length: 251.60 +/- 4.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 821      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99090    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 2.6      |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 1.09     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 98989    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99120, episode_reward=861.27 +/- 15.99\n",
            "Episode length: 260.20 +/- 3.66\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 260      |\n",
            "|    mean_reward     | 861      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99120    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 4.25     |\n",
            "|    ent_coef        | 0.062    |\n",
            "|    ent_coef_loss   | 0.248    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99019    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99150, episode_reward=690.34 +/- 101.11\n",
            "Episode length: 224.40 +/- 29.86\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 224      |\n",
            "|    mean_reward     | 690      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99150    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.89     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99049    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99180, episode_reward=780.87 +/- 90.72\n",
            "Episode length: 244.40 +/- 20.29\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 244      |\n",
            "|    mean_reward     | 781      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99180    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 2.77     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.339   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99079    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99210, episode_reward=647.83 +/- 12.39\n",
            "Episode length: 214.00 +/- 3.58\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 648      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99210    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 4.2      |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.0638   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99109    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99240, episode_reward=940.20 +/- 161.45\n",
            "Episode length: 302.40 +/- 47.99\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 302      |\n",
            "|    mean_reward     | 940      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99240    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.84     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.498   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99139    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99270, episode_reward=673.58 +/- 8.52\n",
            "Episode length: 220.00 +/- 2.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 220      |\n",
            "|    mean_reward     | 674      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99270    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 2.51     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.257    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99169    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99300, episode_reward=904.27 +/- 7.24\n",
            "Episode length: 274.00 +/- 2.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 274      |\n",
            "|    mean_reward     | 904      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99300    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.66     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.178    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99199    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99330, episode_reward=651.45 +/- 6.40\n",
            "Episode length: 215.80 +/- 1.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 216      |\n",
            "|    mean_reward     | 651      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99330    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 6.34     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.455    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99229    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99360, episode_reward=648.02 +/- 2.42\n",
            "Episode length: 212.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 648      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99360    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 2.39     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | -0.0271  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99259    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99390, episode_reward=633.49 +/- 2.63\n",
            "Episode length: 211.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 211      |\n",
            "|    mean_reward     | 633      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99390    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.16     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | 0.373    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99289    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99420, episode_reward=831.02 +/- 83.71\n",
            "Episode length: 255.20 +/- 19.81\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 255      |\n",
            "|    mean_reward     | 831      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99420    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 4.1      |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | 0.301    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99319    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99450, episode_reward=883.98 +/- 6.63\n",
            "Episode length: 264.00 +/- 2.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 264      |\n",
            "|    mean_reward     | 884      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99450    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.39     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.515   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99349    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99480, episode_reward=643.94 +/- 4.09\n",
            "Episode length: 213.80 +/- 1.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 644      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99480    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -178     |\n",
            "|    critic_loss     | 3.05     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.705   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99379    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99510, episode_reward=903.65 +/- 2.86\n",
            "Episode length: 275.20 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 275      |\n",
            "|    mean_reward     | 904      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99510    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 5.82     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 0.867    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99409    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99540, episode_reward=802.98 +/- 74.39\n",
            "Episode length: 251.60 +/- 17.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 252      |\n",
            "|    mean_reward     | 803      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99540    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -186     |\n",
            "|    critic_loss     | 9.85     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.043   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99439    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99570, episode_reward=1076.63 +/- 16.16\n",
            "Episode length: 330.20 +/- 10.26\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 330      |\n",
            "|    mean_reward     | 1.08e+03 |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99570    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -185     |\n",
            "|    critic_loss     | 6.46     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.292   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99469    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99600, episode_reward=639.37 +/- 3.19\n",
            "Episode length: 214.40 +/- 1.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 214      |\n",
            "|    mean_reward     | 639      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99600    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 3.16     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.778   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99499    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99630, episode_reward=646.47 +/- 3.10\n",
            "Episode length: 210.20 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 210      |\n",
            "|    mean_reward     | 646      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99630    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -179     |\n",
            "|    critic_loss     | 2.98     |\n",
            "|    ent_coef        | 0.0618   |\n",
            "|    ent_coef_loss   | -0.00891 |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99529    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99660, episode_reward=799.73 +/- 103.01\n",
            "Episode length: 250.20 +/- 23.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 250      |\n",
            "|    mean_reward     | 800      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99660    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 2.74     |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | -0.723   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99559    |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 252      |\n",
            "|    ep_rew_mean     | 754      |\n",
            "| time/              |          |\n",
            "|    episodes        | 624      |\n",
            "|    fps             | 16       |\n",
            "|    time_elapsed    | 6229     |\n",
            "|    total_timesteps | 99671    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -176     |\n",
            "|    critic_loss     | 3.56     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | 1.31     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99570    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99690, episode_reward=645.63 +/- 10.59\n",
            "Episode length: 212.00 +/- 3.74\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 646      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99690    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -187     |\n",
            "|    critic_loss     | 3.78     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.0763   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99589    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99720, episode_reward=709.16 +/- 54.23\n",
            "Episode length: 220.80 +/- 12.92\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 221      |\n",
            "|    mean_reward     | 709      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99720    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -180     |\n",
            "|    critic_loss     | 3.78     |\n",
            "|    ent_coef        | 0.0619   |\n",
            "|    ent_coef_loss   | -0.67    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99619    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99750, episode_reward=870.87 +/- 10.16\n",
            "Episode length: 268.80 +/- 4.31\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 269      |\n",
            "|    mean_reward     | 871      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99750    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -175     |\n",
            "|    critic_loss     | 4.27     |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | 0.245    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99649    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99780, episode_reward=636.83 +/- 3.47\n",
            "Episode length: 211.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 212      |\n",
            "|    mean_reward     | 637      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99780    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 5.13     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.136    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99679    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99810, episode_reward=688.08 +/- 87.26\n",
            "Episode length: 221.00 +/- 19.14\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 221      |\n",
            "|    mean_reward     | 688      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99810    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 178      |\n",
            "|    ent_coef        | 0.0617   |\n",
            "|    ent_coef_loss   | -0.372   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99709    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99840, episode_reward=763.74 +/- 95.04\n",
            "Episode length: 239.20 +/- 19.53\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 239      |\n",
            "|    mean_reward     | 764      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99840    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -182     |\n",
            "|    critic_loss     | 7.68     |\n",
            "|    ent_coef        | 0.0616   |\n",
            "|    ent_coef_loss   | 0.0515   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99739    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99870, episode_reward=684.51 +/- 93.22\n",
            "Episode length: 223.60 +/- 22.97\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 224      |\n",
            "|    mean_reward     | 685      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99870    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -184     |\n",
            "|    critic_loss     | 7.72     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.319   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99769    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99900, episode_reward=696.93 +/- 77.31\n",
            "Episode length: 225.00 +/- 16.01\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 225      |\n",
            "|    mean_reward     | 697      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99900    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -181     |\n",
            "|    critic_loss     | 3.2      |\n",
            "|    ent_coef        | 0.0612   |\n",
            "|    ent_coef_loss   | -0.571   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99799    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99930, episode_reward=776.25 +/- 116.39\n",
            "Episode length: 257.80 +/- 36.59\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 258      |\n",
            "|    mean_reward     | 776      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99930    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -174     |\n",
            "|    critic_loss     | 4.33     |\n",
            "|    ent_coef        | 0.0613   |\n",
            "|    ent_coef_loss   | 0.369    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99829    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99960, episode_reward=880.28 +/- 6.85\n",
            "Episode length: 268.00 +/- 2.37\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 268      |\n",
            "|    mean_reward     | 880      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99960    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -172     |\n",
            "|    critic_loss     | 3.82     |\n",
            "|    ent_coef        | 0.0614   |\n",
            "|    ent_coef_loss   | -0.585   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99859    |\n",
            "---------------------------------\n",
            "Eval num_timesteps=99990, episode_reward=917.92 +/- 141.51\n",
            "Episode length: 295.00 +/- 40.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 295      |\n",
            "|    mean_reward     | 918      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 99990    |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -183     |\n",
            "|    critic_loss     | 2.8      |\n",
            "|    ent_coef        | 0.0615   |\n",
            "|    ent_coef_loss   | 0.298    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 99889    |\n",
            "---------------------------------\n",
            "Modello salvato come ./sim2real/models/df_JR.zip\n",
            "/usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "given by the platformdirs library.  To remove this warning and\n",
            "see the appropriate new directories, set the environment variable\n",
            "`JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "The use of platformdirs will be the default in `jupyter_core` v6\n",
            "  from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n",
            "Figure(1000x600)\n",
            "Reward plot saved in ./sim2real/plots/rewards_plot.png\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [3.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRain con CheckPoint (Crea Modelli Stiwy)"
      ],
      "metadata": {
        "id": "LgQ7S2Ouf715"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zwVmKH3PH9-f",
        "outputId": "81c64830-f9fe-430c-94ea-a48ffd75eda6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py --model_name df_CK_TEST --total_timesteps 10000 --env CustomHopper-source-v0"
      ],
      "metadata": {
        "id": "drdi8-t6f7Ey",
        "outputId": "78a4f561-db4f-457b-be62-17673163c085",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-13 11:02:13.366969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-13 11:02:13.413074: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-13 11:02:13.426345: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-13 11:02:13.455813: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-13 11:02:15.673418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/google/colab/_import_hooks/_altair.py:16: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
            "  import imp  # pylint: disable=deprecated-module\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "Checkpoint file /content/drive/MyDrive/sim2real/models/df_CK_TEST_checkpoint.zip does not exist.\n",
            "Using cpu device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "Eval num_timesteps=30, episode_reward=18.87 +/- 0.16\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 30       |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=60, episode_reward=18.79 +/- 0.26\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 16.8     |\n",
            "|    ep_rew_mean     | 11.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 145      |\n",
            "|    time_elapsed    | 0        |\n",
            "|    total_timesteps | 67       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90, episode_reward=18.71 +/- 0.20\n",
            "Episode length: 24.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.6     |\n",
            "|    mean_reward     | 18.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120, episode_reward=18.82 +/- 0.07\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 120      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150, episode_reward=18.87 +/- 0.17\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 150      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 21.5     |\n",
            "|    ep_rew_mean     | 20.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 158      |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 172      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=180, episode_reward=18.85 +/- 0.11\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 180      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=210, episode_reward=18.93 +/- 0.06\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 210      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 19.1     |\n",
            "|    ep_rew_mean     | 17.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 229      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=240, episode_reward=18.93 +/- 0.15\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 240      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=270, episode_reward=18.86 +/- 0.14\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 270      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.8     |\n",
            "|    ep_rew_mean     | 15       |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 148      |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 285      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300, episode_reward=18.97 +/- 0.17\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 300      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=330, episode_reward=18.94 +/- 0.10\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 330      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.7     |\n",
            "|    ep_rew_mean     | 14.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 354      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=360, episode_reward=19.02 +/- 0.11\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 360      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=390, episode_reward=18.92 +/- 0.14\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 390      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=420, episode_reward=18.93 +/- 0.14\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 420      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.7     |\n",
            "|    ep_rew_mean     | 14.9     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 146      |\n",
            "|    time_elapsed    | 2        |\n",
            "|    total_timesteps | 425      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=450, episode_reward=18.82 +/- 0.18\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 450      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480, episode_reward=18.99 +/- 0.11\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 480      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.3     |\n",
            "|    ep_rew_mean     | 13.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 147      |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 485      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=510, episode_reward=18.89 +/- 0.07\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 510      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=540, episode_reward=18.78 +/- 0.15\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 540      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17       |\n",
            "|    ep_rew_mean     | 13.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 149      |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 543      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=570, episode_reward=18.93 +/- 0.18\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 570      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=600, episode_reward=18.87 +/- 0.08\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 600      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.4     |\n",
            "|    ep_rew_mean     | 12.8     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 626      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=630, episode_reward=18.94 +/- 0.12\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 630      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=660, episode_reward=18.90 +/- 0.08\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 660      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=690, episode_reward=18.91 +/- 0.16\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 690      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.2     |\n",
            "|    ep_rew_mean     | 12.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 151      |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 690      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=720, episode_reward=18.97 +/- 0.09\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 720      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=750, episode_reward=19.01 +/- 0.11\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 750      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.4     |\n",
            "|    ep_rew_mean     | 12.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 155      |\n",
            "|    time_elapsed    | 4        |\n",
            "|    total_timesteps | 765      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=780, episode_reward=18.80 +/- 0.09\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 780      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=810, episode_reward=18.82 +/- 0.12\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 810      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.3     |\n",
            "|    ep_rew_mean     | 12.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 156      |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 829      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840, episode_reward=18.83 +/- 0.13\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 840      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=870, episode_reward=18.77 +/- 0.17\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 870      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17       |\n",
            "|    ep_rew_mean     | 12.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 153      |\n",
            "|    time_elapsed    | 5        |\n",
            "|    total_timesteps | 886      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=900, episode_reward=19.03 +/- 0.10\n",
            "Episode length: 25.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 19       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 900      |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=930, episode_reward=18.72 +/- 0.19\n",
            "Episode length: 24.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.6     |\n",
            "|    mean_reward     | 18.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 930      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960, episode_reward=18.84 +/- 0.20\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 960      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=990, episode_reward=18.87 +/- 0.09\n",
            "Episode length: 24.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 24.8     |\n",
            "|    mean_reward     | 18.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 990      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 17.7     |\n",
            "|    ep_rew_mean     | 13.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 150      |\n",
            "|    time_elapsed    | 6        |\n",
            "|    total_timesteps | 990      |\n",
            "---------------------------------\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py\", line 198, in <module>\n",
            "    main()\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py\", line 183, in main\n",
            "    model, reward_logger = train_model(args, env, hyperparameters)\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py\", line 150, in train_model\n",
            "    save_training_state(checkpoint_path, model, reward_logger, start_timesteps)\n",
            "  File \"/content/project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py\", line 79, in save_training_state\n",
            "    'evaluations_results': reward_logger.evaluations_results.tolist(),  # Convert to list\n",
            "AttributeError: 'list' object has no attribute 'tolist'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 3**\n",
        "\n",
        "Task 3 Train two agents with your algorithm of choice, on the source and target domains respectively. Then,\n",
        "test each model and report its average return over 50 test episodes. In particular, report results for the\n",
        "following “training→test” configurations:\n",
        "● source→source,\n",
        "● source→target (lower bound),\n",
        "● target→target (upper bound).\n",
        "Test with different hyperparameters and report the best results found together with the parameters used.\n",
        "Question 3.1 Why do we expect lower performances from the “source→target” configuration w.r.t. the\n",
        "“target→target”?\n",
        "Question 3.2 If higher performances can be reached by training on the target environment directly, what\n",
        "prevents us from doing so (in a sim-to-real setting)?\n"
      ],
      "metadata": {
        "id": "F1A4wHFZIoSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source->Source"
      ],
      "metadata": {
        "id": "CHD2OM2zVvq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_100k --env source"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKXEOnK2It8A",
        "outputId": "9f7d1888-26e0-42c5-c822-162fbd4b12f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-11 17:08:06.988029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 17:08:07.032358: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 17:08:07.046965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 17:08:07.093706: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 17:08:09.665554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "Mean reward: 1.9794784910173622, Std: 0.22928069925799968\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source -> Target"
      ],
      "metadata": {
        "id": "jBRkzZQ2VzTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_100k --env target"
      ],
      "metadata": {
        "id": "i3eCsyBAO0_T",
        "outputId": "32004be7-ea20-41e0-fe86-7987febe65c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-11 17:08:49.877505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 17:08:49.901926: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 17:08:49.908867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 17:08:49.925389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 17:08:51.402222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "Mean reward: 4.117900728830718, Std: 0.20261402056807074\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [3.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target -> Target"
      ],
      "metadata": {
        "id": "oZMXI3FhV2qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name target_def_100k --env target"
      ],
      "metadata": {
        "id": "VvoXoumaPA7D",
        "outputId": "ab7071c7-4872-4bf1-f127-ebb23bc8b007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-11 17:10:35.256666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 17:10:35.296871: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 17:10:35.309283: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 17:10:35.336744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 17:10:36.782265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "Mean reward: 787.8366466903686, Std: 100.96606713081682\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [3.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target -> Source"
      ],
      "metadata": {
        "id": "IsMRdUxdV76N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name target_def_100k --env source"
      ],
      "metadata": {
        "id": "rSDoVaHGV_rP",
        "outputId": "3d5e3dde-8a47-4c95-de6d-6f99d2729163",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-11 17:10:58.391339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-11 17:10:58.414311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-11 17:10:58.420966: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-11 17:10:58.437014: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-11 17:10:59.715308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "Mean reward: 889.6089495968819, Std: 6.620765572688358\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TEST"
      ],
      "metadata": {
        "id": "BgNKnsQy7NFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python  project-sim2real-rialti-giunti-gjinaj/test_random_policy.py"
      ],
      "metadata": {
        "id": "3rlyRUTP5p7o",
        "outputId": "ae395dee-741a-4666-cf1a-eed20959aa2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n",
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "\u001b[1;34m[swscaler @ 0x6686300] \u001b[0m\u001b[0;33mWarning: data is not aligned! This can lead to a speed loss\n",
            "\u001b[0mVideo saved at sim2real/plots/random_policy.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VIDEO OUR MODELS"
      ],
      "metadata": {
        "id": "uJfviZcLET_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name target_def_100k --env source --render true"
      ],
      "metadata": {
        "id": "h6dtU2VtEIAs",
        "outputId": "286d25a4-bdcd-4c04-dcc6-f38b35fe6828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-12 15:34:33.231250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-12 15:34:33.256421: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-12 15:34:33.263841: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-12 15:34:33.281826: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-12 15:34:34.677215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "Mean reward: 887.8703376674653, Std: 7.726954021633901\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:280: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
            "  logger.warn(\n",
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (500, 500) to (512, 512) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "\u001b[1;34m[swscaler @ 0x7174380] \u001b[0m\u001b[0;33mWarning: data is not aligned! This can lead to a speed loss\n",
            "\u001b[0mVideo saved at sim2realtmp/plots/target_def_100k_on_source.mp4\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 4"
      ],
      "metadata": {
        "id": "dpSUk4Gef167"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python  project-sim2real-rialti-giunti-gjinaj/task4.py --model_name target_def_100k --env source"
      ],
      "metadata": {
        "id": "ym9llfvGf1S0"
      },
      "execution_count": null,
      "outputs": []
>>>>>>> f42d68bfcd91b5c6e025235d44a46817b6b43650
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target -> Target"
   ],
   "metadata": {
    "id": "oZMXI3FhV2qE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name target_def_100k --env target"
   ],
   "metadata": {
    "id": "VvoXoumaPA7D",
    "outputId": "ab7071c7-4872-4bf1-f127-ebb23bc8b007",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target -> Source"
   ],
   "metadata": {
    "id": "IsMRdUxdV76N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name target_def_100k --env source"
   ],
   "metadata": {
    "id": "rSDoVaHGV_rP",
    "outputId": "3d5e3dde-8a47-4c95-de6d-6f99d2729163",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "TEST"
   ],
   "metadata": {
    "id": "BgNKnsQy7NFM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python  project-sim2real-rialti-giunti-gjinaj/test_random_policy.py"
   ],
   "metadata": {
    "id": "3rlyRUTP5p7o"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
