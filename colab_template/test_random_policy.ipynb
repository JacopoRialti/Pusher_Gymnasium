{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PolitoVandal/project-sim2real-rialti-giunti-gjinaj/blob/main/colab_template/test_random_policy.ipynb)"
   ],
   "metadata": {
    "id": "h9EafyX8NLlA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!rm -rf \"project-sim2real-rialti-giunti-gjinaj\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!rm -rf \"sim2real\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!rm -rf \"sample_data\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
    "!git clone https://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@github.com/PolitoVandal/project-sim2real-rialti-giunti-gjinaj\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "if not os.path.exists('sim2realtmp/models'):\n",
    "    os.makedirs('sim2realtmp/models')\n",
    "\n",
    "if not os.path.exists('sim2realtmp/plots'):\n",
    "    os.makedirs('sim2realtmp/plots')"
   ],
   "metadata": {
    "id": "UG4ZM2KG_SLS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "495515a6-6a42-4e54-d936-2dc686350bdc",
    "collapsed": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AW6XT0jSJI8e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6b19177d-0148-4fba-a927-218060d4447f",
    "collapsed": true
   },
   "source": [
    "!apt-get install -y \\\n",
    "    libgl1-mesa-dev \\\n",
    "    libgl1-mesa-glx \\\n",
    "    libglew-dev \\\n",
    "    libosmesa6-dev \\\n",
    "    software-properties-common\n",
    "\n",
    "!apt-get install -y patchelf\n",
    "!apt-get install -y xvfb ffmpeg\n",
    "\n",
    "!pip install gym\n",
    "!pip install free-mujoco-py\n",
    "!pip install importlib-metadata\n",
    "!pip install shimmy\n",
    "!pip install stable-baselines3[extra]\n",
    "!pip install pyvirtualdisplay"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the custom Hopper environment and provided util functions\n",
    "\n",
    "\n",
    "\n",
    "1.   Upload `custom_hopper.zip` to the current session's file storage\n",
    "2.   Un-zip it by running cell below\n"
   ],
   "metadata": {
    "id": "gwIRXGd5K3xJ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 1**\n"
   ],
   "metadata": {
    "id": "nUs5gQXCaiRS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.chdir('project-sim2real-rialti-giunti-gjinaj/colab_template')\n",
    "!unzip custom_hopper.zip"
   ],
   "metadata": {
    "id": "T9WsofDVLaCC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3d37a0d6-c49d-4f1a-b8b8-de3af35228ac",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "id": "7pJC_JevLf1f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "**Test a random policy on the Gym Hopper environment**\n",
    "\n",
    "\\\n",
    "\n",
    "\n",
    "\n",
    "Play around with this code to get familiar with the\n",
    "Hopper environment.\n",
    "\n",
    "For example, what happens if you don't reset the environment\n",
    "even after the episode is over?\n",
    "When exactly is the episode over?\n",
    "What is an action here?"
   ],
   "metadata": {
    "id": "W4NsuF6pJPVJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import gym\n",
    "from env.custom_hopper import *"
   ],
   "metadata": {
    "id": "uTYmUufrJTNl",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e861c60a-d232-4e8a-9551-1c8265f63dd0",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make('CustomHopper-source-v0')\n",
    "# env = gym.make('CustomHopper-target-v0')\n",
    "print('State space:', env.observation_space)  # state-space\n",
    "print('Action space:', env.action_space)  # action-space\n",
    "print(\"Mass values of each link:\", env.model.body_mass)\n",
    "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper\n",
    "print(\"Bodies defined in the environment:\", env.model.body_names)\n",
    "print(\"Number of degrees of freedom (DoFs) of the robot:\", env.model.nv)\n",
    "print(\"Number of DoFs for each body:\", env.model.body_dofnum)\n",
    "print(\"Number of actuators:\", env.model.nu)"
   ],
   "metadata": {
    "id": "QcCfCGg-Jyc3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a3b4fde-dc9c-4019-98e6-fe8091230afd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis of the Hopper Environment in MuJoCo\n",
    "\n",
    "This document provides a detailed analysis of the Hopper environment based on the provided terminal output and information from the MuJoCo and Gym documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.1: What is the state space in the Hopper environment? Is it discrete or continuous?**\n",
    "\n",
    "### **Answer:**\n",
    "- **State Space Description**:\n",
    "  The state space is represented by a **Box** object with shape `(11,)`, meaning it is a vector of 11 continuous values. These values typically include the positions, velocities, and potentially other sensory data (like contact forces) of the Hopper's components.\n",
    "  \n",
    "- **Nature of State Space**:\n",
    "  The state space is **continuous**, as indicated by the range `(-inf, inf)` and the use of the `Box` object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.2: What is the action space in the Hopper environment? Is it discrete or continuous?**\n",
    "\n",
    "### **Answer:**\n",
    "- **Action Space Description**:\n",
    "  The action space is represented by a **Box** object with shape `(3,)`, meaning it consists of 3 continuous values. These correspond to the torques applied to the actuators controlling the Hopper's joints.\n",
    "\n",
    "- **Nature of Action Space**:\n",
    "  The action space is **continuous**, as indicated by the range `(-1.0, 1.0)` and the use of the `Box` object.\n",
    "\n",
    "---\n",
    "\n",
    "## **Question 1.3: What is the mass value of each link of the Hopper environment, in the source and target variants respectively?**\n",
    "\n",
    "### **Answer:**\n",
    "- **Mass Values for the Source Variant**:\n",
    "  From the terminal output:\n",
    "Mass values of each link: [0. 2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
    "These correspond to the masses of the bodies:\n",
    "- `world`: 0.0 (fixed reference point)\n",
    "- `torso`: 2.5343\n",
    "- `thigh`: 3.9270\n",
    "- `leg`: 2.7143\n",
    "- `foot`: 5.0894\n",
    "\n",
    "- **Mass Values for the Target Variant**:\n",
    "The mass values for the target variant can be obtained by switching the environment initialization to:\n",
    "```python\n",
    "env = gym.make('CustomHopper-target-v0')\n",
    "(Ensure to re-run the relevant command to print the mass values.)\n",
    "\n",
    "Comparison of Source and Target Variants: Any differences in mass values between the source and target variants must be explicitly checked in the respective initialization. These differences typically simulate dynamics variability to test robustness.\n",
    "\n",
    "Additional Information Derived from the Environment\n",
    "Bodies Defined in the Environment:\n",
    "\n",
    "('world', 'torso', 'thigh', 'leg', 'foot')\n",
    "These represent the main components of the Hopper system.\n",
    "\n",
    "Number of Degrees of Freedom (DoFs) of the Robot: 6\n",
    "This includes translational and rotational movements of the robot.\n",
    "\n",
    "Number of DoFs for Each Body:\n",
    "[0 3 1 1 1]\n",
    "world: 0 (fixed body)\n",
    "torso: 3\n",
    "thigh: 1\n",
    "leg: 1\n",
    "foot: 1\n",
    "\n",
    "Number of Actuators:\n",
    "The Hopper has 3 actuators controlling its joints.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The Hopper environment features a continuous state and action space, making it well-suited for reinforcement learning tasks. Understanding the dynamics, including body masses and degrees of freedom, is crucial for designing robust controllers and algorithms."
   ],
   "metadata": {
    "id": "AYhXnui3h70A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "n_episodes = 5\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "  done = False\n",
    "  observation = env.reset()\t # Reset environment to initial state\n",
    "\n",
    "  while not done:  # Until the episode is over\n",
    "\n",
    "    action = env.action_space.sample()\t# Sample random action\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\t# Step the simulator to the next timestep"
   ],
   "metadata": {
    "id": "DT1oXr8HJ05h"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 2**"
   ],
   "metadata": {
    "id": "Dfas8tGOax8E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task2.py --model_name df_JR --total_timesteps 100000 --env CustomHopper-target-v0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKi913Qgj2xI",
    "outputId": "ad27e56e-3628-4070-c480-110cbdea7612",
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "TRain con CheckPoint (Crea Modelli Stiwy)"
   ],
   "metadata": {
    "id": "LgQ7S2Ouf715"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "zwVmKH3PH9-f",
    "outputId": "81c64830-f9fe-430c-94ea-a48ffd75eda6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!python project-sim2real-rialti-giunti-gjinaj/task2CheckPoints.py --model_name df_CK_TEST --total_timesteps 10000 --env CustomHopper-source-v0"
   ],
   "metadata": {
    "id": "drdi8-t6f7Ey",
    "outputId": "78a4f561-db4f-457b-be62-17673163c085",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 3**\n",
    "\n",
    "Task 3 Train two agents with your algorithm of choice, on the source and target domains respectively. Then,\n",
    "test each model and report its average return over 50 test episodes. In particular, report results for the\n",
    "following “training→test” configurations:\n",
    "● source→source,\n",
    "● source→target (lower bound),\n",
    "● target→target (upper bound).\n",
    "Test with different hyperparameters and report the best results found together with the parameters used.\n",
    "Question 3.1 Why do we expect lower performances from the “source→target” configuration w.r.t. the\n",
    "“target→target”?\n",
    "Question 3.2 If higher performances can be reached by training on the target environment directly, what\n",
    "prevents us from doing so (in a sim-to-real setting)?\n"
   ],
   "metadata": {
    "id": "F1A4wHFZIoSH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Source->Source"
   ],
   "metadata": {
    "id": "CHD2OM2zVvq8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_500k --env source"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKXEOnK2It8A",
    "outputId": "e85f2add-31f7-412f-ccdc-1b307acb4a59"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Source -> Target"
   ],
   "metadata": {
    "id": "jBRkzZQ2VzTP"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name source_def_500k --env target"
   ],
   "metadata": {
    "id": "i3eCsyBAO0_T",
    "outputId": "3544148a-4e03-4658-a69a-e383fedabdce",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target -> Target"
   ],
   "metadata": {
    "id": "oZMXI3FhV2qE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name task2_target_500k --env target"
   ],
   "metadata": {
    "id": "VvoXoumaPA7D",
    "outputId": "e768d42c-de1e-411a-9805-122e39049bda",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Target -> Source"
   ],
   "metadata": {
    "id": "IsMRdUxdV76N"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name task2_target_500k --env source"
   ],
   "metadata": {
    "id": "rSDoVaHGV_rP",
    "outputId": "d8f44626-cc00-4e0c-b731-91418bdd6d5a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training and Testing Configurations\n",
    "We trained two RL agents using our selected algorithm on the Hopper environment's source and target domains. The models were evaluated in the following configurations, reporting the average return over 50 test episodes:\n",
    "\n",
    "Source → Source: Training and testing in the source domain.\n",
    "Source → Target: Training in the source domain and testing in the target domain (lower bound).\n",
    "Target → Target: Training and testing in the target domain (upper bound).\n",
    "Target → Source: Training in the target domain and testing in the source domain.\n",
    "Results Summary\n",
    "Training → Testing Configuration\tMean Reward\tStd Deviation\tDynamics Parameters (Torso Mass)\n",
    "Source → Source\t1604.10\t2.15\t2.5343\n",
    "Source → Target\t1210.71\t296.44\t3.5343\n",
    "Target → Target\t1326.08\t1.82\t3.5343\n",
    "Target → Source\t1171.93\t4.69\t2.5343\n",
    "\n",
    "\n",
    "Analysis of Results\n",
    "\n",
    "\n",
    "Question 3.1: Why do we expect lower performance from the “Source → Target” configuration compared to “Target → Target”?\n",
    "In the Source → Target configuration, the policy is trained in the source domain with dynamics that differ from those in the target domain. Specifically, the torso mass differs between the two environments, causing a \"reality gap.\" This discrepancy makes the policy less optimal when tested in the target domain, as it was not exposed to the target's dynamics during training. Conversely, in the Target → Target configuration, the policy is trained and tested under the same conditions, leading to better performance.\n",
    "\n",
    "Question 3.2: Why don't we train directly in the target environment in a sim-to-real setting?\n",
    "Training directly in the target environment (i.e., the real world) is often impractical for the following reasons:\n",
    "\n",
    "Safety Concerns: Real robots may get damaged during training due to suboptimal actions generated in early episodes.\n",
    "Cost: Training in the real world can be expensive due to wear and tear on hardware components.\n",
    "Time Constraints: Training in the real world is slower compared to simulation, as physical constraints like hardware reset times and environmental changes affect the pace of learning.\n",
    "Repeatability: Simulation provides controlled and repeatable conditions, enabling systematic debugging and testing of RL algorithms.\n",
    "By training in simulation and employing techniques like domain randomization, we aim to develop policies that transfer effectively to the real world while addressing these challenges."
   ],
   "metadata": {
    "id": "12gRmXcTHa5G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "TEST"
   ],
   "metadata": {
    "id": "BgNKnsQy7NFM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python  project-sim2real-rialti-giunti-gjinaj/test_random_policy.py"
   ],
   "metadata": {
    "id": "3rlyRUTP5p7o",
    "outputId": "ae395dee-741a-4666-cf1a-eed20959aa2f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "VIDEO OUR MODELS"
   ],
   "metadata": {
    "id": "uJfviZcLET_I"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name task2_target_500k --env target --render true"
   ],
   "metadata": {
    "id": "h6dtU2VtEIAs",
    "outputId": "93da6da2-e8b3-4e07-b226-0ebe32b4d7c1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Task 4"
   ],
   "metadata": {
    "id": "dpSUk4Gef167"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!python  project-sim2real-rialti-giunti-gjinaj/task4.py --model_name target_def_100k --env source"
   ],
   "metadata": {
    "id": "ym9llfvGf1S0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test del random env su source"
   ],
   "metadata": {
    "id": "IDHi5ECmDuwq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name udr_source_500k  --env source"
   ],
   "metadata": {
    "id": "BEMAgrwSDdez",
    "outputId": "a2884d66-e72a-45c3-c1b6-2f15aadc372b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "test del random env su target"
   ],
   "metadata": {
    "id": "1reGTHnuEBsG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "! python  project-sim2real-rialti-giunti-gjinaj/task3.py --model_name udr_source_500k  --env target"
   ],
   "metadata": {
    "id": "7HYnQWiGEAeY",
    "outputId": "1b7a6211-6211-49c8-b3bd-6f30d696341a",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 4,
   "outputs": []
  }
 ]
}
