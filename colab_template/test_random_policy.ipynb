{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PolitoVandal/project-sim2real-rialti-giunti-gjinaj/blob/main/colab_template/test_random_policy.ipynb)"
      ],
      "metadata": {
        "id": "h9EafyX8NLlA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf \"project-sim2real-rialti-giunti-gjinaj\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sim2real\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!rm -rf \"sample_data\"  # Cancella la cartella e il suo contenuto usando il comando !rm\n",
        "!git clone https://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@github.com/PolitoVandal/project-sim2real-rialti-giunti-gjinaj\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "if not os.path.exists('sim2real/models'):\n",
        "    os.makedirs('sim2real/models')\n",
        "\n",
        "if not os.path.exists('sim2real/plots'):\n",
        "    os.makedirs('sim2real/plots')"
      ],
      "metadata": {
        "id": "UG4ZM2KG_SLS",
        "outputId": "d5bd8399-7191-4d47-dcd4-3a06535a3914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'project-sim2real-rialti-giunti-gjinaj'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 175 (delta 17), reused 18 (delta 12), pack-reused 144 (from 2)\u001b[K\n",
            "Receiving objects: 100% (175/175), 959.67 KiB | 5.13 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AW6XT0jSJI8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1736a11-f9e0-49be-980a-b17f303406ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "software-properties-common is already the newest version (0.99.22.9).\n",
            "The following additional packages will be installed:\n",
            "  libegl-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "The following NEW packages will be installed:\n",
            "  libegl-dev libgl-dev libgl1-mesa-dev libgl1-mesa-glx libgles-dev libgles1 libglew-dev\n",
            "  libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev libglx-dev libopengl-dev libosmesa6\n",
            "  libosmesa6-dev\n",
            "0 upgraded, 15 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,013 kB of archives.\n",
            "After this operation, 19.4 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5,584 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [6,848 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libglew-dev amd64 2.2.0-4 [287 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6 amd64 23.2.1-1ubuntu3.1~22.04.3 [3,115 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libosmesa6-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [8,984 B]\n",
            "Fetched 4,013 kB in 1s (3,340 kB/s)\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../01-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../02-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
            "Preparing to unpack .../03-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
            "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../04-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../05-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../06-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglew-dev:amd64.\n",
            "Preparing to unpack .../12-libglew-dev_2.2.0-4_amd64.deb ...\n",
            "Unpacking libglew-dev:amd64 (2.2.0-4) ...\n",
            "Selecting previously unselected package libosmesa6:amd64.\n",
            "Preparing to unpack .../13-libosmesa6_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libosmesa6-dev:amd64.\n",
            "Preparing to unpack .../14-libosmesa6-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libosmesa6:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libosmesa6-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglew-dev:amd64 (2.2.0-4) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  patchelf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 72.1 kB of archives.\n",
            "After this operation, 186 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 patchelf amd64 0.14.3-1 [72.1 kB]\n",
            "Fetched 72.1 kB in 0s (145 kB/s)\n",
            "Selecting previously unselected package patchelf.\n",
            "(Reading database ... 123772 files and directories currently installed.)\n",
            "Preparing to unpack .../patchelf_0.14.3-1_amd64.deb ...\n",
            "Unpacking patchelf (0.14.3-1) ...\n",
            "Setting up patchelf (0.14.3-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting free-mujoco-py\n",
            "  Downloading free_mujoco_py-2.1.6-py3-none-any.whl.metadata (586 bytes)\n",
            "Collecting Cython<0.30.0,>=0.29.24 (from free-mujoco-py)\n",
            "  Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.17.1)\n",
            "Collecting fasteners==0.15 (from free-mujoco-py)\n",
            "  Downloading fasteners-0.15-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting glfw<2.0.0,>=1.4.0 (from free-mujoco-py)\n",
            "  Downloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (2.36.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from free-mujoco-py) (1.26.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fasteners==0.15->free-mujoco-py) (1.17.0)\n",
            "Collecting monotonic>=0.1 (from fasteners==0.15->free-mujoco-py)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi<2.0.0,>=1.15.0->free-mujoco-py) (2.22)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0.0,>=2.9.0->free-mujoco-py) (11.1.0)\n",
            "Downloading free_mujoco_py-2.1.6-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.15-py2.py3-none-any.whl (23 kB)\n",
            "Downloading Cython-0.29.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-1.12.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38-none-manylinux2014_x86_64.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, glfw, fasteners, Cython, free-mujoco-py\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.11\n",
            "    Uninstalling Cython-3.0.11:\n",
            "      Successfully uninstalled Cython-3.0.11\n",
            "Successfully installed Cython-0.29.37 fasteners-0.15 free-mujoco-py-2.1.6 glfw-1.12.0 monotonic-1.6\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (8.5.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata) (3.21.0)\n",
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy) (1.26.4)\n",
            "Collecting gymnasium>=1.0.0a1 (from shimmy)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium>=1.0.0a1->shimmy)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, shimmy\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 shimmy-2.0.0\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.69.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.4.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ale-py, stable-baselines3\n",
            "Successfully installed ale-py-0.10.1 stable-baselines3-2.4.1\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "\n",
        "!apt-get install -y patchelf\n",
        "\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!pip install importlib-metadata\n",
        "!pip install shimmy\n",
        "!pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the custom Hopper environment and provided util functions\n",
        "\n",
        "\n",
        "\n",
        "1.   Upload `custom_hopper.zip` to the current session's file storage\n",
        "2.   Un-zip it by running cell below\n"
      ],
      "metadata": {
        "id": "gwIRXGd5K3xJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 1**\n"
      ],
      "metadata": {
        "id": "nUs5gQXCaiRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('project-sim2real-rialti-giunti-gjinaj/colab_template')\n",
        "!unzip custom_hopper.zip"
      ],
      "metadata": {
        "id": "T9WsofDVLaCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d37a0d6-c49d-4f1a-b8b8-de3af35228ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  custom_hopper.zip\n",
            "   creating: env/\n",
            "  inflating: env/__init__.py         \n",
            "  inflating: env/custom_hopper.py    \n",
            "  inflating: env/mujoco_env.py       \n",
            "   creating: env/assets/\n",
            "  inflating: env/assets/hopper.xml   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7pJC_JevLf1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Test a random policy on the Gym Hopper environment**\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "\n",
        "Play around with this code to get familiar with the\n",
        "Hopper environment.\n",
        "\n",
        "For example, what happens if you don't reset the environment\n",
        "even after the episode is over?\n",
        "When exactly is the episode over?\n",
        "What is an action here?"
      ],
      "metadata": {
        "id": "W4NsuF6pJPVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from env.custom_hopper import *"
      ],
      "metadata": {
        "id": "uTYmUufrJTNl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e861c60a-d232-4e8a-9551-1c8265f63dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx because it changed.\n",
            "[1/1] Cythonizing /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.pyx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:running build_ext\n",
            "INFO:root:building 'mujoco_py.cymj' extension\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o -fopenmp -w\n",
            "INFO:root:x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/mujoco_py -I/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c /usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.c -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -fopenmp -w\n",
            "INFO:root:creating /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py\n",
            "INFO:root:x86_64-linux-gnu-gcc -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/cymj.o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/temp.linux-x86_64-cpython-310/usr/local/lib/python3.10/dist-packages/mujoco_py/gl/osmesashim.o -L/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/usr/local/lib/python3.10/dist-packages/mujoco_py/binaries/linux/mujoco210/bin -lmujoco210 -lglewosmesa -lOSMesa -lGL -o /usr/local/lib/python3.10/dist-packages/mujoco_py/generated/_pyxbld_2.0.2.13_310_linuxcpuextensionbuilder/lib.linux-x86_64-cpython-310/mujoco_py/cymj.cpython-310-x86_64-linux-gnu.so -fopenmp\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CustomHopper-source-v0')\n",
        "# env = gym.make('CustomHopper-target-v0')\n",
        "print('State space:', env.observation_space)  # state-space\n",
        "print('Action space:', env.action_space)  # action-space\n",
        "print(\"Mass values of each link:\", env.model.body_mass)\n",
        "print('Dynamics parameters:', env.get_parameters())  # masses of each link of the Hopper\n",
        "print(\"Bodies defined in the environment:\", env.model.body_names)\n",
        "print(\"Number of degrees of freedom (DoFs) of the robot:\", env.model.nv)\n",
        "print(\"Number of DoFs for each body:\", env.model.body_dofnum)\n",
        "print(\"Number of actuators:\", env.model.nu)"
      ],
      "metadata": {
        "id": "QcCfCGg-Jyc3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3b4fde-dc9c-4019-98e6-fe8091230afd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Mass values of each link: [0.         2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
            "Bodies defined in the environment: ('world', 'torso', 'thigh', 'leg', 'foot')\n",
            "Number of degrees of freedom (DoFs) of the robot: 6\n",
            "Number of DoFs for each body: [0 3 1 1 1]\n",
            "Number of actuators: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Hopper Environment in MuJoCo\n",
        "\n",
        "This document provides a detailed analysis of the Hopper environment based on the provided terminal output and information from the MuJoCo and Gym documentation.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.1: What is the state space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **State Space Description**:\n",
        "  The state space is represented by a **Box** object with shape `(11,)`, meaning it is a vector of 11 continuous values. These values typically include the positions, velocities, and potentially other sensory data (like contact forces) of the Hopper's components.\n",
        "  \n",
        "- **Nature of State Space**:\n",
        "  The state space is **continuous**, as indicated by the range `(-inf, inf)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.2: What is the action space in the Hopper environment? Is it discrete or continuous?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Action Space Description**:\n",
        "  The action space is represented by a **Box** object with shape `(3,)`, meaning it consists of 3 continuous values. These correspond to the torques applied to the actuators controlling the Hopper's joints.\n",
        "\n",
        "- **Nature of Action Space**:\n",
        "  The action space is **continuous**, as indicated by the range `(-1.0, 1.0)` and the use of the `Box` object.\n",
        "\n",
        "---\n",
        "\n",
        "## **Question 1.3: What is the mass value of each link of the Hopper environment, in the source and target variants respectively?**\n",
        "\n",
        "### **Answer:**\n",
        "- **Mass Values for the Source Variant**:\n",
        "  From the terminal output:\n",
        "Mass values of each link: [0. 2.53429174 3.92699082 2.71433605 5.0893801 ]\n",
        "These correspond to the masses of the bodies:\n",
        "- `world`: 0.0 (fixed reference point)\n",
        "- `torso`: 2.5343\n",
        "- `thigh`: 3.9270\n",
        "- `leg`: 2.7143\n",
        "- `foot`: 5.0894\n",
        "\n",
        "- **Mass Values for the Target Variant**:\n",
        "The mass values for the target variant can be obtained by switching the environment initialization to:\n",
        "```python\n",
        "env = gym.make('CustomHopper-target-v0')\n",
        "(Ensure to re-run the relevant command to print the mass values.)\n",
        "\n",
        "Comparison of Source and Target Variants: Any differences in mass values between the source and target variants must be explicitly checked in the respective initialization. These differences typically simulate dynamics variability to test robustness.\n",
        "\n",
        "Additional Information Derived from the Environment\n",
        "Bodies Defined in the Environment:\n",
        "\n",
        "('world', 'torso', 'thigh', 'leg', 'foot')\n",
        "These represent the main components of the Hopper system.\n",
        "\n",
        "Number of Degrees of Freedom (DoFs) of the Robot: 6\n",
        "This includes translational and rotational movements of the robot.\n",
        "\n",
        "Number of DoFs for Each Body:\n",
        "[0 3 1 1 1]\n",
        "world: 0 (fixed body)\n",
        "torso: 3\n",
        "thigh: 1\n",
        "leg: 1\n",
        "foot: 1\n",
        "\n",
        "Number of Actuators:\n",
        "The Hopper has 3 actuators controlling its joints.\n",
        "\n",
        "Conclusion\n",
        "\n",
        "The Hopper environment features a continuous state and action space, making it well-suited for reinforcement learning tasks. Understanding the dynamics, including body masses and degrees of freedom, is crucial for designing robust controllers and algorithms."
      ],
      "metadata": {
        "id": "AYhXnui3h70A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_episodes = 5\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "  done = False\n",
        "  observation = env.reset()\t # Reset environment to initial state\n",
        "\n",
        "  while not done:  # Until the episode is over\n",
        "\n",
        "    action = env.action_space.sample()\t# Sample random action\n",
        "\n",
        "    observation, reward, done, info = env.step(action)\t# Step the simulator to the next timestep"
      ],
      "metadata": {
        "id": "DT1oXr8HJ05h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TASK 2**"
      ],
      "metadata": {
        "id": "Dfas8tGOax8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python  project-sim2real-rialti-giunti-gjinaj/trainTest.py --model_name sac_hopper --total_timesteps 5000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKi913Qgj2xI",
        "outputId": "ed2b06a7-3d3c-4f5e-d58e-3cc3a67a4999"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/Cython/Distutils/old_build_ext.py:15: DeprecationWarning: dep_util is Deprecated. Use functions from setuptools instead.\n",
            "  from distutils.dep_util import newer, newer_group\n",
            "<frozen importlib._bootstrap>:283: DeprecationWarning: the load_module() method is deprecated and slated for removal in Python 3.12; use exec_module() instead\n",
            "2025-01-10 14:40:39.881091: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-10 14:40:39.905683: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-10 14:40:39.912724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-10 14:40:39.930332: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-10 14:40:41.513837: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "Using cpu device\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:190: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `return_info` to return information from the environment resetting.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(done, (bool, np.bool8)):\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n",
            "Eval num_timesteps=30, episode_reward=150.68 +/- 5.34\n",
            "Episode length: 122.20 +/- 4.83\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 122      |\n",
            "|    mean_reward     | 151      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 30       |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 14.5     |\n",
            "|    ep_rew_mean     | 8.33     |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 94       |\n",
            "|    time_elapsed    | 0        |\n",
            "|    total_timesteps | 58       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=60, episode_reward=149.25 +/- 7.83\n",
            "Episode length: 120.60 +/- 7.34\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 121      |\n",
            "|    mean_reward     | 149      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=90, episode_reward=149.21 +/- 5.78\n",
            "Episode length: 120.60 +/- 5.24\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 121      |\n",
            "|    mean_reward     | 149      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 90       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=120, episode_reward=131.01 +/- 0.94\n",
            "Episode length: 87.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.4     |\n",
            "|    mean_reward     | 131      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 120      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -3.87    |\n",
            "|    critic_loss     | 2.42     |\n",
            "|    ent_coef        | 0.995    |\n",
            "|    ent_coef_loss   | -0.0274  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 19       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=150, episode_reward=107.64 +/- 1.47\n",
            "Episode length: 78.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.6     |\n",
            "|    mean_reward     | 108      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 150      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -4.67    |\n",
            "|    critic_loss     | 0.927    |\n",
            "|    ent_coef        | 0.986    |\n",
            "|    ent_coef_loss   | -0.0733  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 49       |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 19.2     |\n",
            "|    ep_rew_mean     | 17.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 39       |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 154      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -4.61    |\n",
            "|    critic_loss     | 1.24     |\n",
            "|    ent_coef        | 0.985    |\n",
            "|    ent_coef_loss   | -0.0785  |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 53       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=180, episode_reward=71.06 +/- 6.78\n",
            "Episode length: 72.40 +/- 5.12\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 72.4     |\n",
            "|    mean_reward     | 71.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 180      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.11    |\n",
            "|    critic_loss     | 0.629    |\n",
            "|    ent_coef        | 0.977    |\n",
            "|    ent_coef_loss   | -0.118   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 79       |\n",
            "---------------------------------\n",
            "Eval num_timesteps=210, episode_reward=21.66 +/- 0.53\n",
            "Episode length: 30.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 30.4     |\n",
            "|    mean_reward     | 21.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 210      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.38    |\n",
            "|    critic_loss     | 0.523    |\n",
            "|    ent_coef        | 0.968    |\n",
            "|    ent_coef_loss   | -0.163   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 109      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=240, episode_reward=34.52 +/- 0.97\n",
            "Episode length: 42.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 42.4     |\n",
            "|    mean_reward     | 34.5     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 240      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.59    |\n",
            "|    critic_loss     | 0.524    |\n",
            "|    ent_coef        | 0.959    |\n",
            "|    ent_coef_loss   | -0.208   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 139      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=270, episode_reward=61.23 +/- 8.25\n",
            "Episode length: 54.00 +/- 4.90\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 54       |\n",
            "|    mean_reward     | 61.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 270      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -5.93    |\n",
            "|    critic_loss     | 0.407    |\n",
            "|    ent_coef        | 0.951    |\n",
            "|    ent_coef_loss   | -0.254   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 169      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=300, episode_reward=38.84 +/- 2.94\n",
            "Episode length: 40.20 +/- 1.94\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 40.2     |\n",
            "|    mean_reward     | 38.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 300      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -6.32    |\n",
            "|    critic_loss     | 0.419    |\n",
            "|    ent_coef        | 0.942    |\n",
            "|    ent_coef_loss   | -0.298   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 199      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 26.2     |\n",
            "|    ep_rew_mean     | 28       |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 34       |\n",
            "|    time_elapsed    | 9        |\n",
            "|    total_timesteps | 314      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -6.48    |\n",
            "|    critic_loss     | 0.452    |\n",
            "|    ent_coef        | 0.938    |\n",
            "|    ent_coef_loss   | -0.319   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 213      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=330, episode_reward=92.91 +/- 1.62\n",
            "Episode length: 64.00 +/- 1.41\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 64       |\n",
            "|    mean_reward     | 92.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 330      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -6.62    |\n",
            "|    critic_loss     | 0.358    |\n",
            "|    ent_coef        | 0.934    |\n",
            "|    ent_coef_loss   | -0.344   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 229      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=360, episode_reward=78.86 +/- 0.68\n",
            "Episode length: 50.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 50.6     |\n",
            "|    mean_reward     | 78.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 360      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -6.89    |\n",
            "|    critic_loss     | 0.331    |\n",
            "|    ent_coef        | 0.926    |\n",
            "|    ent_coef_loss   | -0.391   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 259      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=390, episode_reward=91.02 +/- 1.58\n",
            "Episode length: 62.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 62.2     |\n",
            "|    mean_reward     | 91       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 390      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -7.07    |\n",
            "|    critic_loss     | 0.393    |\n",
            "|    ent_coef        | 0.917    |\n",
            "|    ent_coef_loss   | -0.431   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 289      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=420, episode_reward=89.76 +/- 1.25\n",
            "Episode length: 60.60 +/- 1.20\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 60.6     |\n",
            "|    mean_reward     | 89.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 420      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -7.49    |\n",
            "|    critic_loss     | 0.429    |\n",
            "|    ent_coef        | 0.909    |\n",
            "|    ent_coef_loss   | -0.479   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 319      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 27.2     |\n",
            "|    ep_rew_mean     | 29.3     |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 436      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -7.71    |\n",
            "|    critic_loss     | 0.256    |\n",
            "|    ent_coef        | 0.905    |\n",
            "|    ent_coef_loss   | -0.496   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 335      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=450, episode_reward=109.33 +/- 5.35\n",
            "Episode length: 80.80 +/- 4.62\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 80.8     |\n",
            "|    mean_reward     | 109      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 450      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -7.81    |\n",
            "|    critic_loss     | 0.752    |\n",
            "|    ent_coef        | 0.901    |\n",
            "|    ent_coef_loss   | -0.519   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 349      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=480, episode_reward=91.34 +/- 2.32\n",
            "Episode length: 62.80 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 62.8     |\n",
            "|    mean_reward     | 91.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 480      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.05    |\n",
            "|    critic_loss     | 0.383    |\n",
            "|    ent_coef        | 0.893    |\n",
            "|    ent_coef_loss   | -0.562   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 379      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=510, episode_reward=90.29 +/- 3.23\n",
            "Episode length: 61.20 +/- 2.23\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.2     |\n",
            "|    mean_reward     | 90.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 510      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.36    |\n",
            "|    critic_loss     | 0.564    |\n",
            "|    ent_coef        | 0.885    |\n",
            "|    ent_coef_loss   | -0.605   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 409      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=540, episode_reward=74.75 +/- 1.78\n",
            "Episode length: 45.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 45.8     |\n",
            "|    mean_reward     | 74.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 540      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.37    |\n",
            "|    critic_loss     | 0.529    |\n",
            "|    ent_coef        | 0.877    |\n",
            "|    ent_coef_loss   | -0.643   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 439      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=570, episode_reward=71.16 +/- 1.49\n",
            "Episode length: 43.00 +/- 1.10\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 43       |\n",
            "|    mean_reward     | 71.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 570      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.75    |\n",
            "|    critic_loss     | 0.719    |\n",
            "|    ent_coef        | 0.869    |\n",
            "|    ent_coef_loss   | -0.694   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 469      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 28.6     |\n",
            "|    ep_rew_mean     | 28.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 19       |\n",
            "|    total_timesteps | 571      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.95    |\n",
            "|    critic_loss     | 0.734    |\n",
            "|    ent_coef        | 0.869    |\n",
            "|    ent_coef_loss   | -0.686   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 470      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=600, episode_reward=67.59 +/- 0.99\n",
            "Episode length: 40.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 40.4     |\n",
            "|    mean_reward     | 67.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 600      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -8.96    |\n",
            "|    critic_loss     | 0.558    |\n",
            "|    ent_coef        | 0.862    |\n",
            "|    ent_coef_loss   | -0.731   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 499      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=630, episode_reward=58.45 +/- 1.09\n",
            "Episode length: 34.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 34.4     |\n",
            "|    mean_reward     | 58.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 630      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -9.59    |\n",
            "|    critic_loss     | 0.31     |\n",
            "|    ent_coef        | 0.854    |\n",
            "|    ent_coef_loss   | -0.782   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 529      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=660, episode_reward=56.95 +/- 1.17\n",
            "Episode length: 33.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 33.4     |\n",
            "|    mean_reward     | 56.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 660      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -9.88    |\n",
            "|    critic_loss     | 0.577    |\n",
            "|    ent_coef        | 0.847    |\n",
            "|    ent_coef_loss   | -0.819   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 559      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=690, episode_reward=59.58 +/- 0.10\n",
            "Episode length: 35.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 35       |\n",
            "|    mean_reward     | 59.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 690      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.2    |\n",
            "|    critic_loss     | 1.34     |\n",
            "|    ent_coef        | 0.839    |\n",
            "|    ent_coef_loss   | -0.859   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 589      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 30       |\n",
            "|    ep_rew_mean     | 33.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 24       |\n",
            "|    total_timesteps | 719      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.4    |\n",
            "|    critic_loss     | 1.51     |\n",
            "|    ent_coef        | 0.832    |\n",
            "|    ent_coef_loss   | -0.901   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 618      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=720, episode_reward=55.45 +/- 1.10\n",
            "Episode length: 32.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 32.4     |\n",
            "|    mean_reward     | 55.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 720      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.3    |\n",
            "|    critic_loss     | 0.758    |\n",
            "|    ent_coef        | 0.832    |\n",
            "|    ent_coef_loss   | -0.892   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 619      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=750, episode_reward=50.74 +/- 0.89\n",
            "Episode length: 29.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 29.2     |\n",
            "|    mean_reward     | 50.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 750      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.7    |\n",
            "|    critic_loss     | 0.552    |\n",
            "|    ent_coef        | 0.824    |\n",
            "|    ent_coef_loss   | -0.924   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 649      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=780, episode_reward=49.92 +/- 1.07\n",
            "Episode length: 28.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 28.6     |\n",
            "|    mean_reward     | 49.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 780      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -10.8    |\n",
            "|    critic_loss     | 1.01     |\n",
            "|    ent_coef        | 0.817    |\n",
            "|    ent_coef_loss   | -0.984   |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 679      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=810, episode_reward=50.39 +/- 0.21\n",
            "Episode length: 29.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 29       |\n",
            "|    mean_reward     | 50.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 810      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.1    |\n",
            "|    critic_loss     | 0.608    |\n",
            "|    ent_coef        | 0.81     |\n",
            "|    ent_coef_loss   | -1       |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 709      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=840, episode_reward=51.03 +/- 0.89\n",
            "Episode length: 29.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 29.4     |\n",
            "|    mean_reward     | 51       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 840      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.4    |\n",
            "|    critic_loss     | 1.16     |\n",
            "|    ent_coef        | 0.803    |\n",
            "|    ent_coef_loss   | -1.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 739      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=870, episode_reward=49.42 +/- 1.02\n",
            "Episode length: 28.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 28.4     |\n",
            "|    mean_reward     | 49.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 870      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.1    |\n",
            "|    critic_loss     | 1.78     |\n",
            "|    ent_coef        | 0.796    |\n",
            "|    ent_coef_loss   | -1.1     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 769      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 31.5     |\n",
            "|    ep_rew_mean     | 34.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 30       |\n",
            "|    total_timesteps | 882      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -11.8    |\n",
            "|    critic_loss     | 0.809    |\n",
            "|    ent_coef        | 0.793    |\n",
            "|    ent_coef_loss   | -1.1     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 781      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=900, episode_reward=46.21 +/- 1.10\n",
            "Episode length: 26.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.4     |\n",
            "|    mean_reward     | 46.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 900      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.2    |\n",
            "|    critic_loss     | 2.26     |\n",
            "|    ent_coef        | 0.789    |\n",
            "|    ent_coef_loss   | -1.12    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 799      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=930, episode_reward=46.99 +/- 0.70\n",
            "Episode length: 26.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.8     |\n",
            "|    mean_reward     | 47       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 930      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -12.7    |\n",
            "|    critic_loss     | 2.35     |\n",
            "|    ent_coef        | 0.782    |\n",
            "|    ent_coef_loss   | -1.17    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 829      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=960, episode_reward=47.09 +/- 0.83\n",
            "Episode length: 26.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.8     |\n",
            "|    mean_reward     | 47.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 960      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -13.1    |\n",
            "|    critic_loss     | 1.98     |\n",
            "|    ent_coef        | 0.775    |\n",
            "|    ent_coef_loss   | -1.23    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 859      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=990, episode_reward=51.33 +/- 0.87\n",
            "Episode length: 29.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 29.6     |\n",
            "|    mean_reward     | 51.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 990      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -13      |\n",
            "|    critic_loss     | 0.738    |\n",
            "|    ent_coef        | 0.769    |\n",
            "|    ent_coef_loss   | -1.25    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 889      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1020, episode_reward=47.78 +/- 0.67\n",
            "Episode length: 26.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.8     |\n",
            "|    mean_reward     | 47.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1020     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -13.8    |\n",
            "|    critic_loss     | 2.52     |\n",
            "|    ent_coef        | 0.762    |\n",
            "|    ent_coef_loss   | -1.28    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 919      |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 32.6     |\n",
            "|    ep_rew_mean     | 39.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 35       |\n",
            "|    total_timesteps | 1044     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14.4    |\n",
            "|    critic_loss     | 1.39     |\n",
            "|    ent_coef        | 0.757    |\n",
            "|    ent_coef_loss   | -1.32    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 943      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1050, episode_reward=46.37 +/- 0.93\n",
            "Episode length: 26.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.4     |\n",
            "|    mean_reward     | 46.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1050     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14      |\n",
            "|    critic_loss     | 1.6      |\n",
            "|    ent_coef        | 0.755    |\n",
            "|    ent_coef_loss   | -1.33    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 949      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1080, episode_reward=48.20 +/- 0.94\n",
            "Episode length: 27.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27.4     |\n",
            "|    mean_reward     | 48.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14.2    |\n",
            "|    critic_loss     | 3.65     |\n",
            "|    ent_coef        | 0.749    |\n",
            "|    ent_coef_loss   | -1.36    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 979      |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1110, episode_reward=48.20 +/- 0.84\n",
            "Episode length: 27.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27.6     |\n",
            "|    mean_reward     | 48.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1110     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -14.9    |\n",
            "|    critic_loss     | 1.41     |\n",
            "|    ent_coef        | 0.742    |\n",
            "|    ent_coef_loss   | -1.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1009     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1140, episode_reward=46.07 +/- 1.13\n",
            "Episode length: 26.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26       |\n",
            "|    mean_reward     | 46.1     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1140     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -15.2    |\n",
            "|    critic_loss     | 3.51     |\n",
            "|    ent_coef        | 0.736    |\n",
            "|    ent_coef_loss   | -1.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1039     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1170, episode_reward=48.27 +/- 0.91\n",
            "Episode length: 27.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27.4     |\n",
            "|    mean_reward     | 48.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1170     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -15.7    |\n",
            "|    critic_loss     | 0.558    |\n",
            "|    ent_coef        | 0.73     |\n",
            "|    ent_coef_loss   | -1.46    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1069     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 33.3     |\n",
            "|    ep_rew_mean     | 42.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 41       |\n",
            "|    total_timesteps | 1198     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16      |\n",
            "|    critic_loss     | 2.57     |\n",
            "|    ent_coef        | 0.724    |\n",
            "|    ent_coef_loss   | -1.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1097     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1200, episode_reward=46.93 +/- 0.94\n",
            "Episode length: 26.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.6     |\n",
            "|    mean_reward     | 46.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -15.9    |\n",
            "|    critic_loss     | 2.83     |\n",
            "|    ent_coef        | 0.723    |\n",
            "|    ent_coef_loss   | -1.49    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1099     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1230, episode_reward=44.70 +/- 1.56\n",
            "Episode length: 25.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25       |\n",
            "|    mean_reward     | 44.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1230     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16.2    |\n",
            "|    critic_loss     | 4.94     |\n",
            "|    ent_coef        | 0.717    |\n",
            "|    ent_coef_loss   | -1.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1129     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1260, episode_reward=44.97 +/- 0.75\n",
            "Episode length: 25.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25.2     |\n",
            "|    mean_reward     | 45       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16.9    |\n",
            "|    critic_loss     | 5.61     |\n",
            "|    ent_coef        | 0.711    |\n",
            "|    ent_coef_loss   | -1.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1159     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1290, episode_reward=45.34 +/- 0.95\n",
            "Episode length: 25.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25.6     |\n",
            "|    mean_reward     | 45.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1290     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.2    |\n",
            "|    critic_loss     | 1.16     |\n",
            "|    ent_coef        | 0.705    |\n",
            "|    ent_coef_loss   | -1.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1189     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1320, episode_reward=47.86 +/- 0.88\n",
            "Episode length: 26.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.4     |\n",
            "|    mean_reward     | 47.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1320     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -16.7    |\n",
            "|    critic_loss     | 3.03     |\n",
            "|    ent_coef        | 0.699    |\n",
            "|    ent_coef_loss   | -1.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1219     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 33.4     |\n",
            "|    ep_rew_mean     | 44.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 45       |\n",
            "|    total_timesteps | 1336     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.4    |\n",
            "|    critic_loss     | 3.07     |\n",
            "|    ent_coef        | 0.696    |\n",
            "|    ent_coef_loss   | -1.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1235     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1350, episode_reward=45.93 +/- 0.86\n",
            "Episode length: 25.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25.4     |\n",
            "|    mean_reward     | 45.9     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1350     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -17.6    |\n",
            "|    critic_loss     | 4        |\n",
            "|    ent_coef        | 0.693    |\n",
            "|    ent_coef_loss   | -1.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1249     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1380, episode_reward=49.21 +/- 1.14\n",
            "Episode length: 27.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27       |\n",
            "|    mean_reward     | 49.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1380     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.5    |\n",
            "|    critic_loss     | 2.87     |\n",
            "|    ent_coef        | 0.687    |\n",
            "|    ent_coef_loss   | -1.68    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1279     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1410, episode_reward=45.99 +/- 1.55\n",
            "Episode length: 25.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25.4     |\n",
            "|    mean_reward     | 46       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1410     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.1    |\n",
            "|    critic_loss     | 4.85     |\n",
            "|    ent_coef        | 0.681    |\n",
            "|    ent_coef_loss   | -1.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1309     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1440, episode_reward=49.64 +/- 1.10\n",
            "Episode length: 27.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27       |\n",
            "|    mean_reward     | 49.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1440     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.2    |\n",
            "|    critic_loss     | 4.3      |\n",
            "|    ent_coef        | 0.675    |\n",
            "|    ent_coef_loss   | -1.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1339     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 33.3     |\n",
            "|    ep_rew_mean     | 45.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 49       |\n",
            "|    total_timesteps | 1464     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.2    |\n",
            "|    critic_loss     | 1.68     |\n",
            "|    ent_coef        | 0.671    |\n",
            "|    ent_coef_loss   | -1.82    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1363     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1470, episode_reward=47.57 +/- 1.33\n",
            "Episode length: 26.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26.2     |\n",
            "|    mean_reward     | 47.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1470     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -18.5    |\n",
            "|    critic_loss     | 3.54     |\n",
            "|    ent_coef        | 0.67     |\n",
            "|    ent_coef_loss   | -1.82    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1369     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1500, episode_reward=46.42 +/- 0.82\n",
            "Episode length: 25.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 25.4     |\n",
            "|    mean_reward     | 46.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1500     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.8    |\n",
            "|    critic_loss     | 6.37     |\n",
            "|    ent_coef        | 0.664    |\n",
            "|    ent_coef_loss   | -1.84    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1399     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1530, episode_reward=47.70 +/- 0.28\n",
            "Episode length: 26.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 26       |\n",
            "|    mean_reward     | 47.7     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1530     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -19.9    |\n",
            "|    critic_loss     | 2.33     |\n",
            "|    ent_coef        | 0.658    |\n",
            "|    ent_coef_loss   | -1.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1429     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1560, episode_reward=56.97 +/- 0.44\n",
            "Episode length: 31.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 31       |\n",
            "|    mean_reward     | 57       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1560     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.5    |\n",
            "|    critic_loss     | 4.54     |\n",
            "|    ent_coef        | 0.653    |\n",
            "|    ent_coef_loss   | -1.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1459     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 32.9     |\n",
            "|    ep_rew_mean     | 46       |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 53       |\n",
            "|    total_timesteps | 1577     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.9    |\n",
            "|    critic_loss     | 2.37     |\n",
            "|    ent_coef        | 0.65     |\n",
            "|    ent_coef_loss   | -1.94    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1476     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1590, episode_reward=49.24 +/- 0.47\n",
            "Episode length: 27.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 27       |\n",
            "|    mean_reward     | 49.2     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1590     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.7    |\n",
            "|    critic_loss     | 2.42     |\n",
            "|    ent_coef        | 0.647    |\n",
            "|    ent_coef_loss   | -1.92    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1489     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1620, episode_reward=56.98 +/- 0.41\n",
            "Episode length: 31.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 31       |\n",
            "|    mean_reward     | 57       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1620     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.9    |\n",
            "|    critic_loss     | 3.85     |\n",
            "|    ent_coef        | 0.642    |\n",
            "|    ent_coef_loss   | -1.89    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1519     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1650, episode_reward=54.82 +/- 0.21\n",
            "Episode length: 30.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 30       |\n",
            "|    mean_reward     | 54.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1650     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.7    |\n",
            "|    critic_loss     | 2.18     |\n",
            "|    ent_coef        | 0.637    |\n",
            "|    ent_coef_loss   | -1.98    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1549     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1680, episode_reward=71.27 +/- 0.67\n",
            "Episode length: 40.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 40.8     |\n",
            "|    mean_reward     | 71.3     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1680     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -20.8    |\n",
            "|    critic_loss     | 2.73     |\n",
            "|    ent_coef        | 0.631    |\n",
            "|    ent_coef_loss   | -2.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1579     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1710, episode_reward=57.62 +/- 0.27\n",
            "Episode length: 31.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 31       |\n",
            "|    mean_reward     | 57.6     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1710     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -21.9    |\n",
            "|    critic_loss     | 2.58     |\n",
            "|    ent_coef        | 0.626    |\n",
            "|    ent_coef_loss   | -1.93    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1609     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1740, episode_reward=60.83 +/- 0.84\n",
            "Episode length: 33.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 33.4     |\n",
            "|    mean_reward     | 60.8     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1740     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.4    |\n",
            "|    critic_loss     | 3.87     |\n",
            "|    ent_coef        | 0.621    |\n",
            "|    ent_coef_loss   | -2       |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1639     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1770, episode_reward=95.35 +/- 5.40\n",
            "Episode length: 54.00 +/- 3.03\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 54       |\n",
            "|    mean_reward     | 95.4     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.1    |\n",
            "|    critic_loss     | 6.39     |\n",
            "|    ent_coef        | 0.616    |\n",
            "|    ent_coef_loss   | -2.13    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1669     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 34.1     |\n",
            "|    ep_rew_mean     | 49.4     |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 29       |\n",
            "|    time_elapsed    | 60       |\n",
            "|    total_timesteps | 1773     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.6    |\n",
            "|    critic_loss     | 3.01     |\n",
            "|    ent_coef        | 0.615    |\n",
            "|    ent_coef_loss   | -2.11    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1672     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1800, episode_reward=171.83 +/- 2.05\n",
            "Episode length: 86.80 +/- 1.33\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.8     |\n",
            "|    mean_reward     | 172      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.8    |\n",
            "|    critic_loss     | 3.41     |\n",
            "|    ent_coef        | 0.611    |\n",
            "|    ent_coef_loss   | -2.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1699     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1830, episode_reward=105.26 +/- 0.99\n",
            "Episode length: 55.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 55.8     |\n",
            "|    mean_reward     | 105      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1830     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -22.9    |\n",
            "|    critic_loss     | 2.76     |\n",
            "|    ent_coef        | 0.606    |\n",
            "|    ent_coef_loss   | -2.13    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1729     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1860, episode_reward=104.65 +/- 1.10\n",
            "Episode length: 55.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 55.2     |\n",
            "|    mean_reward     | 105      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1860     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.3    |\n",
            "|    critic_loss     | 3.5      |\n",
            "|    ent_coef        | 0.601    |\n",
            "|    ent_coef_loss   | -2.13    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1759     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1890, episode_reward=193.07 +/- 1.77\n",
            "Episode length: 97.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.2     |\n",
            "|    mean_reward     | 193      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1890     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.6    |\n",
            "|    critic_loss     | 6.71     |\n",
            "|    ent_coef        | 0.596    |\n",
            "|    ent_coef_loss   | -2.16    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1789     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1920, episode_reward=203.55 +/- 0.60\n",
            "Episode length: 99.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.4     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1920     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -23.8    |\n",
            "|    critic_loss     | 1.64     |\n",
            "|    ent_coef        | 0.591    |\n",
            "|    ent_coef_loss   | -2.21    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1819     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=1950, episode_reward=195.61 +/- 1.32\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 196      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.7    |\n",
            "|    critic_loss     | 11.4     |\n",
            "|    ent_coef        | 0.586    |\n",
            "|    ent_coef_loss   | -2.29    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1849     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=1980, episode_reward=169.77 +/- 3.01\n",
            "Episode length: 87.20 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.2     |\n",
            "|    mean_reward     | 170      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 1980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.1    |\n",
            "|    critic_loss     | 3.21     |\n",
            "|    ent_coef        | 0.581    |\n",
            "|    ent_coef_loss   | -2.29    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1879     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2010, episode_reward=158.33 +/- 5.03\n",
            "Episode length: 81.80 +/- 2.79\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.8     |\n",
            "|    mean_reward     | 158      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2010     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -25.5    |\n",
            "|    critic_loss     | 2.59     |\n",
            "|    ent_coef        | 0.577    |\n",
            "|    ent_coef_loss   | -2.29    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1909     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2040, episode_reward=156.92 +/- 2.82\n",
            "Episode length: 81.20 +/- 1.60\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 81.2     |\n",
            "|    mean_reward     | 157      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2040     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -24.9    |\n",
            "|    critic_loss     | 1.8      |\n",
            "|    ent_coef        | 0.572    |\n",
            "|    ent_coef_loss   | -2.23    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1939     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 36.9     |\n",
            "|    ep_rew_mean     | 56.2     |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 28       |\n",
            "|    time_elapsed    | 72       |\n",
            "|    total_timesteps | 2067     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.3    |\n",
            "|    critic_loss     | 6.36     |\n",
            "|    ent_coef        | 0.568    |\n",
            "|    ent_coef_loss   | -2.38    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1966     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2070, episode_reward=111.94 +/- 1.22\n",
            "Episode length: 60.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 60       |\n",
            "|    mean_reward     | 112      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2070     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -26.7    |\n",
            "|    critic_loss     | 2.14     |\n",
            "|    ent_coef        | 0.567    |\n",
            "|    ent_coef_loss   | -2.38    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1969     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2100, episode_reward=150.60 +/- 1.14\n",
            "Episode length: 78.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.6     |\n",
            "|    mean_reward     | 151      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2100     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -27      |\n",
            "|    critic_loss     | 4        |\n",
            "|    ent_coef        | 0.563    |\n",
            "|    ent_coef_loss   | -2.34    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 1999     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2130, episode_reward=150.10 +/- 3.67\n",
            "Episode length: 78.40 +/- 1.85\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.4     |\n",
            "|    mean_reward     | 150      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2130     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -27      |\n",
            "|    critic_loss     | 8.54     |\n",
            "|    ent_coef        | 0.558    |\n",
            "|    ent_coef_loss   | -2.23    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2029     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2160, episode_reward=110.36 +/- 1.00\n",
            "Episode length: 59.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 59.4     |\n",
            "|    mean_reward     | 110      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2160     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -27.6    |\n",
            "|    critic_loss     | 7.09     |\n",
            "|    ent_coef        | 0.554    |\n",
            "|    ent_coef_loss   | -2.47    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2059     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2190, episode_reward=105.66 +/- 0.64\n",
            "Episode length: 57.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 57.6     |\n",
            "|    mean_reward     | 106      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2190     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -27.8    |\n",
            "|    critic_loss     | 2.9      |\n",
            "|    ent_coef        | 0.549    |\n",
            "|    ent_coef_loss   | -2.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2089     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2220, episode_reward=104.10 +/- 0.79\n",
            "Episode length: 56.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 56.8     |\n",
            "|    mean_reward     | 104      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2220     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -28.1    |\n",
            "|    critic_loss     | 4.57     |\n",
            "|    ent_coef        | 0.545    |\n",
            "|    ent_coef_loss   | -2.42    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2119     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2250, episode_reward=121.35 +/- 1.05\n",
            "Episode length: 63.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 63.2     |\n",
            "|    mean_reward     | 121      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2250     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -28.2    |\n",
            "|    critic_loss     | 4.27     |\n",
            "|    ent_coef        | 0.54     |\n",
            "|    ent_coef_loss   | -2.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2149     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2280, episode_reward=93.95 +/- 0.57\n",
            "Episode length: 51.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 51.2     |\n",
            "|    mean_reward     | 94       |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2280     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -28.9    |\n",
            "|    critic_loss     | 5.64     |\n",
            "|    ent_coef        | 0.536    |\n",
            "|    ent_coef_loss   | -2.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2179     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2310, episode_reward=95.50 +/- 0.89\n",
            "Episode length: 52.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 52       |\n",
            "|    mean_reward     | 95.5     |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2310     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.2    |\n",
            "|    critic_loss     | 3.82     |\n",
            "|    ent_coef        | 0.531    |\n",
            "|    ent_coef_loss   | -2.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2209     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2340, episode_reward=109.07 +/- 1.79\n",
            "Episode length: 58.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 58.8     |\n",
            "|    mean_reward     | 109      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2340     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.3    |\n",
            "|    critic_loss     | 6.62     |\n",
            "|    ent_coef        | 0.527    |\n",
            "|    ent_coef_loss   | -2.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2239     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 39.1     |\n",
            "|    ep_rew_mean     | 61.5     |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 27       |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 2349     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.4    |\n",
            "|    critic_loss     | 4.69     |\n",
            "|    ent_coef        | 0.526    |\n",
            "|    ent_coef_loss   | -2.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2248     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2370, episode_reward=111.45 +/- 2.03\n",
            "Episode length: 60.00 +/- 0.89\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 60       |\n",
            "|    mean_reward     | 111      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2370     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.6    |\n",
            "|    critic_loss     | 4.56     |\n",
            "|    ent_coef        | 0.523    |\n",
            "|    ent_coef_loss   | -2.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2269     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2400, episode_reward=132.56 +/- 1.45\n",
            "Episode length: 66.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 66.4     |\n",
            "|    mean_reward     | 133      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2400     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -29.8    |\n",
            "|    critic_loss     | 2.91     |\n",
            "|    ent_coef        | 0.518    |\n",
            "|    ent_coef_loss   | -2.58    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2299     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2430, episode_reward=151.09 +/- 0.86\n",
            "Episode length: 75.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 75       |\n",
            "|    mean_reward     | 151      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2430     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -30.7    |\n",
            "|    critic_loss     | 5.25     |\n",
            "|    ent_coef        | 0.514    |\n",
            "|    ent_coef_loss   | -2.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2329     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2460, episode_reward=114.62 +/- 0.66\n",
            "Episode length: 61.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 61.4     |\n",
            "|    mean_reward     | 115      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2460     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -30.9    |\n",
            "|    critic_loss     | 5.6      |\n",
            "|    ent_coef        | 0.51     |\n",
            "|    ent_coef_loss   | -2.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2359     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2490, episode_reward=182.41 +/- 1.03\n",
            "Episode length: 87.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 87.2     |\n",
            "|    mean_reward     | 182      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2490     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -31.2    |\n",
            "|    critic_loss     | 4.78     |\n",
            "|    ent_coef        | 0.506    |\n",
            "|    ent_coef_loss   | -2.68    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2389     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2520, episode_reward=157.66 +/- 1.56\n",
            "Episode length: 78.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.8     |\n",
            "|    mean_reward     | 158      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2520     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -31      |\n",
            "|    critic_loss     | 2.35     |\n",
            "|    ent_coef        | 0.502    |\n",
            "|    ent_coef_loss   | -2.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2419     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2550, episode_reward=194.85 +/- 1.76\n",
            "Episode length: 93.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 195      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2550     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -32.3    |\n",
            "|    critic_loss     | 4.97     |\n",
            "|    ent_coef        | 0.498    |\n",
            "|    ent_coef_loss   | -2.7     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2449     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2580, episode_reward=177.65 +/- 1.53\n",
            "Episode length: 85.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.4     |\n",
            "|    mean_reward     | 178      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2580     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -32.2    |\n",
            "|    critic_loss     | 1.94     |\n",
            "|    ent_coef        | 0.493    |\n",
            "|    ent_coef_loss   | -2.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2479     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2610, episode_reward=189.40 +/- 1.38\n",
            "Episode length: 89.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.4     |\n",
            "|    mean_reward     | 189      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2610     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -31.8    |\n",
            "|    critic_loss     | 8.8      |\n",
            "|    ent_coef        | 0.489    |\n",
            "|    ent_coef_loss   | -2.66    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2509     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2640, episode_reward=204.28 +/- 1.81\n",
            "Episode length: 91.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2640     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -32.1    |\n",
            "|    critic_loss     | 10.2     |\n",
            "|    ent_coef        | 0.485    |\n",
            "|    ent_coef_loss   | -2.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2539     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2670, episode_reward=206.29 +/- 1.13\n",
            "Episode length: 99.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.8     |\n",
            "|    mean_reward     | 206      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2670     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -32.9    |\n",
            "|    critic_loss     | 5.98     |\n",
            "|    ent_coef        | 0.481    |\n",
            "|    ent_coef_loss   | -2.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2569     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2700, episode_reward=204.39 +/- 1.30\n",
            "Episode length: 93.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93       |\n",
            "|    mean_reward     | 204      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2700     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.9    |\n",
            "|    critic_loss     | 3.7      |\n",
            "|    ent_coef        | 0.477    |\n",
            "|    ent_coef_loss   | -2.99    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2599     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 42.5     |\n",
            "|    ep_rew_mean     | 69.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 27       |\n",
            "|    time_elapsed    | 100      |\n",
            "|    total_timesteps | 2717     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.5    |\n",
            "|    critic_loss     | 8.92     |\n",
            "|    ent_coef        | 0.475    |\n",
            "|    ent_coef_loss   | -2.9     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2616     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2730, episode_reward=209.93 +/- 0.39\n",
            "Episode length: 94.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 94       |\n",
            "|    mean_reward     | 210      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2730     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -33.8    |\n",
            "|    critic_loss     | 4.68     |\n",
            "|    ent_coef        | 0.474    |\n",
            "|    ent_coef_loss   | -2.98    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2629     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2760, episode_reward=209.27 +/- 1.00\n",
            "Episode length: 101.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 101      |\n",
            "|    mean_reward     | 209      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2760     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -34.4    |\n",
            "|    critic_loss     | 2.98     |\n",
            "|    ent_coef        | 0.47     |\n",
            "|    ent_coef_loss   | -3.02    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2659     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2790, episode_reward=228.27 +/- 3.18\n",
            "Episode length: 98.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 98.2     |\n",
            "|    mean_reward     | 228      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2790     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -34.4    |\n",
            "|    critic_loss     | 7.48     |\n",
            "|    ent_coef        | 0.466    |\n",
            "|    ent_coef_loss   | -3.03    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2689     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2820, episode_reward=205.44 +/- 1.77\n",
            "Episode length: 102.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 102      |\n",
            "|    mean_reward     | 205      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2820     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -35.1    |\n",
            "|    critic_loss     | 12.9     |\n",
            "|    ent_coef        | 0.462    |\n",
            "|    ent_coef_loss   | -2.88    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2719     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2850, episode_reward=231.49 +/- 0.77\n",
            "Episode length: 108.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 108      |\n",
            "|    mean_reward     | 231      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2850     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -34.6    |\n",
            "|    critic_loss     | 7.19     |\n",
            "|    ent_coef        | 0.458    |\n",
            "|    ent_coef_loss   | -2.97    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2749     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=2880, episode_reward=202.86 +/- 0.66\n",
            "Episode length: 96.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 96.6     |\n",
            "|    mean_reward     | 203      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2880     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -34.6    |\n",
            "|    critic_loss     | 4.79     |\n",
            "|    ent_coef        | 0.454    |\n",
            "|    ent_coef_loss   | -2.91    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2779     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2910, episode_reward=200.01 +/- 0.75\n",
            "Episode length: 93.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 93.2     |\n",
            "|    mean_reward     | 200      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2910     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -35.2    |\n",
            "|    critic_loss     | 2.56     |\n",
            "|    ent_coef        | 0.451    |\n",
            "|    ent_coef_loss   | -2.95    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2809     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2940, episode_reward=165.98 +/- 3.34\n",
            "Episode length: 80.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 80.4     |\n",
            "|    mean_reward     | 166      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2940     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -35.9    |\n",
            "|    critic_loss     | 9.22     |\n",
            "|    ent_coef        | 0.447    |\n",
            "|    ent_coef_loss   | -3.07    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2839     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=2970, episode_reward=196.52 +/- 2.86\n",
            "Episode length: 86.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 86.4     |\n",
            "|    mean_reward     | 197      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 2970     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -35.7    |\n",
            "|    critic_loss     | 5.51     |\n",
            "|    ent_coef        | 0.443    |\n",
            "|    ent_coef_loss   | -3.04    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2869     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3000, episode_reward=221.57 +/- 5.90\n",
            "Episode length: 105.20 +/- 0.98\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 105      |\n",
            "|    mean_reward     | 222      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3000     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -36.5    |\n",
            "|    critic_loss     | 8.17     |\n",
            "|    ent_coef        | 0.44     |\n",
            "|    ent_coef_loss   | -2.91    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2899     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3030, episode_reward=194.43 +/- 4.06\n",
            "Episode length: 89.20 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 89.2     |\n",
            "|    mean_reward     | 194      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3030     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -36.7    |\n",
            "|    critic_loss     | 8.57     |\n",
            "|    ent_coef        | 0.436    |\n",
            "|    ent_coef_loss   | -3.06    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2929     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3060, episode_reward=265.32 +/- 2.00\n",
            "Episode length: 119.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 120      |\n",
            "|    mean_reward     | 265      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3060     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -37.4    |\n",
            "|    critic_loss     | 8.85     |\n",
            "|    ent_coef        | 0.433    |\n",
            "|    ent_coef_loss   | -3.14    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2959     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 45.1     |\n",
            "|    ep_rew_mean     | 76.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 26       |\n",
            "|    time_elapsed    | 116      |\n",
            "|    total_timesteps | 3066     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -38.2    |\n",
            "|    critic_loss     | 4.7      |\n",
            "|    ent_coef        | 0.432    |\n",
            "|    ent_coef_loss   | -3.18    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2965     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3090, episode_reward=237.20 +/- 0.36\n",
            "Episode length: 103.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 103      |\n",
            "|    mean_reward     | 237      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3090     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -38      |\n",
            "|    critic_loss     | 3.35     |\n",
            "|    ent_coef        | 0.429    |\n",
            "|    ent_coef_loss   | -3.1     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 2989     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3120, episode_reward=166.36 +/- 0.85\n",
            "Episode length: 79.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 79.8     |\n",
            "|    mean_reward     | 166      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3120     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -37      |\n",
            "|    critic_loss     | 4.78     |\n",
            "|    ent_coef        | 0.426    |\n",
            "|    ent_coef_loss   | -3.16    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3019     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3150, episode_reward=231.64 +/- 1.52\n",
            "Episode length: 102.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 103      |\n",
            "|    mean_reward     | 232      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3150     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.4    |\n",
            "|    critic_loss     | 7.03     |\n",
            "|    ent_coef        | 0.422    |\n",
            "|    ent_coef_loss   | -3.15    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3049     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3180, episode_reward=157.86 +/- 0.25\n",
            "Episode length: 78.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78       |\n",
            "|    mean_reward     | 158      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3180     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -38.3    |\n",
            "|    critic_loss     | 5.14     |\n",
            "|    ent_coef        | 0.419    |\n",
            "|    ent_coef_loss   | -3.19    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3079     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3210, episode_reward=156.93 +/- 1.29\n",
            "Episode length: 77.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 77.6     |\n",
            "|    mean_reward     | 157      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3210     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.1    |\n",
            "|    critic_loss     | 7.79     |\n",
            "|    ent_coef        | 0.415    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3109     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3240, episode_reward=153.89 +/- 1.23\n",
            "Episode length: 78.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.2     |\n",
            "|    mean_reward     | 154      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3240     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.2    |\n",
            "|    critic_loss     | 4.9      |\n",
            "|    ent_coef        | 0.412    |\n",
            "|    ent_coef_loss   | -3.3     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3139     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3270, episode_reward=174.31 +/- 1.43\n",
            "Episode length: 88.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 88.6     |\n",
            "|    mean_reward     | 174      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3270     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -40      |\n",
            "|    critic_loss     | 3.63     |\n",
            "|    ent_coef        | 0.409    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3169     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3300, episode_reward=182.33 +/- 32.42\n",
            "Episode length: 90.80 +/- 13.64\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 90.8     |\n",
            "|    mean_reward     | 182      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3300     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.9    |\n",
            "|    critic_loss     | 5.31     |\n",
            "|    ent_coef        | 0.405    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3199     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3330, episode_reward=191.12 +/- 3.17\n",
            "Episode length: 92.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 92.8     |\n",
            "|    mean_reward     | 191      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3330     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.1    |\n",
            "|    critic_loss     | 4.23     |\n",
            "|    ent_coef        | 0.402    |\n",
            "|    ent_coef_loss   | -3.41    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3229     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3360, episode_reward=153.45 +/- 1.52\n",
            "Episode length: 79.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 79.6     |\n",
            "|    mean_reward     | 153      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3360     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -39.7    |\n",
            "|    critic_loss     | 11       |\n",
            "|    ent_coef        | 0.399    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3259     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3390, episode_reward=148.00 +/- 1.71\n",
            "Episode length: 78.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 78.2     |\n",
            "|    mean_reward     | 148      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3390     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.7    |\n",
            "|    critic_loss     | 3.29     |\n",
            "|    ent_coef        | 0.396    |\n",
            "|    ent_coef_loss   | -3.16    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3289     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3420, episode_reward=167.90 +/- 0.93\n",
            "Episode length: 85.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 85.6     |\n",
            "|    mean_reward     | 168      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3420     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 7.72     |\n",
            "|    ent_coef        | 0.393    |\n",
            "|    ent_coef_loss   | -3.26    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3319     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 47.8     |\n",
            "|    ep_rew_mean     | 83.6     |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 26       |\n",
            "|    time_elapsed    | 131      |\n",
            "|    total_timesteps | 3438     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.1    |\n",
            "|    critic_loss     | 7.46     |\n",
            "|    ent_coef        | 0.391    |\n",
            "|    ent_coef_loss   | -3.32    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3337     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3450, episode_reward=238.61 +/- 7.51\n",
            "Episode length: 116.20 +/- 2.48\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 116      |\n",
            "|    mean_reward     | 239      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3450     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.5    |\n",
            "|    critic_loss     | 10.7     |\n",
            "|    ent_coef        | 0.39     |\n",
            "|    ent_coef_loss   | -3.48    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3349     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3480, episode_reward=152.33 +/- 1.08\n",
            "Episode length: 80.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 80.4     |\n",
            "|    mean_reward     | 152      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3480     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -41.8    |\n",
            "|    critic_loss     | 3.69     |\n",
            "|    ent_coef        | 0.386    |\n",
            "|    ent_coef_loss   | -3.11    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3379     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3510, episode_reward=234.00 +/- 6.25\n",
            "Episode length: 111.40 +/- 2.06\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 111      |\n",
            "|    mean_reward     | 234      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3510     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.6    |\n",
            "|    critic_loss     | 9.25     |\n",
            "|    ent_coef        | 0.383    |\n",
            "|    ent_coef_loss   | -3.24    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3409     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3540, episode_reward=188.64 +/- 45.92\n",
            "Episode length: 91.20 +/- 18.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 91.2     |\n",
            "|    mean_reward     | 189      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3540     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -42.2    |\n",
            "|    critic_loss     | 8.38     |\n",
            "|    ent_coef        | 0.38     |\n",
            "|    ent_coef_loss   | -3.27    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3439     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3570, episode_reward=284.34 +/- 2.75\n",
            "Episode length: 126.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 126      |\n",
            "|    mean_reward     | 284      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3570     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44      |\n",
            "|    critic_loss     | 8.39     |\n",
            "|    ent_coef        | 0.377    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3469     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3600, episode_reward=199.33 +/- 49.49\n",
            "Episode length: 97.60 +/- 20.50\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 97.6     |\n",
            "|    mean_reward     | 199      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3600     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.2    |\n",
            "|    critic_loss     | 10.5     |\n",
            "|    ent_coef        | 0.374    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3499     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3630, episode_reward=170.95 +/- 30.83\n",
            "Episode length: 84.80 +/- 12.61\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 84.8     |\n",
            "|    mean_reward     | 171      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3630     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.6    |\n",
            "|    critic_loss     | 3.97     |\n",
            "|    ent_coef        | 0.371    |\n",
            "|    ent_coef_loss   | -3.38    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3529     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3660, episode_reward=277.03 +/- 1.20\n",
            "Episode length: 122.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 122      |\n",
            "|    mean_reward     | 277      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3660     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.4    |\n",
            "|    critic_loss     | 7.95     |\n",
            "|    ent_coef        | 0.368    |\n",
            "|    ent_coef_loss   | -3.44    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3559     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3690, episode_reward=147.58 +/- 1.13\n",
            "Episode length: 77.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 77.4     |\n",
            "|    mean_reward     | 148      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3690     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -43.9    |\n",
            "|    critic_loss     | 8.27     |\n",
            "|    ent_coef        | 0.365    |\n",
            "|    ent_coef_loss   | -3.45    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3589     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3720, episode_reward=291.64 +/- 0.49\n",
            "Episode length: 140.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 140      |\n",
            "|    mean_reward     | 292      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3720     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.5    |\n",
            "|    critic_loss     | 11.6     |\n",
            "|    ent_coef        | 0.363    |\n",
            "|    ent_coef_loss   | -3.56    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3619     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3750, episode_reward=275.36 +/- 0.90\n",
            "Episode length: 123.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 123      |\n",
            "|    mean_reward     | 275      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3750     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -44.3    |\n",
            "|    critic_loss     | 14.9     |\n",
            "|    ent_coef        | 0.36     |\n",
            "|    ent_coef_loss   | -3.33    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3649     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3780, episode_reward=248.99 +/- 0.74\n",
            "Episode length: 112.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 112      |\n",
            "|    mean_reward     | 249      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3780     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45      |\n",
            "|    critic_loss     | 9.45     |\n",
            "|    ent_coef        | 0.357    |\n",
            "|    ent_coef_loss   | -3.33    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3679     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3810, episode_reward=289.80 +/- 1.00\n",
            "Episode length: 126.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 127      |\n",
            "|    mean_reward     | 290      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3810     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.8    |\n",
            "|    critic_loss     | 3.77     |\n",
            "|    ent_coef        | 0.354    |\n",
            "|    ent_coef_loss   | -3.68    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3709     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3840, episode_reward=267.50 +/- 2.18\n",
            "Episode length: 125.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 126      |\n",
            "|    mean_reward     | 267      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3840     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -45.2    |\n",
            "|    critic_loss     | 5.45     |\n",
            "|    ent_coef        | 0.351    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3739     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3870, episode_reward=286.70 +/- 1.60\n",
            "Episode length: 130.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 131      |\n",
            "|    mean_reward     | 287      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3870     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.6    |\n",
            "|    critic_loss     | 9.95     |\n",
            "|    ent_coef        | 0.348    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3769     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 51.1     |\n",
            "|    ep_rew_mean     | 91.7     |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 25       |\n",
            "|    time_elapsed    | 153      |\n",
            "|    total_timesteps | 3887     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.6    |\n",
            "|    critic_loss     | 2.49     |\n",
            "|    ent_coef        | 0.347    |\n",
            "|    ent_coef_loss   | -3.37    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3786     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3900, episode_reward=246.31 +/- 2.66\n",
            "Episode length: 114.20 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 114      |\n",
            "|    mean_reward     | 246      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3900     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.3    |\n",
            "|    critic_loss     | 9.5      |\n",
            "|    ent_coef        | 0.346    |\n",
            "|    ent_coef_loss   | -3.36    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3799     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3930, episode_reward=278.50 +/- 1.85\n",
            "Episode length: 134.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 134      |\n",
            "|    mean_reward     | 279      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3930     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.1    |\n",
            "|    critic_loss     | 7.94     |\n",
            "|    ent_coef        | 0.343    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3829     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=3960, episode_reward=292.41 +/- 1.38\n",
            "Episode length: 131.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 131      |\n",
            "|    mean_reward     | 292      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3960     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.4    |\n",
            "|    critic_loss     | 5.69     |\n",
            "|    ent_coef        | 0.34     |\n",
            "|    ent_coef_loss   | -3.51    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3859     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=3990, episode_reward=291.23 +/- 2.08\n",
            "Episode length: 137.00 +/- 0.63\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 137      |\n",
            "|    mean_reward     | 291      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 3990     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -46.9    |\n",
            "|    critic_loss     | 8.23     |\n",
            "|    ent_coef        | 0.338    |\n",
            "|    ent_coef_loss   | -3.44    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3889     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4020, episode_reward=270.16 +/- 1.22\n",
            "Episode length: 116.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 116      |\n",
            "|    mean_reward     | 270      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4020     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47      |\n",
            "|    critic_loss     | 7.73     |\n",
            "|    ent_coef        | 0.335    |\n",
            "|    ent_coef_loss   | -3.5     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3919     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4050, episode_reward=282.67 +/- 2.18\n",
            "Episode length: 123.40 +/- 0.80\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 123      |\n",
            "|    mean_reward     | 283      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4050     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.7    |\n",
            "|    critic_loss     | 3.4      |\n",
            "|    ent_coef        | 0.332    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3949     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4080, episode_reward=296.56 +/- 0.77\n",
            "Episode length: 138.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 139      |\n",
            "|    mean_reward     | 297      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4080     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -47.9    |\n",
            "|    critic_loss     | 15       |\n",
            "|    ent_coef        | 0.33     |\n",
            "|    ent_coef_loss   | -3.62    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 3979     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4110, episode_reward=317.39 +/- 1.09\n",
            "Episode length: 133.80 +/- 0.75\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 134      |\n",
            "|    mean_reward     | 317      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4110     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.2    |\n",
            "|    critic_loss     | 3.75     |\n",
            "|    ent_coef        | 0.327    |\n",
            "|    ent_coef_loss   | -3.55    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4009     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4140, episode_reward=284.85 +/- 1.40\n",
            "Episode length: 121.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 121      |\n",
            "|    mean_reward     | 285      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4140     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.5    |\n",
            "|    critic_loss     | 3.39     |\n",
            "|    ent_coef        | 0.325    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4039     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4170, episode_reward=252.20 +/- 0.38\n",
            "Episode length: 115.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 116      |\n",
            "|    mean_reward     | 252      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4170     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49      |\n",
            "|    critic_loss     | 11.6     |\n",
            "|    ent_coef        | 0.322    |\n",
            "|    ent_coef_loss   | -3.76    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4069     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4200, episode_reward=260.04 +/- 2.85\n",
            "Episode length: 118.40 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 118      |\n",
            "|    mean_reward     | 260      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4200     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.6    |\n",
            "|    critic_loss     | 7.69     |\n",
            "|    ent_coef        | 0.319    |\n",
            "|    ent_coef_loss   | -3.8     |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4099     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4230, episode_reward=256.44 +/- 0.75\n",
            "Episode length: 111.60 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 112      |\n",
            "|    mean_reward     | 256      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4230     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -48.6    |\n",
            "|    critic_loss     | 4.7      |\n",
            "|    ent_coef        | 0.317    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4129     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4260, episode_reward=260.26 +/- 0.54\n",
            "Episode length: 114.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 115      |\n",
            "|    mean_reward     | 260      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4260     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.9    |\n",
            "|    critic_loss     | 8.62     |\n",
            "|    ent_coef        | 0.314    |\n",
            "|    ent_coef_loss   | -3.87    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4159     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4290, episode_reward=274.34 +/- 5.83\n",
            "Episode length: 119.80 +/- 1.47\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 120      |\n",
            "|    mean_reward     | 274      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4290     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.5    |\n",
            "|    critic_loss     | 14.6     |\n",
            "|    ent_coef        | 0.312    |\n",
            "|    ent_coef_loss   | -3.68    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4189     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4320, episode_reward=284.91 +/- 1.41\n",
            "Episode length: 124.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 125      |\n",
            "|    mean_reward     | 285      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4320     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.3    |\n",
            "|    critic_loss     | 7        |\n",
            "|    ent_coef        | 0.309    |\n",
            "|    ent_coef_loss   | -3.73    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4219     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4350, episode_reward=214.30 +/- 0.83\n",
            "Episode length: 99.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 99.4     |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4350     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -50.9    |\n",
            "|    critic_loss     | 12.7     |\n",
            "|    ent_coef        | 0.307    |\n",
            "|    ent_coef_loss   | -3.94    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4249     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 54.7     |\n",
            "|    ep_rew_mean     | 101      |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 24       |\n",
            "|    time_elapsed    | 177      |\n",
            "|    total_timesteps | 4378     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.4    |\n",
            "|    critic_loss     | 7.06     |\n",
            "|    ent_coef        | 0.305    |\n",
            "|    ent_coef_loss   | -3.59    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4277     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4380, episode_reward=276.21 +/- 1.24\n",
            "Episode length: 116.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 116      |\n",
            "|    mean_reward     | 276      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4380     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -49.9    |\n",
            "|    critic_loss     | 9.99     |\n",
            "|    ent_coef        | 0.304    |\n",
            "|    ent_coef_loss   | -3.65    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4279     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4410, episode_reward=274.04 +/- 0.75\n",
            "Episode length: 117.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 118      |\n",
            "|    mean_reward     | 274      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4410     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.5    |\n",
            "|    critic_loss     | 8.21     |\n",
            "|    ent_coef        | 0.302    |\n",
            "|    ent_coef_loss   | -3.83    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4309     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4440, episode_reward=273.09 +/- 2.72\n",
            "Episode length: 126.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 127      |\n",
            "|    mean_reward     | 273      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4440     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52      |\n",
            "|    critic_loss     | 7.51     |\n",
            "|    ent_coef        | 0.3      |\n",
            "|    ent_coef_loss   | -3.84    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4339     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4470, episode_reward=261.53 +/- 0.65\n",
            "Episode length: 112.80 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 113      |\n",
            "|    mean_reward     | 262      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4470     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.3    |\n",
            "|    critic_loss     | 7.34     |\n",
            "|    ent_coef        | 0.297    |\n",
            "|    ent_coef_loss   | -3.74    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4369     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4500, episode_reward=270.87 +/- 0.75\n",
            "Episode length: 127.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 127      |\n",
            "|    mean_reward     | 271      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4500     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.3    |\n",
            "|    critic_loss     | 5.64     |\n",
            "|    ent_coef        | 0.295    |\n",
            "|    ent_coef_loss   | -3.71    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4399     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4530, episode_reward=261.70 +/- 0.50\n",
            "Episode length: 115.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 115      |\n",
            "|    mean_reward     | 262      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4530     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -51.8    |\n",
            "|    critic_loss     | 10.7     |\n",
            "|    ent_coef        | 0.292    |\n",
            "|    ent_coef_loss   | -3.57    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4429     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4560, episode_reward=270.92 +/- 0.86\n",
            "Episode length: 119.40 +/- 1.36\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 119      |\n",
            "|    mean_reward     | 271      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4560     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.5    |\n",
            "|    critic_loss     | 12.8     |\n",
            "|    ent_coef        | 0.29     |\n",
            "|    ent_coef_loss   | -3.85    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4459     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4590, episode_reward=272.17 +/- 0.99\n",
            "Episode length: 115.20 +/- 0.40\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 115      |\n",
            "|    mean_reward     | 272      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4590     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -52.4    |\n",
            "|    critic_loss     | 8.32     |\n",
            "|    ent_coef        | 0.288    |\n",
            "|    ent_coef_loss   | -3.86    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4489     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4620, episode_reward=269.22 +/- 0.50\n",
            "Episode length: 113.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 113      |\n",
            "|    mean_reward     | 269      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4620     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.2    |\n",
            "|    critic_loss     | 4.53     |\n",
            "|    ent_coef        | 0.286    |\n",
            "|    ent_coef_loss   | -3.88    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4519     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4650, episode_reward=310.95 +/- 1.26\n",
            "Episode length: 125.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 125      |\n",
            "|    mean_reward     | 311      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4650     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -53.8    |\n",
            "|    critic_loss     | 9.29     |\n",
            "|    ent_coef        | 0.283    |\n",
            "|    ent_coef_loss   | -4.06    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4549     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4680, episode_reward=211.36 +/- 0.66\n",
            "Episode length: 95.00 +/- 0.00\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 95       |\n",
            "|    mean_reward     | 211      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4680     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.1    |\n",
            "|    critic_loss     | 4.31     |\n",
            "|    ent_coef        | 0.281    |\n",
            "|    ent_coef_loss   | -3.89    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4579     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4710, episode_reward=213.87 +/- 1.43\n",
            "Episode length: 100.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 101      |\n",
            "|    mean_reward     | 214      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4710     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.1    |\n",
            "|    critic_loss     | 11.6     |\n",
            "|    ent_coef        | 0.279    |\n",
            "|    ent_coef_loss   | -3.78    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4609     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4740, episode_reward=261.02 +/- 1.51\n",
            "Episode length: 108.40 +/- 0.49\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 108      |\n",
            "|    mean_reward     | 261      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4740     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.2    |\n",
            "|    critic_loss     | 9.18     |\n",
            "|    ent_coef        | 0.277    |\n",
            "|    ent_coef_loss   | -3.63    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4639     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4770, episode_reward=431.83 +/- 3.97\n",
            "Episode length: 167.20 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 167      |\n",
            "|    mean_reward     | 432      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4770     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -54.6    |\n",
            "|    critic_loss     | 11.2     |\n",
            "|    ent_coef        | 0.274    |\n",
            "|    ent_coef_loss   | -4.03    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4669     |\n",
            "---------------------------------\n",
            "New best mean reward!\n",
            "Eval num_timesteps=4800, episode_reward=243.67 +/- 69.58\n",
            "Episode length: 123.40 +/- 29.43\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 123      |\n",
            "|    mean_reward     | 244      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4800     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.9    |\n",
            "|    critic_loss     | 12.2     |\n",
            "|    ent_coef        | 0.272    |\n",
            "|    ent_coef_loss   | -3.93    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4699     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4830, episode_reward=270.63 +/- 71.13\n",
            "Episode length: 135.20 +/- 22.93\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 135      |\n",
            "|    mean_reward     | 271      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4830     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55      |\n",
            "|    critic_loss     | 9.78     |\n",
            "|    ent_coef        | 0.27     |\n",
            "|    ent_coef_loss   | -4.01    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4729     |\n",
            "---------------------------------\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 57.8     |\n",
            "|    ep_rew_mean     | 108      |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 24       |\n",
            "|    time_elapsed    | 200      |\n",
            "|    total_timesteps | 4858     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.8    |\n",
            "|    critic_loss     | 11.7     |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -4.07    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4757     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4860, episode_reward=257.32 +/- 2.05\n",
            "Episode length: 123.80 +/- 1.17\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 124      |\n",
            "|    mean_reward     | 257      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4860     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.6    |\n",
            "|    critic_loss     | 4.23     |\n",
            "|    ent_coef        | 0.268    |\n",
            "|    ent_coef_loss   | -3.64    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4759     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4890, episode_reward=284.12 +/- 2.36\n",
            "Episode length: 137.60 +/- 1.02\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 138      |\n",
            "|    mean_reward     | 284      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4890     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.4    |\n",
            "|    critic_loss     | 7.34     |\n",
            "|    ent_coef        | 0.266    |\n",
            "|    ent_coef_loss   | -4.06    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4789     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4920, episode_reward=255.71 +/- 2.37\n",
            "Episode length: 127.60 +/- 1.62\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 128      |\n",
            "|    mean_reward     | 256      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4920     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -55.3    |\n",
            "|    critic_loss     | 9.59     |\n",
            "|    ent_coef        | 0.264    |\n",
            "|    ent_coef_loss   | -3.52    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4819     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4950, episode_reward=278.11 +/- 2.61\n",
            "Episode length: 143.20 +/- 1.72\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 143      |\n",
            "|    mean_reward     | 278      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4950     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -56.5    |\n",
            "|    critic_loss     | 6.41     |\n",
            "|    ent_coef        | 0.262    |\n",
            "|    ent_coef_loss   | -3.88    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4849     |\n",
            "---------------------------------\n",
            "Eval num_timesteps=4980, episode_reward=245.53 +/- 80.45\n",
            "Episode length: 115.40 +/- 33.21\n",
            "---------------------------------\n",
            "| eval/              |          |\n",
            "|    mean_ep_length  | 115      |\n",
            "|    mean_reward     | 246      |\n",
            "| time/              |          |\n",
            "|    total_timesteps | 4980     |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -57.2    |\n",
            "|    critic_loss     | 14.4     |\n",
            "|    ent_coef        | 0.259    |\n",
            "|    ent_coef_loss   | -3.88    |\n",
            "|    learning_rate   | 0.0003   |\n",
            "|    n_updates       | 4879     |\n",
            "---------------------------------\n",
            "Modello salvato come ./sim2real/models/sac_hopper.zip\n",
            "/usr/local/lib/python3.10/dist-packages/jupyter_client/connect.py:28: DeprecationWarning: Jupyter is migrating its paths to use standard platformdirs\n",
            "given by the platformdirs library.  To remove this warning and\n",
            "see the appropriate new directories, set the environment variable\n",
            "`JUPYTER_PLATFORM_DIRS=1` and then run `jupyter --paths`.\n",
            "The use of platformdirs will be the default in `jupyter_core` v6\n",
            "  from jupyter_core.paths import jupyter_data_dir, jupyter_runtime_dir, secure_write\n",
            "Figure(1000x600)\n",
            "Reward plot saved in ./sim2real/plots/rewards_plot.png\n",
            "State space: Box(-inf, inf, (11,), float64)\n",
            "Action space: Box(-1.0, 1.0, (3,), float32)\n",
            "Dynamics parameters: [2.53429174 3.92699082 2.71433605 5.0893801 ]\n"
          ]
        }
      ]
    }
  ]
}